\section{Cognitive Radio and AI-Enabled Network Symposium}
\subsection{Deep Learning I}
\subsubsection{Densely-Accumulated Convolutional Network for Accurate LPI Radar Waveform Recognition}
abstract:This paper presents a deep learning-based method to automatically recognize low probability of intercept (LPI) radar waveforms against diversified jamming attacks. Concretely, an efficient convolutional neural network (CNN) architecture, namely Densely-Accumulated Network (DANet), is introduced to learn the time-frequency representation transformed by the Wigner-Ville distribution. Such an architecture has several novel densely-accumulated connection modules specified by various symmetric and asymmetric convolutional layers to enrich diversified features at multiple representational maps. Besides, the skip-connection and dense-connection are leveraged to improve feature learning efficiency and prevent the vanishing gradient when the network goes deeper. Some image processing techniques (e.g., global thresholding and digital filtering) are adopted to enhance the quality of time-frequency image. Relying on simulations, we benchmark the proposed method on a synthetic 13-waveform dataset and also investigate the influence of hyper-parameters (such as image size, number of modules, training data size) on the overall recognition performance. Remarkably, with average accuracy of 98.2% at 0 dB signal-to-noise ratio (SNR), DANet outperforms several backbone CNNs and state-of-the-art networks of LPI waveform recognition while keeping a cost-efficient model.
\subsubsection{MCformer: A transformer based deep neural network for automatic modulation classification}
abstract:In this paper, we propose MCformer - a novel deep neural network for the automatic modulation classification task of complex-valued raw radio signals. MCformer architecture leverages convolution layer along with self-attention based encoder layers and provides state of the art classification accuracy at all signal-to-noise ratios in the RadioML2016.10b data-set. MCformer provides significantly better performance with less number of parameters which is critical for faster and energy-efficient performance.
\subsubsection{Short-Packet Communications in Multi-Hop WPINs: Performance Analysis and Deep Learning Design}
abstract:In this paper, we study short-packet communications (SPCs) in multi-hop wireless-powered Internet-of-Things networks (WPINs), where IoT devices transmit short packets to multiple destination nodes by harvesting energy from multiple power beacons. To improve system block error rate (BLER) and throughput, we propose a best relay-best user (bR-bU) selection scheme with an accumulated energy harvesting mechanism. Closed-form expressions for the BLER and throughput of the proposed scheme over Rayleigh fading channels are derived and the respective asymptotic analysis is also carried out. To support real-time settings, we design a deep neural network (DNN) framework to predict the system throughput under different channel settings. Numerical results demonstrate that the proposed bR-bU selection scheme outperforms several baseline ones in terms of the BLER and throughput, showing to be an efficient strategy for multi-hop SPCs. The resulting DNN can estimate accurately the throughput with low execution time. The effects of message size on reliability and latency are also evaluated and discussed.
\subsubsection{Adversarial Learning for Hiding Wireless Signals}
abstract:Spectrum access in the next generation congested cognitive wireless networks will be vulnerable to malicious intents of strong adversaries. One of the ways to obscure an adversary is to hide a signal to avoid the threats of being intercepted or jammed. In this paper, we present such a wireless steganography technique, where we hide the secret message as a form of noise. We use adversarial learning model to transform the domain of secret message, from bits to complex signal that is statistically identical to hardware noise of a transmitter. A three-node neural network, an encoder, a decoder, and a critic (steganalyzer), is optimized jointly to encode and decode a secret message while adhering to statistically identical properties of the encoded covert signal and hardware generated noise. Once the domain transfer is complete, this covert signal can be carried by any cover signal, independent of its waveform or modulation order. We provide an information-theoretic analysis to evaluate the embedding capability of the learning model. Our results show that the covert signal achieves a throughput of 12Mbps at 12dB SNR and is resilient to different levels of hardware noise.
\subsection{Deep Learning II}
\subsubsection{A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification}
abstract:Advantages of deep learning over traditional methods have been demonstrated for radio signal classification in the recent years. However, various researchers have discovered that even a small but intentional feature perturbation known as adversarial examples can significantly deteriorate the performance of the deep learning based radio signal classification. Among various kinds of adversarial examples, universal adversarial perturbation has gained considerable attention due to its feature of being data independent, hence as a practical strategy to fool the radio signal classification with a high success rate. Therefore, in this paper, we investigate a defense system called neural rejection system to propose against universal adversarial perturbations, and evaluate its performance by generating white-box universal adversarial perturbations. We show that the proposed neural rejection system is able to defend universal adversarial perturbations with significantly higher accuracy than the undefended deep neural network.
\subsubsection{DQN-based Beamforming for Uplink mmWave Cellular-Connected UAVs}
abstract:Unmanned aerial vehicles (UAVs) are the emerging vital components of millimeter wave (mmWave) wireless systems. Accurate beam alignment is essential for efficient beam based mmWave communications of UAVs with base stations (BSs). Conventional beam sweeping approaches often have large over-head due to the high mobility and autonomous operation of UAVs. Learning-based approaches greatly reduce the overhead by leveraging UAV data, like position to identify optimal beam directions. In this paper, we propose a reinforcement learning(RL)-based framework for UAV-BS beam alignment using deep Q-Network (DQN) in a mmWave setting. We consider uplink communications where the UAV hovers around 5G new radio(NR) BS coverage area, with varying channel conditions. The proposed learning framework uses the location information to maximize data rate through the optimal beam-pairs efficiently, upon every communication request from UAV inside the multi-location environment. We compare our proposed framework against Multi-Armed Bandit (MAB) learning-based approach and the traditional exhaustive approach, respectively and also analyse the training performance of DQN-based beam alignment over different coverage area requirements and channel conditions. Our results show that the proposed DQN-based beam alignment converge faster and generic for different environmental conditions. The framework can also learn optimal beam alignment comparable to the exhaustive approach in an online manner under real-time conditions.
\subsubsection{A Deep Learning Framework for Distributed Channel Selection in a Congested Uncooperative Spectrum}
abstract:In this paper, a deep learning framework for distributed interference assessment and channel selection in a congested, uncooperative spectrum is proposed. We present a deep learning based spectrum sensing model that is trained to estimate channel quality. Such training is conducted offline in a supervised manner using labeled channel energy captures and their associated packet error rate measurements. Packet error rate estimates are derived from numerical simulations of the system model. These estimates can be used to take action on network channel selection. Distributed application of our model is efficiently realized through online training at the level of edge devices and then the trained local models are periodically aggregated using federated learning. The model views packet error rate assessment as a classification problem, where the packet error rate is assigned a class based on a set of classes with upper and lower bounds. The trained models are tested against two metrics: classification accuracy and the mean ratio of error rate in the optimal operating channel to that in a channel selected based on classification inference. In the simulation scenarios, the model when trained in a centralized manner has a classification accuracy of 91.9% and a mean selection error rate ratio 91.3%, averaged over all scenarios. When trained in a distributed manner, the model has a classification accuracy of 83.3% and a mean selection error rate ratio of 88.6%. The distributed approach is estimated to reduce the communications overhead by two orders of magnitude to approximately 34 MB per edge device.
\subsubsection{Augmented Convolutional Neural Networks with Transformer for Wireless Interference Identification}
abstract:As electromagnetic environments are more and more complex, wireless interference identification (WII) is becoming vital for non-cooperative communication systems in both civilian and military scenarios. With the enormous success of deep learning (DL), methods that optimize convolutional neural networks (CNNs) for WII have been proposed. However, due to the intrinsic characteristics of CNN, the existing networks are difficult to capture long-range feature dependencies, causing the low recognition accuracy and the high computational complexity. Motivated by the success of transformers in natural language processing (NLP) domain, we propose an augmented convolutional neural network with transformer (ACNNT), which combines both the advantages of CNNs and transformers to simultaneously strengthen locality and establish long-range dependencies. Specifically, the ACNNT has multiple stages, and every stage consists of a convolutional layer and a transformer module to model local and long-range dependencies of context, respectively. At the end of the network, a classification token is used for classification. A channel attention (CA) module is proposed to further improve the expressive ability of the transformers. Extensive experiments demonstrate that the proposed method leads to performance improvement as compared to conventional DL based methods.
\subsection{Intelligent Resource Allocation}
\subsubsection{Multiple Relay Robots-Assisted URLLC for Industrial Automation with Deep Neural Networks}
abstract:Ultra-reliable low-latency communications (URLLC) has unravelled wireless intelligent control and operation in factory automation. However, the severe shadowing effects in highly dense factories renders the automation control signal's reception less reliable, which poses a huge challenge in meeting stringent quality-of-service (QoS) in industry automation. As such, in this paper, we propose to use multiple mobile robots as relay terminals to assist the wireless connectivity between the base stations and industrial Internet-of-Things(IIoT) devices. Under the strict latency constraint via short blocklength, we propose an optimal resource allocation scheme to minimise the error probability at the IIoT devices. For fast deployment, we propose a deep neural network to optimise the positions of the mobile robots. Then, a joint blocklength and power allocation optimisation of the base stations and relay robots is considered. Due to non-convexity of such optimization problem, we propose a sub-problem with an effective iterative algorithm for solving the reliability maximisation. Representativenumerical results are provided to demonstrate the advantages of our proposed scheme over the conventional approach.
\subsubsection{Adaptive Power and Rate Control for Mixed Proactive Pushing and On-demand Traffic: A CMDP Approach}
abstract:Proactive pushing has been recognized as a promising solution to support the dramatically increasing global data traffic, thereby gaining much attention recently. By proactively pushing popular files to users prior to their requests, the data traffic load over peak hours can be relieved, leading to substantially reduced content access latency. However, extra power consumption is incurred due to pushing. How to efficiently schedule proactive pushing and on-demand transmission becomes an important problem. In this paper, we present a unified framework for joint proactive pushing and on-demand transmission. Based on the joint proactive pushing and on-demand transmission scheme, we minimize the average content queueing latency through a Constrained Markov Decision Process (CMDP) approach while satisfying a power constraint. Through linear programming (LP) formulation, we obtain the optimal delay-power tradeoff of the traditional on-demand transmission only scheme and the joint pushing and on-demand transmission scheme, respectively. Finally, simulation results present that joint scheduling of pushing and on-demand transmission significantly reduces the content queueing delay.
\subsubsection{Social Welfare Maximization Auction in Joint Radar Communication Systems for Autonomous Vehicles}
abstract:Joint radar-communications (JRC) has been proposed recently for autonomous vehicles (AVs) to simultaneously perform radar sensing, e.g., detecting distant vehicles and pedestrian, and data transmission, e.g., to edge computing services, all on the same waves. However, due to the high AV density in urban area, the spectrum service provider (SSP) needs to allocate the spectrum resources optimally. In this paper, we consider the social welfare of the network which is defined as the total revenue of the SSP and the utilities of the AV users, and propose an auction-based algorithm to model the competition among the AV users to obtain the spectrum resources and maximize the social welfare.Since some AV users can have critical and useful information about each other (e.g., AV neighbours sharing traffic data), we consider the network effect in our proposed auction mechanism to incentivize more AV users to join the auction. The numerical results demonstrate the effectiveness of our proposed design compared to traditional schemes.
\subsubsection{Temporal and Spectral Analysis of Spectrum Hole Distributions in an LTE Cell}
abstract:Dynamic Spectrum Access (DSA) is proposed to improve spectrum efficiency by enabling opportunistic access of underutilized spectrum resources. The key to successful DSA operations is the correct understanding of spectrum hole distributions. Though huge amounts of studies have been conducted on spectrum tenancy due to the significance of spectrum hole distributions, there are still two overlooked aspects. One is the measurement resolution, and the other is the spectrum distribution in the spectral perspective. Since the spectrum hole analysis relies on the measurement data, we decode the LTE downlink control information to obtain the spectrum tenancy at the same time-frequency granularity with LTE scheduling. We analyze the spectrum hole distributions in fine resolutions along both the temporal and the spectral dimensions, and investigate the performance of two widely used spectrum tenancy models, the Markov and the on/off models, in terms of their capabilities on capturing the distributions of spectrum holes. Our observations include but are not limited to the following. The spectrum holes follow the power law distributions when examined in the LTE scheduling unit from both the time and the frequency perspectives. Both Markov and on/off models should be fitted to the spectrum tenancy along the frequency perspective to achieve their best performance.
\subsubsection{Load-balanced Task Allocation for Covid-19 Close Contact Detection in Heterogeneous MEC Networks}
abstract:In this paper, we investigate the close contact detection for COVID-19 patients based on the heterogeneous mobile edge computing (MEC) framework. Collecting the spatial-temporal data of a large number of mobile users, the base stations equipped with MEC servers organize these data via the R-tree structure. The cloud center (CC) aggregates the spatial-temporal data from all MEC servers. Considering the mobility of users as well as various positions of MEC servers, the CC then partitions and assigns the close contact detection tasks to different servers for faster processing. Aiming to minimize the system latency, we propose a Deep Deterministic Policy Gradient-based task and resource allocation scheme, where the computing loads are balanced among different servers. Simulation results show that a minimum system latency is reached while maintaining the load balance among all servers. Up to 37% detection accuracy enhancement is achieved compared with an existing task allocation scheme without load balance.
\subsection{Intelligent Spectrum Sharing}
\subsubsection{Wireless Standard Classification Using Convolutional Neural Networks}
abstract:The growing prominence of spectrum sharing technologies has spurred interest in spectrum monitoring technologies with the ability to identify unknown wireless signals.This paper presents a convolutional neural network (CNN) deep learning model to classify 4G LTE downlink, 4G LTE uplink, 5G NR downlink, 5G NR uplink, IEEE 802.11ax (WiFi 6), and Bluetooth Low Energy (BLE) 5.0 signals.The classifier operates on In-phase and Quadrature (I/Q) samples and does not require synchronization with the unknown signals.To improve the generalizability of the classifier, comprehensive signal datasets are generated to include a wide range of signal configurations found in the standards.These signals are impaired with additive white Gaussian noise (AWGN), Rayleigh or Ricean multipath fading channels, frequency offsets, and I/Q imbalances to make the signals more realistic.The exploration of time domain, frequency domain, and time-frequency domain features reveals high frequency resolution time-frequency domain features perform best.The proposed CNN model achieves a high classification accuracy in the presence of all of the aforementioned impairments, achieving over 94% accuracy for signal to noise ratios (SNR) greater than 0 dB.
\subsubsection{Enhancing Multi-RAT Coexistence in Unlicensed mmWave Bands Using Hybrid-Beamforming}
abstract:The radio spectrum is becoming an increasingly scarce and valuable resource to such an extent that sharing the unlicensed bands is inevitable. In this work, we consider a multi-cell, multi-user massive multiple-input multiple-output (MIMO) coexistence scenario where multiple 5G New Radio Unlicensed (NR-U) and Wireless Gigabit (WiGig) links share an unlicensed millimeter-wave band. Our aim is to enhance the performance of coexisting networks by maximizing the overall network throughput via hybrid beamforming. This throughput is a function of both operators' medium access control protocols and physical layer parameters. We propose a novel hidden node aware hybrid beamforming design. The hybrid precoders and combiners are optimal in the sense that they simultaneously maximize the signal power at the desired users while minimizing the received inter-cell and intra-cell intereferences at the undesired users (leakage). The performance of the proposed scheme is examined through simulations. A comparison among the proposed method, a beam-steering solution, and an optimal unconstrained precoding design indicates the efficiency of the proposed algorithm.
\subsubsection{Weighted Centroid Location Based Spectrum Status Identification in Cognitive Radio Network}
abstract:Due to the non-cooperative coexistence mechanism between the primary users (PUs) and secondary users (SUs), seeking spectrum opportunities for the SUs is usually unreliable by merely relying on traditional spectrum sensing technology. As one kind of auxiliary information, the mutual location information of the PUs and SUs can assist in determining the status of the licensed frequency band (LFB). This paper proposes a low-complexity neighborhood-based weighted centroid localization (NB-WCL) algorithm to solve the SU localization problem in the cellular cognitive radio network (CCRN). The proposed algorithm is capable of setting the LFB-access enabling flag for the SUs in the CCRN, based on their positioning results. The root mean square error (RMSE) of the proposed two-dimensional position estimation algorithm is analyzed. Theoretical analysis and experimental results suggest that the proposed algorithm outperforms some existing conventional localization algorithms with more robustness and better error performance. The proposed algorithm can serve as a practically effective candidate solution for LFB status identification in the CCRN.
\subsubsection{Mutualistic Mechanism in Symbiotic Radios}
abstract:In symbiotic radio (SR), also called cognitive backscatter communications, a secondary transmitter (STx) transmits messages by modulating its information over the RF signals from a primary transmitter (PTx), and in return, the secondary transmission provides multipath gain instead of interference to the primary transmission when the spreading factor K of SR is large enough. In this paper, we are interested in the fundamental mutualistic mechanism between the primary and secondary transmissions in SR, which describes the condition through which the two systems can benefit each other. We first derive the closed-form expressions for the bit error rates (BERs) for both primary and secondary transmissions for general K, then obtain the condition on K to enable mutualistic symbiosis in SR. It is observed that the critical point of K is related to the average strengths of the direct and backscatter links when the number of receiving antennas is large. Extensive simulation and numerical results are provided to verify the accuracy of theoretical analysis and demonstrate the interrelationship between primary and secondary transmissions.
\subsection{Reinforcement Learning}
\subsubsection{Power Allocation for Device-to-Multi-Device Enabled HetNets: A Deep Reinforcement Learning Approach}
abstract:Device-to-device (D2D) communication exploits the geographical proximity by allowing neighboring devices to directly communicate with each other, which becomes one of the most promising technologies to improve the spectral and energy efficiency for 5G and beyond communication systems. To further improve the spectral efficiency and generalize application scenarios, the emerging device-to-multi-device (D2MD) communication enables the D2D transmitter to communicate with multiple receivers simultaneously. In this paper, we consider a heterogeneous network (HetNet) where multiple D2MD clusters coexist with the base station (BS) and cellular users (CUs). All D2MD clusters share the same downlink channel as the cellular network, which potentially leads to severe co-channel interference. To solve this problem, we leverage the deep reinforcement learning (DRL) and propose the deep reinforcement power allocation (DRPA) algorithm to dynamically allocate power for D2MD communication in HetNets. In addition, we apply the centralized training distributed execution (CTDE) technique to accelerate the training process and improve the robustness of DRPA. Simulation results demonstrate that the DRPA algorithm outperforms baseline methods in terms of maximizing the average sum-rate. In addition, the DRPA algorithm is robust to the changes of network environment while achieving near-optimal performance.
\subsubsection{Learning-based Strategy for RIS-Assisted Terahertz Virtual Reality Networks}
abstract:The quality of experience (QoE) requirement of wireless Virtual Reality (VR) can only be satisfied with high data rate, high reliability and low VR interaction latency. This high data rate over short transmission distances may be achieved via abundant bandwidth in the terahertz (THz) band. However, THz waves suffer from severe signal attenuation, which may be compensated by the reconfigurable intelligent surface (RIS) technology with adjustable phase-shift of each reflecting element. Motivated by above, in this paper, we propose a RIS-assisted THz VR network in an indoor scenario, taking into account the viewpoint prediction and downlink transmission. We first propose a Genie-aided online Gated Recurrent Unit (GRU) and an integration of online Long-short Term Memory (LSTM) and Convolutional Neural Network (CNN) algorithm to predict the viewpoint, location and the LoS/NLoS status of VR users over time, with the aim to optimize the long-term QoE of VR users. We then develop a constrained deep reinforcement learning algorithm to select the optimal phase shifts of the RIS for the downlink transmission under the latency constraint. Simulation results show that our proposed ensemble learning architecture achieves near-optimal QoE as that of the Exhaustive algorithm, and about two times improvement in QoE compared to the random phase shift selection scheme.
\subsubsection{Multi-Objective Network Congestion Control via Constrained Reinforcement Learning}
abstract:Traditional congestion control algorithms rely on various model-based methods to improve the end-to-end (E2E) performance of packet transmission. The resulting decisions quickly become less effective amid the dynamics of network conditions. In order to perform congestion control adaptively, reinforcement learning (RL) can be adopted to continuously learn the optimal strategy from the network environment. Oftentimes, the reward of such a learning problem is a weighted sum of multiple E2E performance metrics, such as throughput, delay, and fairness. Unfortunately, those weights can be only manually tuned based on extensive experiments. To address this issue, in this paper, we design a constrained RL algorithm for congestion control named CRL-CC to adaptively tune those weights, with the objective of effectively improving the overall E2E packet transmission performance. In particular, the multi-objective optimization problem is firstly formulated as a constrained optimization problem. Then, the Lagrangian relaxation method is leveraged to transform the constrained optimization problem into a single-objective optimization problem, which is solved by designing a multi-objective reward function with Lagrangian multipliers. Extensive experiments based on OpenAI-Gym show that the proposed CRL-CC algorithm can achieve higher overall performance in various network conditions. In particular, the CRL-CC algorithm outperforms the benchmark algorithm on Pantheon by 21.7%, 27.4%, and 5.3% in throughput, delay, and fairness, respectively.
\subsubsection{Reliable Reinforcement Learning Based NOMA Schemes for URLLC}
abstract:In this paper, we propose a deep state-action-reward-state-action (SARSA) .\lambda. learning approach for optimising the uplink resource allocation in non-orthogonal multiple access (NOMA) aided ultra-reliable low-latency communication (URLLC). To reduce the mean decoding error probability in time-varying network environments, this work designs a reliable learning algorithm for providing a long-term resource allocation, where the reward feedback is based on the instantaneous network performance. With the aid of the proposed algorithm, this paper addresses three main challenges of the reliable resource sharing in NOMA-URLLC networks: 1) Dynamic user clustering; 2) Instantaneous feedback system; and 3) Optimal resource allocation. All of these designs interact with the considered communication environment. The simulation outcomes show that: 1) Compared with the traditional Q learning algorithm, the proposed solution converges faster and obtains better performance; 2) NOMA assisted URLLC outperforms traditional OMA systems in terms of decoding error probabilities; and 3) The dynamic feedback system is efficient for the long-term learning process.
\subsubsection{Dynamic Channel Access via Meta-Reinforcement Learning}
abstract:In this paper, we address the channel access problem in a dynamic wireless environment via meta-reinforcement learning. Spectrum is a scarce resource in wireless communications, especially with the dramatic increase in the number of devices in networks. Recently, inspired by the success of deep reinforcement learning (DRL), extensive studies have been conducted in addressing wireless resource allocation problems via DRL. However, training DRL algorithms usually requires a massive amount of data collected from the environment for each specific task and the well-trained model may fail if there is a small variation in the environment. In this work, in order to address these challenges, we propose a meta-DRL framework that incorporates the method of Model-Agnostic Meta-Learning (MAML). In the proposed framework, we train a common initialization for similar channel selection tasks. From the initialization, we show that only a few gradient descents are required for adapting to different tasks drawn from the same distribution. We demonstrate the performance improvements via simulation results.
\section{Communication & Information System Security}
\subsection{AI aided Security 1}
\subsubsection{Dual-Masking Framework against Two-Sided Model Attacks in Federated Learning}
abstract:With the popularity of AIoT (Artificial Intelligence of Things) services, we can foresee that smart end devices will generate tremendous user data at the edge. In particular, it is critical to address how to properly distill knowledge from the edge network in a communication-efficient and privacy-preserving manner. Federated learning (FL), one of the promising machine learning frameworks, ensures data privacy by allowing end devices to collaboratively train a shared model without exposing raw data to an aggregation server. However, due to its distributed nature, the framework is vulnerable to two major threats: the Model Inversion Attacks and the Model Poisoning Attacks. An abnormal aggregator or malicious end devices may probably launch these attacks in the training phase. The former leaks sensitive information by reversing the model weights to users' raw data. Still, the latter can break the model security and mislead the global model to wrong inference results. Unfortunately, the existing research has not tackled such two-sided model attacks that occurred concurrently in FL. Therefore, in this paper, we propose a dual-masking federated learning (DMFL) framework that advocates partial weights uploading in the aggregation process and applies two kinds of masks on both the end device and the aggregator sides. Based on the benchmark data for image classification, our experimental results show that the proposed DMFL framework outperforms other baselines, confirming that it can successfully preserve weights privacy and protect model security for AIoT.
\subsubsection{FedGR: A Lossless-Obfuscation Approach for Secure Federated Learning}
abstract:Federated learning is a promising new technology in the field of artificial intelligence. However, the unprotected model gradient parameters in federated learning may reveal sensitive participants information. To address this problem, we present a secure federated learning framework called FedGR. We use Paillier homomorphic encryption to design a new gradient security replacement algorithm, which eliminates the connections between gradient parameters and user sensitive data. In addition, we revisit the previous work by Aono and Hayashi(IEEE TIFS 2017) and show that, with their method, the user's local computing burden is too heavy. We then solve this problem by constructing an enhanced system with the following characteristics: 1) The system does not leak any information to the server. 2) Compared with that of ordinary deep learning systems, the accuracy of federated training results yielded by our system remains unchanged. 3)The proposed approach greatly reduces the user's local computing overhead.
\subsubsection{FL-PATE: Differentially Private Federated Learning with Knowledge Transfer}
abstract:Federated learning provides a solution for data privacy protection, while enabling training over the local data samples, without exchanging them. However, it is far from practical and secure because data privacy is still vulnerable due to the well-studied attacks, e.g., membership inference attacks and model inversion attacks. In this paper, to further prevent data leakage against these attacks, we propose FL-PATE, a differentially private federated learning framework with knowledge transfer. Specifically, participants with sensitive data are grouped to train teacher models under federated learning settings, and the knowledge of teacher models is transferred to a publicly accessible student model for prediction via aggregating teacher models' outputs of public datasets. A modified client-level differential privacy mechanism is used to guarantee each participant's data privacy during the corresponding teacher model's training process. The proposed framework preserves participant's privacy against membership inference attacks and the differential privacy cost is fixed. The privacy analysis and experiments demonstrate that trained teacher and student models have an excellent performance in accuracy and robustness theoretically and empirically.
\subsubsection{Augmented Dual-Shuffle-based Moving Target Defense to Ensure CIA-triad in Federated Learning}
abstract:In today's "Internet of Everything (IoE)" era, the collaboration from massive participants significantly boosts the performance and efficiency of model training. This trend also unavoidably stirs up considerable concerns about multi-dimensional security problems. Under the circumstances, federated learning (FL) is enthusiastically adopted, as it protects privacy to a certain extent by only processing personal data locally. Nevertheless, FL's characteristics of concealment also pave the way for several emerging attacks during the training process, i.e., model inversion, poisoning, and backdoor. Currently, although partially mitigating attack effects, existing countermeasures against those threats are studied separately and orthogonal. This separation makes those defense methods mutually exclusive and restrictive in real-world application scenarios, far from satisfying. In this paper, we extensively model different attack paradigms into three types based on CIA-triad, the well-known information security primitive, and propose a novel dual-shuffle method to thwart aforementioned threats jointly. Concretely speaking, our primary model shuffling mechanism provides the confidentiality guarantee based on the information-theoretic notion of identifiability; then, an augmented client shuffling mechanism purges the user group of adversaries proactively without any compromise of anonymous constraints. By conducting a series of experiments on benchmark datasets, we demonstrate that our method could achieve significant security and convergence performance against three state-of-the-art attacks.
\subsubsection{BPFL: A Blockchain Based Privacy-Preserving Federated Learning Scheme}
abstract:Federated Learning (FL), which allows multiple participants to co-train machine Learning models without exposing local data, has been recognized as a promising method in the past few years. However, in the FL process, the attacker could backtrack the users' sensitive information from the local model through attacks such as the Membership-Inference attack. Most existing privacy-preservation FL schemes seldom verify the submitted local data model, which increases the possibility of malicious adversaries poisoning the global model. In this paper, we propose a Blockchain based Privacy-preserving Federated Learning scheme named BPFL, which uses blockchain as the underlying distributed framework of FL. Homomorphic encryption and Multi-Krum technology are combined to achieve ciphertext-level model aggregation and model filtering, which can guarantee the verifiability of local models while realizing privacy-preservation. Security analysis and performance evaluation prove that the proposed scheme can achieve enhanced security and improve the performance of the FL model.
\subsection{AI aided Security 2}
\subsubsection{HFL-DP: Hierarchical Federated Learning with Differential Privacy}
abstract:Federated learning (FL) is a framework of distributed machine learning, which aims to protect data privacy by transferring parameters instead of private data from local clients. Compared with the typical cloud-client architecture, applying FL on a cloud-edge-client hierarchical architecture could train the model faster and achieve better communication-computation trade-offs. However, hierarchical federated learning (HFL) still suffers from privacy leakage by analyzing uploaded parameters from clients or edge servers. To address this problem, we propose a privacy-preserving based on the approach of local differential privacy (LDP) by adding the noise to the shared model parameters before uploading them to edge and cloud servers. According to our analysis by the moment accounting, the proposed algorithm can realize the strict differential privacy guarantee for the layers of clients and edge servers with adjustable privacy protection levels. We evaluate its performance based on the image classification tasks and the result demonstrates that our theoretical analysis are consistent with simulations.
\subsubsection{Safeguard the Original Data in Federated Learning via Data Decomposition}
abstract:In federated learning, more and more studies have discovered that attackers can recover the original data from the shared gradients of participants. However, existing defense models struggle to balance the privacy of participants and the effectiveness of federated learning in the face of cutting-edge attack models. Therefore, we propose a powerful defense model to protect the original data while ensuring the effect of classification. First, we get two featured datasets from original data based on Sparse Dictionary Learning (DL) or QR decomposition. In these two featured datasets, we select one dataset to replace the original data for federated training named co-trained data, and the other one is kept on the local client named left data. At this point, the adversary in federated learning can only obtain co-trained data, which cannot recover the original data due to the lack of left data. Following the completion of the federated learning, the participant requests a parameter from the server. Using this parameter, we can combine the aggregated global model over co-trained data with the offline-trained local model of an arbitrary participant to develop the final classification results. Some theories and a lot of experiments demonstrate the classification effectiveness of our model. It also can be a general solution to the original data leakage problems caused by gradient leakage.
\subsubsection{Secure and Cooperative Target Tracking via AUV Swarm: A Reinforcement Learning Approach}
abstract:The autonomous underwater vehicle (AUV) has gradually become an important platform for performing various underwater tasks. Due to the shortcomings resulting from a single AUV's poor detection, information processing and moving capabilities, more and more tasks are completed in a cooperative manner by multiple AUVs. However, most of the existing works do not consider security factors in the process of multi-AUV cooperation. In this paper, we propose a novel cooperative tracking scheme towards an underwater moving target, performed by an intelligent AUV swarm. In this scheme, a cooperative multi-agent reinforcement learning (MARL) based tracking algorithm is proposed following a centralized training with distributed execution (CT-DE) manner. After centralized training in the designed secure private network, no information sharing is required during the mission execution. This feature ensures the security of the whole system, especially in a complex confrontation scenario. In addition, we build models of the AUV underwater dynamics and the target sonar detection, which make the algorithm applicable to real target tracking enabled AUV swarms. Then, based on the multi-agent deep deterministic policy gradient (MADDPG) algorithm, we design an end-to-end AUV control algorithm. Simulation results validate that the proposed algorithm can achieve competitive performance in tracking success rate and tracking stability against baselines, while ensuring the security of the entire system.
\subsubsection{Network Resilience Under Epidemic Attacks: Deep Reinforcement Learning Network Topology Adaptations}
abstract:In this work, we proposed a Deep reinforcement learning (DRL)-based NETwork Adaptations for network Resilience algorithm, namely DeepNETAR, which aims to generate robust network topologies against epidemic attacks. In DeepNETAR, a DRL agent aims to generate a robust network topology against epidemic attacks by removing vulnerable edges or adding least vulnerable edges, given multiple objectives of system security and performance. Most existing network topology adaptation algorithms have used the size of the giant component (SGC) to ensure service availability based on network connectivity. However, in real communication networks, where packets may be dropped either from the presence of inside attackers or congestion on long routes, a larger SGC does not necessarily ensure higher service availability. In addition, for the DRL agent to learn fast and handle multiple system objectives, which are often conflicting to each other, we considered vulnerability-based selection of adaptable edge candidates, fractal-based solution search, and diverse reward functions aiming to achieve multi-objectives optimization. Via extensive simulation experiments, we analyzed what DeepNETAR-based schemes using different objectives can achieve with respect to those two conflicting system objectives along with the comparison of existing and baseline counterparts.
\subsubsection{CVAE-AN: Atypical Attack Flow Detection Using Incremental Adversarial Learning}
abstract:Network Intrusion Detection Systems (NIDS) are powerful tools for identifying and deterring cybersecurity attacks nowadays. However, while these modern IDS can detect typical attacks, recent studies show their poor performances in identifying unknown or dynamically changing atypical attacks. Another issue with the training aspect of such systems is the problem of class imbalance which impedes their performance, especially for minority attack classes. This renders IDS systems vulnerable to both adversarial as well as non-AI synthesized atypical attacks when deployed in a real network. To reduce misclassification (especially for minority classes) and detect atypical attack flows, we propose a novel adversarial incremental learning approach based on a hybrid model consisting of a Conditional Variational Autoencoder (CVAE) and a Generative Adversarial Network (GAN) namely, CVAE-Adversarial Network (CVAE-AN). The proposed binary IDS has been trained using the CICIDS2017 dataset and evaluated using multiple atypical attacks. The simulation results demonstrate that the proposed technique significantly improves the performance of the IDS against different atypical attacks and outperforms the state-of-the-art detection models as well as class balancing methods.
\subsection{AI aided Security 3}
\subsubsection{Deep Learning for Hardware-Impaired Wireless Secret Key Generation with Man-in-the-Middle Attacks}
abstract:Wireless secret key generation (WSKG) allows efficient key agreement protocols for securing the sixth generation (6G) wireless networks. Nevertheless, due to external adversaries or internal impairments, WSKG schemes might become vulnerable during the randomness distillation, where the legitimate nodes try to observe their source of common randomness. In this paper, we investigate the WSKG scheme with legitimate parties suffering from hardware impairments (HIs), while an active adversary acts as a man-in-the-middle (MiM) via injecting fake pilot signals. We first utilize randomized pilots to overcome the MiM. We also leverage the concept of recurrent neural networks (RNNs) to further enhance the randomness distillation. More specifically, the long short-term memory networks (LSTMs)-as a well-established type of RNNs-are implemented to learn the long-term dependencies between the observations of legitimate parties. The achievable secret key rate (SKR) and the impactof MiM on system's performance are analyzed. Our numerical results verify the performance gain of our proposed learning-based approach compared with the state-of-the-art methods and provide useful insights on system design.
\subsubsection{MIRAI Botnet Attack Detection with Auto-Associative Dense Random Neural Network}
abstract:Internet connected IoT devices have often been particularly vulnerable to Botnet attacks of the Mirai family in recent years. Thus we develop an attack detection scheme for Mirai Botnets, using the Auto-Associative Dense Random Neural Network that has recently been successful for other attacks such as the SYN attack. The resulting method is trained with normal traffic and tested with attack traffic, and shown to result in high accuracy detection of attacks with low false alarms. Theapproach is compared on the same data set with two other common Machine learning methods (Lasso and KNN) and shown to have higher accuracy, and much lower computation times than KNN and slightly higher (but comparable) computation timeswith respect to Lasso.
\subsubsection{A Reminiscent Intrusion Detection Model Based on Deep Autoencoders and Transfer Learning}
abstract:Machine learning techniques for network-based intrusion detection often assumes that network traffic does not change over time, or that model updates can be easily performed.In this paper we propose a novel reminiscent intrusion detection model based on deep autoencoders and transfer learning to easiness the model update burden, implemented twofold.First, deep autoencoder is used as an additional feature extraction stage to extract an historical feature representation of network traffic.Second, at model updates, the deep autoencoder parameters are updated through a transfer learning procedure, thus, significantly decreasing the amount of needed labeled training data and the computational costs.Experiments performed on dataset with 8TB of data, containing real and valid network traffic that span for a year, have shown that approaches in the literature are unable to deal with network traffic changes over time, while also demanding unfeasible amounts of labeled data during model training.In addition, if no model updates are performed, the proposed scheme is able to improve the true-negative rate by up to 23.9%, if done so, it is able to provide similar accuracy rates of traditional techniques while demanding only 22% of labeled training data and 28% of computational costs.
\subsubsection{Exploiting Ensemble Learning for Edge-assisted Anomaly Detection Scheme in e-healthcare System}
abstract:With the thriving of wearable devices and thewidespread use of smartphones, the e-healthcare system emergesto cope with the high demand of health services. However, thisintegrated smart health system is vulnerable to various attacks,including intrusion attacks. Traditional detection schemes arenot powerful enough and lack the diversity to identify attacksin complex scenarios. Moreover, the use of cloud-based attackdetection may result in higher detection latency. In this paper,we propose an Edge-assisted Anomaly Detection (EAD) schemeto detect malicious attacks. Specifically, we first identify fourtypes of attackers according to their attacking capabilities. Todistinguish attacks from normal behaviors, we then propose awrapper feature selection method. This selection method eliminates the impact of irrelevant and redundant features so that thedetection accuracy can be improved. Moreover, we investigate thediversity of classifiers and exploit ensemble learning to improvethe detection rate. To reduce high detection latency in the cloud,edge nodes are applied to concurrently implement the proposedlightweight scheme. We evaluate the EAD performance basedon two real-world datasets, i.e., NSL-KDD and UNSW-NB15datasets. The simulation results show that the EAD outperformsother state-of-the-art methods in terms of accuracy and detectionrate, and has a lower detection time than cloud-assisted scheme.
\subsection{Application Security}
\subsubsection{Accurate and Fast Detection of DDoS Attacks in High-Speed Network with Asymmetric Routing}
abstract:The existing DDoS attack detection methods based on a single monitoring point only consider symmetric routing scenarios, which may not be practical. Such schemes will produce high false positives when facing the asymmetric routing scenarios. Besides, few of them are applicable in high-speed networks. The paper designs a DDoS detection scheme customized for high-speed networks and takes asymmetric routing scenarios into account. Systematic sampling is applied to high-speed incoming traffic, and a proposed Double Composite Structure Sketch (DCSS) is utilized for fast recording and extraction of features based on the characteristics of DDoS attacks in both symmetric and asymmetric routing scenarios. Then classifiers are trained for online DDoS detection. Our experimental results using the public dataset show that in a 10Gbps network with asymmetric routing, our approach can accurately detect UDP Flood and SYN Flood attacks within 20 seconds when the sampling rate is set to 1/2048.
\subsubsection{Slider: Towards Precise, Robust and Updatable Sketch-based DDoS Flooding Attack Detection}
abstract:Distributed Denial of Service (DDoS) flooding attacks have been a severe threat to the Internet for decades. These attacks usually are launched by exhausting bandwidth, network resources or server resources. Since most of these attacks are launched abruptly and severely, it is crucial to develop an efficient DDoS flooding attack detection system.In this paper, we present Slider, an online sketch-based DDoS flooding attack detection system. Slider utilizes a new type of sketch structure, namely Rotation Sketch, to effectively detect DDoS flooding attacks and efficiently identify the malicious hosts. Meanwhile, Slider also learns the characteristics of the current network during the time specified by the network operator to periodically update the parameters of its detection model. We have developed a prototype of Slider and the evaluation results on real-world traffic and public DDoS/DoS attack datasets demonstrate that Slider can effectively detect various DDoS flooding attacks with high precision and robustness.
\subsubsection{Stand-in Backdoor: A Stealthy and Powerful Backdoor Attack}
abstract:Lack of transparency in deep learning models makes them vulnerable to backdoor attack, which can cause severe security consequences. For a backdoored model, the specific inputs can trigger misclassification rules while it performs normal behaviors on clean data. Existing backdoor attacks usually generate poisoned data by adding an obvious trigger to the original data and mislabeling them, which suffers from poor invisibility and hence can be easily detected. In this paper, we propose Stand-in Backdoor, a more stealthy and powerful backdoor attack, which can completely hide the trigger while maintaining correct labels of poisoned data. Specifically, we design a novel optimization strategy to transform triggers into imperceptible perturbation in the feature space. Furthermore, utilizing the transferability of feature perturbation, we fine-tune the victim model with well-constructed poisoned data that are correctly labeled. Extensive experiments conducted on various image classification tasks demonstrate that our attack outperforms the state-of-the-art work in terms of backdoor stealth and attack performance, without sacrificing the model's utility.
\subsubsection{DOLPHIN: Phonics based Detection of DGA Domain Names}
abstract:Botnets are the machines that increasingly controlled by cyber criminals to perform various attacks. They use Domain Generation Algorithm (DGA) to frequently generate their illegitimate domains for preventing detection. To overcome such dynamics, existing solutions try to capture the characteristics of domain names, such that the automatically generated domains can be identified. However, those solutions are not conform to the linguistic conventions of reading and writing. For a comprehensive understanding of strings of domain names, we present DOmain Linguistic PHonIcs detectioN (DOLPHIN), a novel method that can detect the illegitimate domain names generated by DGAs. Considering the correspondence between pronunciations and spellings, we design the DOLPHIN patterns. They are the classification of vowels and consonants in variable lengths as follow the principles of phonics. DOLPHIN recognizes strings of domain names and reconstructs them with the components of variable-length vowels and consonants following the DOLPHIN patterns. We implement the features used DOLPHIN in supervised learning methods and compare them to the foremost method FANCI. Experimental results show that, compared to FANCI with RFs, DOLPHIN is able to achieve higher detection accuracy of 0.0238 in average with lower FPR without much overhead.
\subsubsection{Measuring Adoption of DNS Security Mechanisms with Cross-Sectional Approach}
abstract:The threat of attacks targeting a DNS, such as DNS cache poisoning attacks and DNS amplification attacks, continues unabated. In addition, attacks that exploit the difficulty in determining the authenticity of domain names, such as phishing sites and fraudulent emails, continue to be a significant threat.Various DNS security mechanisms have been proposed, standardized, and implemented as effective countermeasures against DNS-related attacks.However, it is not clear how widespread these security mechanisms are in the DNS ecosystem and how effectively they work in the wild. With this background, this study targets the major DNS security mechanisms deployed for the DNS name servers, DNSSEC, DNS Cookies, CAA, SPF, DMARC, MTA-STS, DANE, and TLSRPT, and a large-scale measurement analysis of their deployment is conducted.Our results quantitatively reveal that, as of 2021, the adoption rate of most DNS security mechanisms, except SPF, remains low, and the adoption rate is lower for mechanisms that are more difficult to configure.These findings suggest the importance of developing easy-to-deploy tools to promote the adoption of security mechanisms.
\subsection{Blockchain}
\subsubsection{Blockchain-based system for e-voting using Blind Signature protocol}
abstract:Reports of possible interference in the voting process by third parties, unauthorized voting, disenfranchisement, and technological failures raise questions about the transparency of elections around the world. At the same time, e-voting has partially replaced the traditional paper ballot-based system, with the advantages of high human work efficiency, low cost, and low probability of errors in the process. However, their use has not been extended due to the security issues of the developed solutions. Blockchain technology has been proposed to supporte-voting processes to provide integrity, resilience, and verifiability of votes, but by itself, Blockchain is not able to guarantee the anonymity and privacy required by voters. In this paper, we propose a voting mechanism that uses Blockchain and a cryptographic Blind Signature protocol to guarantee anonymity and privacy to voters. The performance in terms of computational requirements and scalability is evaluated via simulation. The analysis of simulation results shows that the Blockchain-based voting mechanism proposed in this paper achieves good performance in terms of computational cost and scalability.
\subsubsection{Blockchain-based secure Handover for IoT using Zero-Knowledge Proof protocol}
abstract:Currently, the Internet of Things (IoT) is widely used by the emerging of internet-integrated wireless devices in daily life. In order to communicate and exchange data with each other, IoT devices must pass by a gateway that belongs to the same network. But, considering the mobility of such devices and their constraints, it becomes difficult to trust and to connect from a gateway to another. Therefore, this paper introduces a blockchain-based secure handover protocol. It ensures an anonymous mutual authentication solution between a mobile IoT device and a visited gateway by using Zero-Knowledge Proof (ZKP) protocol. To do, simulations have been performed thanks to a discrete events simulator. In addition to a good security level, compared with the most used protocol (DTLS), our simulation results lead to better performance in terms of temporal complexity, energy consumption, and communication cost.
\subsubsection{On Strategic Interactions in Blockchain Markets: A Three-stage Stackelberg Game Approach}
abstract:Blockchain technology is a promising approach for solving the security and personal privacy problems in Internet applications. The successful commercial deployment of Blockchain markets relies on a comprehensive understanding of the economic and strategic interactions among different entities involved. In this paper, we focus on a blockchain market consisting of a blockchain platform (BP), multiple miners, and blockchain users (BUs), and formulate their interactions as a three-stage Stackelberg game. In Stage I, the BP strategizes the rewards granted to the miners, so as to attract the miners to contribute more computing power used for improving the security and privacy of the blockchain. In Stage II, each miner strategizes its computing power individually for winning the mining competition, which is modeled as a non-cooperative game. In Stage III, the BUs strategize the transaction fee to acquire a corresponding service experience. With the objective of utility maximization, we develop a theoretical framework to analyze the hierarchical interactive behaviors among the entities in a backward inductive way. By solving the Stackelberg equilibrium, we determine the optimal strategies of entities in closed-form. Numerical results are provided to demonstrate the performance of the strategic interactions in the blockchain market.
\subsubsection{Trust-based gateway selection in a multi-tiers blockchain architecture}
abstract:In a multi-tiers blockchain (BC) architecture, we find heterogeneous BC with different security levels that need to communicate. This communication is ensured by node on each BC tier, called the gateway. This node has a sensitive role in the network, it represents a weak point and a source of attacks propagation between networks. In this paper, we focus on the selection of the gateway node using a distributed trust model to evaluate nodes behaviors inside each BC network. The trust model is inspired from the Spence's model where signals, cost and rewards are used to incite nodes to collaborate and to act honestly. In addition, we introduce penalties for nodes acting maliciously or being selfish. Moreover, the gateway undergoes severe penalties in case he changes his behavior after election. Nodes can complain about the current gateway, their complaints are considered if they are trustworthy and quite numerous. Then, we propose a 2 phases algorithm based on the proposed trust model for a dynamic selection of the gateway node. The 2 phases ensure the election of an honest node with the optimal performances. Finally, we establish a security analysis of our solution to prove it's ability to detect security attacks and weakness like the presence of malicious nodes, malicious gateway, nodes with a changing behavior, a DDOS attack on the gateway, and unauthorized access to the network.
\subsubsection{Vicinity-based Consensus: A Fast in-Neighborhood Convergence Consensus Mechanism for Blockchain}
abstract:Private blockchains tend to apply deterministic consensus mechanisms as a more efficient alternative to the proof-based consensus. Deterministic mechanisms tolerate two types of failures, byzantine and crash-fault. Byzantine fault-tolerant consensus assumes restrictive assumptions of time and number of failures to guarantee validity, while the termination depends on message broadcasting among nodes. Crash-Fault tolerant consensus is less rigorous to ensure termination and higher throughput while sacrificing agreement. This paper proposes a lightweight consensus mechanism based on vicinity voting with confirmed message broadcasting. Formation rules in the neighborhoods of the peer-to-peer network relax the trade-off between agreement and termination. Experimental results show that the proposal guarantees agreement and termination in case of more permissive formation rules. Besides, the cost of achieving consensus is reduced by up to 46% in more rigorous formation rules with limited impact on termination and agreement.
\subsection{Communications Security}
\subsubsection{Insider-Resistant Context-Based Pairing for Multimodality Sleep Apnea Test}
abstract:The increasingly sophisticated at-home screening systems for obstructive sleep apnea (OSA), integrated with both contactless and contact-based sensing modalities, bring convenience and reliability to remote chronic disease management. However, the device pairing processes between system components are vulnerable to wireless exploitation from a non-compliant user wishing to manipulate the test results. This work presents SIENNA, an insider-resistant context-based pairing protocol. SIENNA leverages JADE-ICA to uniquely identify a user's respiration pattern within a multi-person environment and fuzzy commitment for automatic device pairing, while using friendly jamming technique to prevent an insider with knowledge of respiration patterns from acquiring the pairing key. Our analysis and test results show that SIENNA can achieve reliable (> 90% success rate) device pairing under a noisy environment and is robust against the attacker with full knowledge of the context information.
\subsubsection{Joint Power and Channel Allocation for Safeguarding Cognitive Satellite-UAV Networks}
abstract:Outside the coverage of terrestrial cellular networks, non-terrestrial infrastructures, e.g., satellites and unmanned aerial vehicles (UAVs), should be utilized, to efficiently cover the remote areas. This requires a cognitive satellite-UAV network, where satellites and UAVs share the spectrum to save cost, and the network resources are orchestrated in an on-demand manner. In this paper, we focus on the physical layer security issue of the cognitive satellite-UAV networks, which is important due to the openness of both satellite links and UAV links. We formulate a joint power and channel allocation problem, using only the slowly-varying large-scale channel state information (CSI), to maximize the sum secrecy rate of UAV users. By resorting to the random matrix theory, the max-min optimization tool, as well as the bipartite graph matching algorithm, we propose a sub-optimal low-complexity solution, the superiority of which is verified by simulation results.
\subsubsection{Fundamental Limits of Activity-Based Covert Channels}
abstract:Covert communication considers the ability of transmitter Alice to communicate reliably to intended receiver Bob without being detected by adversary warden Willie. One collection of approaches to covert signaling is for Alice to alter the state of a system in such a way that the altered state conveys information to Bob. Motivated by recent work on the foundations of covert communications that has largely considered the physical layer, we provide a fundamental characterization of one approach to covert signaling via activity: employing a codebook pre-shared with Bob, Alice encodes a message by selecting from the codebook the appropriate pattern of slots to insert innocuous packets into a slotted ALOHA system in the presence of other users. The intended recipient Bob detects patterns in the activity of the slotted ALOHA system to determine which codeword was sent. We provide a fundamental analysis of the performance of such a system under a covertness constraint. First, we consider signaling schemes derived specifically for the proposed channel when Bob or Willie has various abilities to discern the number of packets in a given slot. Given the challenges in such design, we next recognize that techniques from optical communications, although designed for a different channel, can potentially be employed and thus yield a large class of schemes that provide lower bounds on the achievable rate. Numerical results are provided to support the analytical development and to demonstrate the potential of covert signaling through such an approach.
\subsubsection{Perfect Secrecy in the Bounded Storage Model}
abstract:In this paper, we propose a new provably secure cryptosystem for two party communication that provides security in the face of new technological breakthroughs. Most of the practical cryptosystems in use today can be breached in future with new sophisticated methods. This jeopardizes the security of older but highly confidential messages in the future.Our protocol is based on the bounded storage model first introduced in [1]. The protocol is secure as long as there is bound on the storage, however large it may be. Our protocol is an improvement over previously known protocols and uses short key and optimal number of public random bits. The size of both the keys and public random string is independent of the message length and only depends on the security parameter and bound on the storage of adversary. The smaller length of key and public random string makes the scheme practical. The protocol generates key uses starting values and primitives of group Z_n to generate key. While the starting values are sampled uniformly, primitives can be sampled using any probability distribution making our protocol very general and simple to implement. Our protocol is a step forward in making provably secure cryptosystem practical. An important open problem raised in [1] was designing an algorithm with short key and size of public random string O(n) where n bounds the storage of adversary.
\subsubsection{Optimization for IRS-Assisted Systems With Both Multicast and Confidential Messages}
abstract:In this paper, we propose to apply intelligent reflecting surface (IRS) to the physical-layer service integration (PHY-SI) system, where a single-antenna access point (AP) integrates two sorts of service messages, i.e., multicast message and confidential message, via superposition coding to serve multiple single-antenna users. Our goal is to optimize the power allocation (for transmitting different messages) at the AP and the passive beamforming at the IRS to maximize the achievable secrecy rate region. To this end, we formulate this problem as a bi-objective optimization problem. To tackle the non-convexity of this problem, we propose a Charnes-Cooper transformation (CCT)-based algorithm to obtain its high-quality suboptimal solutions, thereby approximately characterizing the secrecy rate region. Numerical results demonstrate the advantages of leveraging IRS in improving the performance of PHY-SI.
\subsection{IoT Security 1}
\subsubsection{Securing IoT Transactions Against Double-Spending Attacks based on Signaling Game Approach}
abstract:With considerable growth in demand for higher throughput, greater capacity, and lower latency for consumers, the Internet of Things (IoT) network is anticipated to meet the desired security and privacy requirements. This study provides high transaction throughput on the critical IoT applications, particularly Bitcoin security against double-spending attacks. To this end, we investigated the signaling game approach to model the interaction between two miners while considering players' behavior (malicious or honest miners) and the incoming transaction throughput. To the best of our knowledge, this is the first work that exploits the signaling game to cover the incoming transactions randomness waiting for validation, which influences the honest miners' behavior. With extensive simulations, we show that our proposed signaling game allows to avoid double-spending attacks performed by weaker attackers. The results illustrate also the benefit of using the signaling game to model the interaction between two miners while handling the incomplete information of the incoming transactions and the type of miners.
\subsubsection{Hijacking Downlink Path Selection in LoRaWAN}
abstract:With the rise of the IoT, many protocols have been developed in order to fulfill the need for a wireless connectivity that assures energy efficiency and low-data rates. LoRaWAN is certainly one of the most widely used protocols. The LoRaWAN 1.1 specification aims to fix some serious security vulnerabilities in the 1.0 specification, however there still exist critical points to address. In this paper, we identify an attack that can affect LoRaWAN 1.0 and 1.1 networks, which hijacks the downlink path from the Network Server to an End Device. The attack exploits the deduplication procedure and the gateway selection during a downlink scheduling by the Network Server, which is in general implementation-dependent.The attack scheme has been proven to be easy to implement, not requiring physical layer-specific operations such as signal jamming, and could target many LoRaWAN devices at once. We discuss the implications of this attack and identify the possible mitigations that could be adopted by network providers to address this vulnerability.
\subsubsection{Cost-Efficient Blockchain-Based Access Control for the Internet of Things}
abstract:Blockchain-based access control (BBAC) has been highly promising to prevent unauthorized resource access in the Internet of Things (IoT). However, maintaining BBAC can be potentially expensive due to the storage cost of the blockchain.To address this issue, we propose a layered BBAC architecture by combining blockchain with blockchain oracle and tamper-proof decentralized storage (e.g., IOTA). The proposed architecture consists of three main layers: a blockchain layer, which provides distributed and trustworthy access control, a storage layer, which stores meta data (e.g., subject/object attributes and policies) used in the access control of the blockchain layer, and an oracle layer, which works as a bridge to help transfer data between the blockchain and decentralized storage. This architecture achieves robust, auditable, and cost-efficient access control by migrating the meta data from the blockchain to the decentralized storage while keeping the fascinating tamper-proof feature of the blockchain. We implement and evaluate this architecture in terms of time and monetary cost to demonstrate its feasibility and superiority over existing ones.
\subsubsection{Secure Load Balancing for UAV-Assisted Wireless Networks}
abstract:The unbalanced traffic distribution is a severe problem in cellular networks, which leads to congestion and reduces spectrum efficiency. To tackle this problem, we propose an unmanned aerial vehicle (UAV)-assisted wireless network architecture in which UAV acts as relay to divert the traffic from the overloaded cell to its neighbor underloaded cell. Considering that UAV communications are easily eavesdropped, we use the secrecy capacity to evaluate the performance of the network. To fully exploit the advantages of the proposed architecture, we formulate a joint UAV position optimization, user association, and time allocation problem to maximize the sum-log-rate of all users in two adjacent cells. To tackle the complicated joint optimization problem, we first design a genetic-based algorithm to optimize the UAV position, and then use the branch-and-bound method to devise a low-complexity algorithm to get the optimal user association and time allocation schemes. The simulation results indicate that the proposed UAV-assisted wireless network architecture is superior to the terrestrial network, and the proposed algorithms can further improve the network performance in comparison with the other schemes.
\subsubsection{Delay Aware Secure Computation Offloading in NOMA aided MEC for IoV Networks}
abstract:In this paper, we investigate a multi-vehicle multi-task non-orthogonal multiple access (NOMA) based mobile edge computing (MEC) system with passive eavesdropping vehicles. To enhance the performance of edge vehicles, we propose a vehicle grouping and pairing method (GPM) to employ the vehicle near to MEC acting as a full-duplex (FD) relay to assist edge vehicles. In order to improve the transmission security, artificial noise (AN) is used to interfere with eavesdropping vehicles. The approximate expression of secrecy outage probability of the system (SOPS) is derived in closed form. This paper aims to minimize the total delay of task completion of edge vehicles by jointly optimizing vehicle task division, power allocation and transmit beamforming. We design a power allocation and task scheduling algorithm relying on genetic algorithm (GA-PATS) to solve our formulated mixed-integer non-linear programming (MINP) problem. Numerical results demonstrate the superiority of our proposed scheme in the aspect of system security and transmission delay.
\subsection{IoT Security 2}
\subsubsection{Secrecy Rate Maximization with Gridded UAV Swarm Jamming for passive Eavesdropping}
abstract:This paper considers the grid formation of an unmanned aerial vehicle (UAV) swarm for maximizing the secrecy rate in the presence of an unknown eavesdropper. In particular, the UAV swarm performs coordinated beamforming onto the null space of the legitimate channel to jam the eavesdropper located at an unknown location. By nulling the channel between the legitimate receiver and the UAV swarm, we obtain an optimal trajectory and jamming power allocation for each UAV enabling wideband single ray beamforming to improve the secrecy rate. Results obtained demonstrate the effectiveness of the proposed UAV-aided jamming scheme as well as the optimal number of UAVs in the swarm necessary to observe a saturation effect in the secrecy rate. We also show the optimal radius of the unknown but constrained location of the eavesdropper.
\subsubsection{VehicleCIDS: An Efficient Vehicle Intrusion Detection System Based on Clock Behavior}
abstract:Nowadays, more and more external interfaces are added into intelligent and connected vehicles. The in-vehicle network, especially the controller area network (CAN), is no longer a closed environment, which provides more approaches for attackers to invade. To resist attacks, numerous researchers have proposed intrusion detection systems (IDSs). However, attackers can intrude CAN bus in a more advanced way, such as masquerade attack, which leads to failures of most IDS. To counter masquerade attacks, we propose an efficient vehicle IDS based on clock behavior, called VehicleCIDS. First, the system uses recursive least squares (RLS) algorithm to estimate the clock behavior of each electronic control unit (ECU). Then, a statistical method called empirical rule is used to detect attack messages. Finally, it utilizes dynamic time warping (DTW) to identify attackers. The experimental results on real vehicles show that the recognition rate of VehicleCIDS can achieve 98.52% in intrusion detection and 87.71% in attacker identification.
\subsubsection{Improper Gaussian Signaling Based Covert Wireless Communication in IoT Networks}
abstract:Covert communication, which can hide the communication behavior, has great potential in guaranteeing the security of information and transmission terminal to the greatest extent. In this paper, we propose a covert communication strategy based on improper Gaussian signaling (IGS) to increase the covert rate in the Internet of Things (IoT) system, in which the existing signals emitted by other transmitters were used as "Spectrum Shelter" to cover the IoT's communication. Specifically, we analyze the system's achievable transmission rate when the IoT node adopts IGS. Then, the optimal detection threshold and the minimum error detection probability of the warden are analyzed. Next, by jointly optimizing the transmission power and circularity coefficient of IGS, we maximize the covert rate under the constraints of the covertness. Finally, the simulation results verify that the proposed scheme can not only guarantee the covertness, but also improve the achievable covert rate.
\subsubsection{Deep Learning Based Device Classification Method for Safeguarding Internet of Things}
abstract:With the rapid development of 5G networks, a great amount of Internet of Things (IoT) devices are connected to the Internet. Most of these devices are cost limited and thus are easily compromised by attackers to launch distributed denial of service (DDoS) attacks. The traditional DDoS defense methods at server side can not adapt to this new challenge, thus access-side DDoS detection architecture is urgently needed. In this paper, we propose a deep learning (DL) based IoT device classification method to support fine-grained behavior modeling of malicious traffic and thus enable access-side DDoS detection. Different from traditional studies based on machine learning (ML) which need expertise feature engineering, we propose a time characteristics extraction method based on 1-D convolutional neural network to capture high level time series features automatically for better classification performance. To avoid the feature loss problem, we propose a feature enhancement method based on residual connection module. Experimental results verify the effectiveness of our method, which offers a meaningful gain in terms of both accuracy and macro F1 score over existing approaches.
\subsection{Physical Layer Security 1}
\subsubsection{Joint Transmit Precoding and Reflect Beamforming for IRS-Assisted MIMO-OFDM Secure Communications}
abstract:The effective combination of physical layer security communication and intelligent reflecting surface (IRS) technology has recently attracted extensive attention to improve the system security. Unlike existing works that mostly focus on single-carrier systems, we consider an IRS-assisted multi-carrier MIMO wireless physical layer security communication system, which consists of a legitimate transmitter, a legitimate receiver, an IRS node and an eavesdropper. With the aim of maximizing the sum secrecy rate, the precoding matrix and IRS reflecting coefficient matrix were jointly optimized under the constraints on the budget of the transmit power and unit modulus of IRS reflecting coefficients. An alternate optimization (AO) based inexact block coordinate descent (IBCD) algorithm was proposed to tackle the non-convexity of the formulated problem, where the Lagrange multiplier method and complex circle manifold (CCM) method were adopted to solve the subproblems and then closed-form solutions were obtained at each iteration. Finally, the simulation results validate the effectiveness of the proposed beamforming schemes.
\subsubsection{Hybrid Beamforming Design for Covert Multicast mmWave Massive MIMO Communications}
abstract:Rather than considering only one legitimate user as in the existing works, we investigate multiple legitimate users served by Alice using multicast millimeter wave communications in this paper. Hybrid beamformers for the max-min fairness problem are designed to maximize the minimum covert rate between Alice and the legitimate users subject to the power constraint for confidential signal (CS) and the covertness constraint. In particular, the fully-digital beamformers for the CS and jamming signal are designed by temporarily neglecting the hardware constraints from the constant envelop for phase shifters and the limited number of RF chains, where a semi-definite programming-based method and a successive convex approximation (SCA)-based method are proposed. To approach the fully-digital beamformers, hybrid beamformers are designed subject to the hardware constraints, where an alternating minimization method is proposed to iteratively optimize the analog and digital beamformers. Simulation results show that the proposed methods can achieve better covert communication performance than the existing methods.
\subsubsection{On the Secrecy Rate under Statistical QoS Provisioning for RIS-assisted MISO Wiretap Channel}
abstract:Reconfigurable intelligent surface (RIS) assisted radio is considered as an enabling technology with great potentialfor the sixth-generation (6G) wireless communications standard.The achievable secrecy rate (ASR) is one of the most fundamentalmetrics to evaluate the capability of facilitating secure communication for RIS-assisted systems. However, the definition ofASR is based on Shannon's information theory, which generallyrequires long codewords and thus fails to quantify the secrecyof emerging delay-critical services. Motivated by this, in thispaper we investigate the problem of maximizing the secrecy rateunder a delay-limited quality-of-service (QoS) constraint, termedas the effective secrecy rate (ESR), for an RIS-assisted multiple-input single-output (MISO) wiretap channel subject to a transmitpower constraint. We propose an iterative method to find astationary solution to the formulated non-convex optimizationproblem using a block coordinate ascent method (BCAM), whereboth the beamforming vector at the transmitter as well as thephase shifts at the RIS are obtained in closed forms in eachiteration. We also present a convergence proof, an efficientimplementation, and the associated complexity analysis for theproposed method. Our numerical results demonstrate that theproposed optimization algorithm converges significantly fasterthat an existing solution. The simulation results also confirmthat the secrecy rate performance of the system with stringentdelay requirements reduces significantly compared to the systemwithout any delay constraints, and that this reduction can besignificantly mitigated by an appropriately placed large-size RIS.
\subsubsection{Anti-Jamming in Cell Free mMIMO systems}
abstract:Recently, cell-free massive multiple-input multiple-output (mMIMO), a distributed version of mMIMO, has increasingly been deployed to increase the spectral and energy efficiency of communication systems. In this paper, we investigate the ability of using cell-free mMIMO to reduce the jamming attacks which is a critical issue for communication. We propose a full-stack framework from detecting to suppressing jamming attacks in cell-free mMIMO system. At first, we exploit the unused pilots to design a jammer detector based on the likelihood functions of the measured signals. The, we propose a jamming suppression method including two tasks: jamming estimation and access point (AP) selection. In the first task, we estimate the phase and amplitude when projecting the received signals onto the unused pilots, and then use them to estimate the jamming signal. In the second task, we employ the neural network-contextual multi-armed bandit (NN-CMAB) for online selection of APs which can provide the multi-user spatial diversity considering the existence of the jammers. Furthermore, we also propose a power control strategy for managing the minimum rate requirement in multi-user settings. Numerical results confirm the advantages of proposed designs over conventional jamming-ignorant-LMMSE strategy in spectral efficiency.
\subsubsection{Secrecy of Massive MIMO Systems Under Antenna Failure}
abstract:In this paper, we study the secrecy performance of massive multiple-input multiple-output (MIMO) system for a scenario in which a number of BS antennas fail to send/receive any signals rendering them to be a source of additive distortion noise. The secrecy model studied here is based on multiple-input single-output single-eavesdropper (MISOSE) wiretap channel in which a single-antenna eavesdropper performs pilot contamination attack on a target user in order to increase its decodability of the confidential message intended to the target user. The lower bound of the achievable ergodic secrecy rate was derived and a closed form expression for the maximum ratio (MR) precoder followed.Results show that for the given MISOSE wiretap channel positive secrecy rate is achievable even with high failure rate.The study also shows that massive MIMO systems with spatially correlated channels yield higher secrecy rate than with uncorrelated spatial channels albeit the earlier showed to be more impacted by BS antenna failure.
\subsection{Physical Layer Security 2}
\subsubsection{Experimental Evaluation of a Modular Coding Scheme for Physical Layer Security}
abstract:In this paper we use a seeded modular coding scheme for implementing physical layer security in a wiretap scenario. This modular scheme consists of a traditional coding layer and a security layer. As traditional coding layer, we use a polar code. We evaluate the performance of the seeded modular coding scheme in an experimental setup with software defined radios and compare it to simulation results. In order to assess the secrecy level of the scheme, we employ the distinguishing security metric. In our experiments, we compare the distinguishing error rate for different seeds and block lengths.
\subsubsection{Physical Layer Security for V2I Communications: Reflecting Surfaces Vs. Relaying}
abstract:Wireless vehicular network (WVN) is exponentially gaining attention from industries and researchers since it is the Keystone of intelligent transportation systems (ITS). Despite the sophisticated features and services that it can offer, it is susceptible to networking attacks such as eavesdropping threats where the confidential transmitted signal could be overheard by a malicious entity. In this paper, we intend to study the physical layer security (PLS) where we consider the eavesdropping attack for vehicle-to-infrastructure (V2I) communications. We analyze the average secrecy capacity, under different scenarios by comparing the performances of employing the decode-and-forward (DF) relay, the amplify-and-forward fixed gain (AFFG) relay, and the intelligent reflecting surface (IRS). Actually, this comparison investigates the efficiency of IRS comparing to the traditional relaying systems, since it was introduced as a novel paradigm in wireless technology with highly promising potential, especially in 5G and 6G.
\subsubsection{Simplified Denoising for Robust Specific Emitter Identification of Preamble-based Waveforms}
abstract:Internet of Things (IoT) deployments continue to grow at an accelerated rate, thus presenting a growing surface over which nefarious actors can conduct attacks. This disturbing revelation is exacerbated by the fact that roughly 70% of all IoTdevices employ weak or no encryption. Deep learning (DL)-based Specific Emitter Identification (SEI) has been put forward as a possible approach by which to secure IoT devices and related infrastructures. This work presents a DL-based SEI approachthat remains robust under degrading signal-to-noise ratio (SNR) conditions while greatly reducing the complexity that is typically associated with DL-based approaches. The presented approach achieves an average percent classification performance of 97% or higher for SNR values greater than or equal to 6 dB.
\subsubsection{Precoding-Based Mode Hopping for Anti-Jamming}
abstract:Owing to the spatial orthogonality, orbital angular momentum (OAM), which describes the helical phase fronts of electromagnetic waves, shows an extensive application prospect for increasing the spectrum efficiency and enhancing the physical layer security of wireless communications. However, efficient anti-jamming results of existing mode hopping (MH) schemes are achieved with pre-shared hopping sequences. Such MH schemes require the strict synchronization between the legitimate transmitter and receiver, thus leading to complex system design. Also, pre-shared hopping sequences greatly limit the MH application scenarios. To solve the problem of no pre-shared hopping sequences between the legitimate transmitter and receiver for anti-jamming, in this paper we propose the precoding-based mode hopping (PoM) scheme. Specifically, we design the transmitter by dividing the input information into the index information and signal information. Based on the index information, the activated OAM-modes for hopping and the phase shift for precoding are determined. OAM-mode and phase shift randomization make jammers difficult to disrupt the communication. Then, the receiver with OAM decomposition and phase shift decoding is designed to extract legitimate signals. Also, the lower bond of achievable rate for our proposed PoM scheme is derived. Numerical results show that our proposed PoM scheme is superior to the conventional index-modulation based mode division multiplexing (IM-MDM) schemes in terms of achievable rate under hostile jamming.
\subsubsection{Covert communication via dynamic spectrum control-assisted transmission scheme}
abstract:To realize secure communication and prevent eavesdroppers from detecting the existence of communication activities, covert communication has attracted substantial research interests. In this paper, we propose a dynamic spectrum control (DSC)-assisted scheme to achieve covert and reliable data transmission. Specifically, by constructing time-frequency division channels, the proposed DSC-assisted scheme generates sequences with iterative and orthogonal transformations. Authorized users can orderly occupy different frequency slots in each time slot under the guidance of these sequences, thus achieving simultaneous data transmission without interfering with each other. Then, the covert performance of the proposed transmission scheme is analyzed to provide the closed-form expressions of covert transmission rate and the reliable transmission probability. Simulation results are provided to validate the accuracy of the theoretical analysis and demonstrate that the proposed scheme can achieve better covert and reliable transmission performances when compared with the existing scheme.
\subsection{Privacy}
\subsubsection{Towards Privacy-Preserving Online Medical Monitoring with Reverse Skyline Query}
abstract:With the flourish of Wireless Body Area Network (WBAN), the online medical monitoring system has attracted extensive attention. Meanwhile, due to the limited resources, the hospital tends to outsource the medical services to the cloud and requires the patients' data to be encrypted before uploading. It is bound to raise a challenge in data availability, e.g., the reverse skyline query that is widely used in monitoring systems. In this paper, we propose a privacy-preserving online medical monitoring system, in which the cloud can answer the reverse skyline query over encrypted data and return the monitored data of high-risk patients to a doctor. To achieve this goal, we first design four secure protocols that can ensure the security of operands while minimizing the communication costs between two cloud servers. Based on these privacy-preserving protocols, we propose two privacy-preserving reverse skyline query schemes that can be used in the monitoring system. Security analysis shows that our proposed scheme is indeed privacy-preserving, and performance evaluations also demonstrate the efficiency of our scheme in terms of computation and communication.
\subsubsection{A Fog-Aided Privacy-Preserving Truth Discovery Framework over Crowdsensed Data Streams}
abstract:With the proliferation of mobile and wearable devices, mobile crowdsensing (MCS) is becoming a new paradigm for data collection and analysis. To effectively identify truthful information from crowdsensed data without privacy leakage, privacy-preserving truth discovery (PPTD) has gained much attention recently. Existing works either didn't consider real-time applications over data streams or failed to achieve enough efficiency for a large group of workers. In this paper, we propose FPTD, a Fog-aided Privacy-preserving Truth Discovery framework which is secure and efficient in handling a large group of workers for real-time applications. To reduce overhead, we adopt cloud-fog computing architecture to divide the complete worker group into many smaller ones. Then we design a unique secure aggregation protocol SecAgg which can securely and efficiently aggregate inputs from multiple worker groups. Finally, we give detailed construction of FPTD, an efficient truth discovery framework based on SecAgg for real-time applications. Through extensive experiments and security analysis, we demonstrate that both SecAgg and FPTD are secure and efficient.
\subsubsection{Data Privacy Protection based on Feature Dilution in Cloud Services}
abstract:Machine learning as a service (MLaaS) brings many benefits to people's daily life. However, the service mode of MLaaS will increase the risk of users' privacy leakage. Existing works focusing on privacy-preserving based on encryption, differential privacy, and distributed framework require high computing resources or cannot be applied in MLaaS. In this paper, we propose feature dilution (FD), a noise-based desensitization algorithm to remove sensitive information in raw data. In particular, FD continuously adds features of raw data to the random noise until it meets the minimum amount for an effective query, and we name this noise as weak-feature noise (WFN). By fine-tuning the MLaaS architecture, we have realized that users can utilize WFN to get normal services without exposing their local private data. Meanwhile, noise addition technology is introduced by us to reduce the risk of privacy leakage caused by "weak features". Extensive experiments have demonstrated that users can use FD to obtain effective services without exposing their private data. Finally, we conducted practical tests on weak-feature noises and found that these noises are difficult to use by malicious service providers.
\subsubsection{Privacy-Preserving Friend Matching for Mobile Social Networks}
abstract:In this paper, we propose an efficient private set intersection protocol, named LL-PSI, to enable two parties (where each party has an individual set) to obtain the intersection of their sets without leaking other information about their sets to each other. Compared with existing protocols, LL-PSI reduces the computational latency of the intersection between two sets significantly at the expense of communication costs between the parties. Based on LL-PSI, we propose a privacy-preserving friend matching scheme for mobile social networks, dubbed PAIRING. PAIRING allows users to match with those who have common interests while preserving users' private information against the malicious server and curious users. We analyze the security of PAIRING and conduct a comprehensive performance evaluation, which demonstrates that PAIRING is secure and efficient.
\subsubsection{Privacy-Preserving Truth Discovery for Sparse Data in Mobile Crowdsensing Systems}
abstract:Truth discovery is an effective method to infer truthful information from a large amount of sensory data in mobile crowdsensing systems. Privacy-preserving truth discovery schemes require the cloud server not to access each worker's sensory data directly so that the privacy of sensory data can be preserved. In some specific applications such as sparse mobile crowdsensing, workers can only contribute sensory data on a small part of sensing tasks, implying that the information of which tasks are completed by a worker should also be preserved. However, existing privacy-preserving truth discovery schemes do not consider such sparse data scenarios in mobile crowdsensing systems. In this paper, we first identify the privacy issues in truth discovery when sensory data are sparse. To address these issues, we design a privacy-preserving truth discovery scheme by employing the additively homomorphic cryptosystem and additive secret sharing with two non-colluding servers. Through detailed analysis and extensive experiments, we demonstrate that our proposed scheme can satisfy strong privacy-preserving requirements with low computation and communication overhead.
\subsection{SDN & Computing Security}
\subsubsection{Game-Theoretic Intrusion Prevention System Deployment for Mobile Edge Computing}
abstract:The network attack such as Distributed Denial-of-Service (DDoS) attack could be critical to latency-critical systems such as Mobile Edge Computing (MEC) as such attacks significantly increase the response delay of the victim service. Intrusion prevention system (IPS) is a promising solution to defend against such attacks, but there will be a trade-off between IPS deployment and application resource reservation as the deployment of IPS will reduce the number of computation resources for MEC applications. In this paper, we proposed a game-theoretic framework to study the joint computation resource allocation and IPS deployment in the MEC architecture. We study the pricing strategy of the MEC platform operator and purchase strategy of the application service provider, given the expected attack strength and end user demands. The best responses of both MPO and ASPs are derived theoretically to identify the Stackelberg equilibrium. The simulation results confirm that the proposed solutions significantly increase the social welfare of the system.
\subsubsection{Privacy-Preserving Fog-Based Multi-Location Task Allocation in Mobile Crowdsourcing}
abstract:As a wave of the rapidly approaching future, vehicles are becoming increasingly "smarter" via equipping with abundant resources for data sensing, processing, and transmitting. To fully make use of these resources, mobile crowdsourcing applications have attracted particular interests from academia and industry, and they are extensively integrated with fog computing for obtaining low latency and location sensitivity. In this paper, we consider a fog-based task allocation service for mobile crowdsourcing tasks with multiple locations, where a task is allocated to the worker whose future trajectory has the smallest Hausdorff semi-distance to the task locations. However, as fog nodes are not fully trusted, there may exist privacy concerns related to the workers and the task owners. To the best of our knowledge, although Hausdorff semi-distance has been applied in various applications, none of the existing works can support privacy-preserving Hausdorff semi-distance evaluation or k nearest neighbor queries. Aiming at this issue, we design a privacy-preserving fog-based multi-location task allocation scheme. Specifically, based on a symmetric homomorphic encryption technique, we build two privacy-preserving protocols for i) computing min/max value from an array and ii) retrieving the key linked to the minimum value from an array of key-value pairs. Then, the proposed scheme is built upon these two protocols. After that, we conduct rigid security analysis and extensive experiments to demonstrate the security and efficiency of our proposed scheme, respectively. The results indicate that our proposed scheme is not only privacy-preserving, but alsoefficient in terms of both computation and communication costs.
\subsubsection{CoWatch: Collaborative Prediction of DDoS Attacks in Edge Computing with Distributed SDN}
abstract:With the development of Edge Computing (EC), security issues have raised concerns. Due to the unusual vulnerability of EC servers and the distributed nature of attack sources, it is a great challenge to efficiently and effectively defend against DDoS attacks. Existing detection solutions, based on the feedback of servers under the attacks, can incur high bandwidth costs and degradation of service performance. To address this problem, we propose a novel collaborative prediction framework, called CoWatch. Based on the distributed software-defined network (SDN), the CoWatch framework can collaboratively predict the DDoS attacks towards the EC servers and detect the attack flows near the attack source in time. To efficiently filter the suspicious flows in distributed SDN, we design an optimal threshold model by balancing the trade-off between collaboration efficiency and prediction effectiveness. We also explore the prototypical LSTM network to design an LSTM-based Collaborative Prediction (LCP) algorithm, which can effectively predict and detect DDoS attacks. Experiment results demonstrate the effectiveness of the prediction and detection of DDoS attacks and validate the efficiency of flow information synchronization in distributed SDN.
\subsubsection{A Novel Distributed Data Backup and Recovery Method for Software Defined-WAN Controllers}
abstract:Software-defined wide area network (SD-WAN) is a new type of network architecture that has developed rapidly in recent years. SD-WAN inherits the centralized control architecture of SDN, but supports more diverse access methods and equipment types and covers a wider area. It is also associated with greater uncertainty in the network environment. These characteristics make the fault management of the SD-WAN controller more challenging, so that the existing SDN-based data backup methods cannot adapt to SD-WAN scenarios. This paper proposes a SD-WAN-oriented Distributed Data Backup and Recovery method (DDBR) based on an improved secret sharing algorithm. To deploy this method, we design an online-offline dual backup framework based on the data freshness requirements of the controller. Under this framework, dynamic data of the controller is divided into different shares, and then stored into the storage of switches. When the controller fails, data recovery can be performed on the backup controller quickly, which greatly improves the network availability. The outstanding feature of the proposed DDBR method is that it ensures the integrity and confidentiality of the backup data in an unreliable network environment, even when some of the storage nodes fail. Evaluation results on file backup example show that the proposed solution has significant advantages over existing methods in terms of backup data storage size and backup success rate.
\subsubsection{NGS: Mitigating DDoS Attacks using SDN-based Network Gate Shield}
abstract:The Internet of Things (IoT) implements a tremendous environment of extensive data streams, whereby any suspicious activities should be detected to safeguard systems' reliability and availability. Distributed Denial of Service (DDoS) attack is a major threat on computer networks, in which a cyber attacker can send huge traffic with multiple IP addresses or machines. In this work, we focus on DDoS attack, and propose Network Gate Shield (NGS) that is a tool that works on SDN architecture based on the RYU controller. It can examine the traffic trustworthiness, and decide whether the current traffic is normal based on packet specification, such as the average packet size during a time interval and the packet per-sec threshold. In the evaluation with an emulated environment, our experimental results indicate that NGS is viable and effective in mitigating DDoS traffic compared with several similar detection approaches.
\subsection{Software Security}
\subsubsection{Robust Detection of Evolving Mobile Malware}
abstract:Mobile malware detection systems are often vulnerable to evasion attacks, in which a malware developer manipulates a malware sample such that it is misclassified as benign. In this paper, we propose a novel mobile malware detection system that can detect benign-looking evasive malware apps. The key idea is to expose the malicious nature of the apps by unraveling the evasive techniques employed by malware. We observed that evasive malware exhibit characteristics of benign apps, and by eliminating these features, the classifier can correctly detect the malware without compromising significantly on benign app detection. The training pipeline of our proposed system is much simpler than existing malware detection methods, as the network is trained end-to-end to jointly learn appropriate features and to perform classification. We train our model on 3.2 million apps collected from AndroZoo dataset. We perform an extensive study on publicly available datasets and malware samples collected from the wild to show the effectiveness of the proposed technique. In particular, we show that our algorithm achieves an accuracy and F1-score of 95.36% and 0.95, respectively.
\subsubsection{Friend or Foe: Discerning Benign vs Malicious Software and Malware Family}
abstract:Malware remains one of the gravest threats to cybersecurity, second only to social engineering or a lack of user security awareness. This is especially true for Windows systems in enterprise environments. As malware continues to evolve and frustrate legacy detection and prevention mechanisms, additional approaches are necessary to ensure security resilience. Machine learning offers many opportunities to better combat malware threats through the advantage of big datasets. Our research highlights how machine learning can be leveraged to identify malware threats with rapid results, enabling cybersecurity professionals to learn and adapt to these threats. The approach we present in this paper produces an efficient methodology to discern malware family and function through analysis of just the first 3,000 Windows system API function calls. We compare MLP, CNN, and SVM networks to determine the best performance in terms of accuracy and speed and find that MLP works the best with our dataset.
\subsubsection{Mal-LSGAN: An Effective Adversarial Malware Example Generation Model}
abstract:Various Machine Learning (ML) models have been developed for malware detection. But their widespread application is challenged by adversarial attack using adversarial malware examples. Generative Adversarial Networks (GAN) is one of the effective approaches to help build possible unknown attacks and expose the vulnerability of targeted systems. The existing GAN-based ML models have the weaknesses of unstable training and low-quality adversarial examples. In this paper, we propose a novel Mal-LSGAN model to tackle these weaknesses. By using a Least Square (LS) loss function and a newly-designed network structure, Mal-LSGAN achieves a higher Attack Success Rate (ASR) and a lower True Positive Rate (TPR) in 9 ML detectors, compared with the existing MalGAN and Imp-MalGAN. In Multi-Layer Perceptron (MLP), Mal-LSGAN can even decrease TPR from 97.81% of original examples to 1.09% of adversarial examples. The experimental results also demonstrate that Mal-LSGAN gets preferable transferability of adversarial malware examples.
\subsubsection{Adversarial Text-Based CAPTCHA Generation Method Utilizing Spatial Smoothing}
abstract:The development of deep learning (DL) techniques has enabled cracking traditional text-based CAPTCHA, resulting in new security issues.As a countermeasure against DL based attacks, the adversarial CAPTCHA is well suited since it can increase the difficulty of machine recognition while ensuring human readability.However, spatial smoothing can negate the effectiveness of adversarial CAPTCHAs because adversarial noises in them are subject to averaging pixels. There are no effective counters against spatial smoothing, whereas it is the critical problem which facilitates spreading automated attacks. Therefore, to address the unsolved problem, in this paper, we propose an adversarial text-based CAPTCHA generation method using spatial smoothing. We focus on the fact that when spatial smoothing is applied to an image, the amount of information it carries decreases, making the whole image blurred. Spatial smoothing is only viable as an attack when the mitigation of the adversarial noise has a larger impact than the whole image getting blurred. Thus, when the degree of spatial smoothing exceeds a certain threshold, the impact of the two aspects reverses, and the difficulty of the recognition increase. By utilizing this phenomenon in the generation of CAPTCHAs, the proposed method can indirectly neutralize the originally expected effect of spatial smoothing by attackers, preventing the recognition rate from increasing. Our evaluation shows the proposed method can reduce the recognition rate by up to 34%, compared to the conventional method. Besides, an experiment on human recognition rates marked 73.67%, showing that human recognition is maintained at an acceptable level.
\subsubsection{HyperKRP: A Kernel Runtime Security Architecture with A Tiny Hypervisor on Commodity Hardware}
abstract:The large body of kernel code provides broad attack surfaces to exploitable bugs or misconfigurations. Current mitigations are difficult to be integrated together or have a non-trivial performance or code size impact. Thus, systematical protection for the kernel is of critical importance and is required. In this paper, we propose a kernel runtime security architecture, called HyperKRP, to provide systematical protection for kernel code, critical kernel data, and efficient kernel page tables. We have implemented a fully working prototype for a recent Linux kernel running on the Intel x86 processor. Our prototype is compromised of three protection engines based on a small size hypervisor. The evaluation shows that HyperKRP effectively ensures kernel runtime security with acceptable overhead.
\subsection{Wireless Networks Security}
\subsubsection{Preamble Injection and Spoofing Attacks in Wi-Fi Networks}
abstract:In Wi-Fi networks, every frame begins with a preamble that is used to support frame detection, synchronization, and channel estimation. The preamble also establishes compatibility and interoperability among devices that operate different Wi-Fi versions (e.g., IEEE 802.11a/g/n/ac/ax). Despite the crucial functions of the preamble, no guarantees can be made on its authenticity or confidentiality. Only weak integrity protection is currently possible. In this paper, we introduce novel Preamble Injection and Spoofing (PrInS) attacks that exploit the vulnerabilities of the preamble. Specifically, an adversary can inject forged preambles without any payload for the purpose of disrupting legitimate receptions or forcing a legitimate user to defer its transmission. The proposed PrInS attacks are effective irrespective of the Wi-Fi versions used by the adversary and its targets, as the attacks take advantage of the physical (PHY) layer receive state machine and/or capture effect. We develop an adversarial model and use it to launch PrInS attacks on Wi-Fi systems. Our attacks are validated experimentally using software-defined radios (SDRs). Our results show that the adversary can almost silence the channel, bringing the throughput of a legitimate user to 2% of its normal throughput. Even at 30 dB less power, the adversary still causes 87% reduction in the legitimate users' throughput. To mitigate the PrInS attacks, we propose a backward-compatible scheme for preamble authentication.
\subsubsection{Privacy-preserving D2D Cooperative Location Verification}
abstract:Device-to-Device (D2D) cooperative location verification allows a device to verify its location with the help of neighbouring devices. It is especially handy in location-based services where location verification is essential. However, the exposure of device location during verification rises a big privacy risk for participants since they have to send their real-time locations to unknown verifiers holding anonymous identities. Thus, a privacy-preserving solution is urgently needed to provide verification without location disclosure. Traditional solutions based on Paillier and garbled circuits can solve the problem but also introduce high cost. Based on order-preserving encryption, we propose an efficient protocol with high-security guarantee to address this issue. Apart from rigorous security proof, complexity analysis and extensive experiments are also conducted to evaluate the proposed solution. The results compared with related work show that order-preserving encryption based mechanism achieves the best balance with regards to privacy, utility and performance requirements.
\subsubsection{iCoding: Countermeasure Against Interference and Eavesdropping in Wireless Communications}
abstract:With the rapid development of wireless communication technologies, interference management (IM) and security/privacy in data transmission have become critically important. On one hand, due to the broadcast nature of wireless medium, the interference superimposed on the desired signal can destroy the integrity of data transmission. On the other hand, malicious receivers (Rxs) may eavesdrop a legitimate user's transmission and thus breach the confidentiality of communication. To counter these threats, we propose a novel encoding method, called immunizing coding (iCoding), which handles both IM and physical-layer security simultaneously. By exploiting both channel state information (CSI) and data carried in the interference, an iCoded signal is generated and sent by the legitimate transmitter (Tx). The iCoded signal interacts with the interference at the desired/legitimate Rx, so that the intended data can be recovered without the influence of disturbance, i.e., immunity to interference is achieved. In addition, since the data carried in the iCoded signal which is obtained via encoding the desired data and interference cooperatively, is different from the original desired data, the eavesdropper cannot access legitimate information by wiretapping the desired signal. Therefore, immunity to eavesdropping is achieved. Our theoretical analysis and in-depth simulation have shown iCoding to effectively manage interference while preventing potential eavesdropping, hence enhancing the legitimate user's data transmission and secrecy thereof.
\subsubsection{Detecting Abnormal Nodes in Cluster-tree Networks via the Likelihood Ratio Test of Packet Losses}
abstract:While packet loss rate is a crucial indicator of network reliability, abnormal nodes dropping the transit traffic lead to higher packet loss rates. One significant challenge is to efficiently distinguish the abnormal nodes from those with normal packet losses. To address this issue, this paper presents a new detection scheme based on the Likelihood Ratio Test (LRT) for cluster-tree networks -- a typical architecture of wireless networks. Due to the hierarchical structure of the networks, LRT is implemented in two phases, i.e., the local detection phase on non-root nodes and the overall detection phase on the root node. The observed forwarding behaviors of the monitored node are modeled and quantified as log-likelihood ratios where the packet loss rate in the alternative hypothesis is a default value. In this way, the log-likelihood ratios are compared with a threshold under the maximum a posteriori probability criterion to identify the anomalies. The correctness of the proposed scheme is theoretically proved if the packet loss rate in the abnormal node exceeds a critical detection point -- whose expression is also formally derived. Simulation results validate the critical detection point and demonstrate the superiority of the proposed detection scheme compared to the state-of-the-art methods.
\subsubsection{Prediction-based Cache Mechanism to Accelerate Block Validation for Lightweight Full Nodes}
abstract:Since version 0.11 of Bitcoin Core, a user can run a full node in pruning mode, i.e. a pruned node, in resource-limited devices. The pruned node is a lightweight full node because it just maintains some recent blocks and the entire Unspent Transaction Output (UTXO) set. However, the increasing UTXO set results in that the main parts of the UTXO set are stored in the low-speed disk, and thus slows down block validation in the lightweight full node. The existing schemes ignore that most transactions in a new block have already been in the mempool of the lightweight full node. In this paper, we utilize this inherent property and propose a prediction-based cache mechanism, which can accelerate block validation for lightweight full nodes. The basic idea is that the mempool of the lightweight full node is similar to that of miners, and most miners select transactions to maximize total transaction fees when mining a new block. Thus, we can predict which UTXOs will be utilized in the next block. The experimental results demonstrate that the proposed mechanism can accelerate block validation for lightweight full nodes with little memory requirement.
\section{Communication Theory}
\subsection{Channel Modeling and Analysis}
\subsubsection{Stochastic Model for Time-Varying Millimeter-Wave Beam Gains with User Orientation Changes}
abstract:In the widely used spatial channel model (SCM), the millimeter wave channels encountered by a 5G system are constructed using a stochasto-geometric approach. However, a probabilistic characterization of the time evolution of the gain of any transmit-receive beam pair is not available. We propose a novel modified bivariate Nakagami-m (MBN) that presents such a characterization. It accurately captures the non-stationary time-variation in the beam gain due to user mobility and user device orientation changes. It applies to both positive and negative correlations between the beam gains. We derive closed-form expressions for the MBN parameters in terms of the SCM parameters. We then apply this tractable model to develop a new, effective, and robust beam selection rule. It predicts in closed-form the signal-to-noise ratios of the beam pairs at the time a beam pair is selected given the beam pair gain measurements that are made at different times.
\subsubsection{A new discrete-time model for channels impaired by phase noise}
abstract:We propose a novel discrete-time model for the phase noise signal, in case of free-running and phase-locked oscillators. The strength of the proposed model is that it can be easily expressed in terms of measurement parameters of practical oscillators. We then analyse the most common discrete-time phase noise channel model with reference to the measurement parameters and to the system bandwidth. The derived analytical models for the discrete-time phase noise signal can be used for the design of estimation/detection algorithms, for performance evaluation, or simply for fast simulations.
\subsubsection{Asymptotic Analysis of Diversity Receptions Over Correlated Lognormal-Rician Fading Channels}
abstract:Lognormal-Rician random variables (RV) can model large-scale and small-scale fading, line-of-sight and non-line-of-sight channels in wireless communications. However, the versatile stochastic tool is mathematically intractable for correlated cases. In this work, we derive closed-form expressions for the outage probabilities of diversity receptions over dual-branch lognormal-Rician fading channels with arbitrary correlation at high signal-to-noise ratio (SNR). The analytical results show that the lognormal RV and the Rician RV, respectively, contribute an exponential factor and a fractional factor to the outage probability expressions. Some important insights are revealed based on the elegant expressions. For example, negative correlation between the lognormal RVs can suppress the outage probability, and the phase of the complex correlation coefficient between the accompanying Gaussian RVs in the Rician channels can significantly influence the performance.
\subsubsection{Coverage in Terahertz Cellular Networks with Imperfect Beam Alignment}
abstract:We develop a novel stochastic geometry framework to quantify the SINR coverage probability of a Terahertz (THz) cellular network. THz frequencies will require highly directional beams, leading to inevitably imperfect beam alignment. We derive a tractable and accurate semi-closed form lower bound for the coverage probability of a typical user in the network, introducing a novel approach of characterizing non line-of-sight (NLOS) links as equivalent LoS links using a non-homogeneous Poisson Point Process. We use the coverage bound to investigate the SINR scaling trends with base station density and array directivity at the base station and user. Dense base station deployments are required to achieve sufficient coverage and our analysis exposes a tradeoff between directivity (array gain) and loss in SINR due to misalignment.
\subsection{Coding (I)}
\subsubsection{Multilevel Polar-Coded Modulation: Performance Analysis and Code Construction}
abstract:Multilevel polar-coded modulation with multistage decoding is a capacity-achieving coded modulation scheme. In this paper, we propose a general formulation for analyzing the performance of multilevel polar-coded modulation and then derive the error probability upper bounds. The analysis explicitly reveals the effect of the modulation scheme on the performance of component polar codes. Based on the derived upper bounds, we also propose two construction methods for multilevel polar-coded modulation. Compared with conventional methods, such as density evolution and Gaussian approximation which involve complicated recursive calculations, the proposed methods have a linear computational complexity. Simulation results also show that the proposed construction methods can achieve comparable performance to existing methods under SC decoding, and even better performance under SC list decoding.
\subsubsection{Multi-level Polar Coded Modulation for the Decode-Forward Relay Channel}
abstract:We investigate the performance of multi-level polar coded modulation in the decode-forward relay channel. We begin by numerically analyzing the rates assigned to polar codes of all levels via chain rule and error exponent. The construction of polar codes follows the 5G standard. A joint decoding based on maximum ratio combining with multistage decoding is proposed for the destination. We simulate the error performances under $16$QAM with gray labeling and Ungerboeck's set partitioning. In the half-duplex mode, a gain of $2.5$dB is observed compared with the state of the art, consisting of $0.7$dB gain due to multistage decoding and $1.8$dB gain due to the choice of labeling. In addition, the error performance according to error exponent is compared with the chain rule. A dispersion bound for the decode-forward relaying is calculated.
\subsubsection{The Complete Affine Automorphism Group of Polar Codes}
abstract:Recently, a permutation-based successive cancellation(PSC) decoding framework for polar codes attaches muchattention. It decodes several permuted codewords with independentsuccessive cancellation (SC) decoders. Its latency thus canbe reduced to that of SC decoding. However, the PSC frameworkis ineffective for permutations falling into the lower-triangularaffine (LTA) automorphism group, as they are invariant under SCdecoding. As such, a larger block lower-triangular affine (BLTA)group that contains SC-variant permutations was discovered fordecreasing polar codes. But it was unknown whether BLTAequals the complete automorphism group. In this paper, we provethat BLTA equals the complete automorphisms of decreasingpolar codes that can be formulated as affine transformations.
\subsubsection{Optimization-based Block Coordinate Gradient Coding}
abstract:Existing gradient coding schemes introduce identical redundancy across the coordinates of gradients and hence cannot fully utilize the computation results from partial stragglers. This motivates the introduction of diverse redundancies across the coordinates of gradients. This paper considers a distributed computation system consisting of one master and N workers characterized by a general partial straggler model and focuses on solving a general large-scale machine learning problem with L model parameters. We show that it is sufficient to provide at most N levels of redundancies for tolerating 0, 1,..., N-1 stragglers, respectively. Consequently, we propose an optimal block coordinate gradient coding scheme based on a stochastic optimization problem that optimizes the partition of the L coordinates into N blocks, each with identical redundancy, to minimize the expected overall runtime for collaboratively computing the gradient. We obtain an optimal solution using a stochastic projected subgradient method and propose two low-complexity approximate solutions with closed-from expressions, for the stochastic optimization problem. We also show that under a shifted-exponential distribution, for any L, the expected overall runtimes of the two approximate solutions and the minimum overall runtime have sub-linear multiplicative gaps in N. To the best of our knowledge, this is the first work that optimizes the redundancies of gradient coding introduced across the coordinates of gradients.
\subsubsection{Block Orthogonal Sparse Superposition Codes}
abstract:This paper introduces block orthogonal sparse superposition (BOSS) codes for efficient short-packet communica- tions over Gaussian channels. Unlike conventional sparse superposition codes, an encoder of BOSS code uses multiple unitary matrices as a fact dictionary matrix and maps information bits such that multiple subgroups of codewords are orthogonal. Exploiting this orthogonal property per group, a two-stage maximum a posteriori (MAP) decoding algorithm is presented. The key idea of the two-stage MAP decoder is to successively estimate the non-zero alphabets corresponding to the orthogonal columns in a dictionary matrix and the index of a sub-dictionary matrix containing the columns. This decoding algorithm achieves a near-optimal decoding performance while requiring polynomial time complexity in blocklength. Via simulations, we show that the proposed encoding and decoding techniques achieve enhanced block-error-rate performances in the short blocklength regime compared to the state-of-the-art coded modulation methods.
\subsection{Coding (II)}
\subsubsection{Algebraic Design of a Class of Rate 1/3 Quasi-Cyclic LDPC Codes}
abstract:An algebraic design procedure is given for constructing rate 1/3 quasi-cyclic LDPC codes with Tanner graphs with girth 6. Parity check matrices consist of 2 x 3 block matrices of circulants of size p, a prime, exhibiting invariance under the action of a subgroup of the multiplicative group mod p. The group invariance converts the problem of avoiding short cycles into that of choosing orbits to solve connectivity constraints. A series of very low density H matrix codes constructed show surprisingly good minimum distances.
\subsubsection{IGRAND: decode any product code}
abstract:We introduce Iterative GRAND (IGRAND), a universal product code decoder that applies iterative bounded distance decoding and decodes component codes using code-agnostic Guessing Random Additive Noise Decoding (GRAND). We empirically determine its accuracy and, based on GRAND hardware measurements, its complexity, showing gains over alternative algorithms. We prove that the class of product codes with random linear component codes, which IGRAND is capable of decoding, are capacity-achieving in hard-decision channels.
\subsubsection{Refined Density Evolution Analysis of LDPC Codes for Successive Interference Cancellation}
abstract:Successive interference cancellation (SIC) is a fundamental decoding technique for Gaussian multiple access channels (GMAC). In SIC, transmit signals of each user are separately decoded. In this paper, we analyze an asymptotic decoding threshold of SIC decoding for practical low-density parity-check (LDPC) codes over N-user GMAC. Conventionally, the decoding thresholds are evaluated based on the so-called channel approximation (CA) in which the channel model of each decoding stage of a SIC decoder is approximated by simple Gaussian noise channels resulting in errors of decoding thresholds. To avoid this, we propose a refined DE analysis called DE-SIC which uses mixed-Gaussian noise channels corresponding to each decoding stage.We demonstrate DE-SIC by solving a received power optimization problem in GMAC and comparing it to the conventional DE analysis with CA. The results show that the proposed DE-SIC accurately evaluates the decoding thresholds whereas the conventional analysis underestimates the thresholds.
\subsubsection{A Novel Iterative Soft-Decision Decoding Algorithm for RS-SPC Product Codes}
abstract:This paper presents a generalized construction of RS-SPC product codes. A low-complexity joint-decoding scheme is proposed for these codes, in which a BP-based iterative decoding is performed based on the binary expansion of the whole parity-check matrix. Various powerful RS codes can be used as the component codes for RS-SPC product codes, which gives a good performance for local decoding (decode a single component codeword). The proposed BP-based iterative decoding is a global decoding, and it achieves an error-correcting capability comparable to codes of large blocklengths. This two-phase decoding scheme preserves the low decoding latency and complexity of the local decoding while achieves high reliability through the global decoding. The complexity of the proposed iterative decoding is discussed, and the simulation results show the proposed scheme offers a good trade-off between the complexity and the error performance.
\subsection{Communication Theory}
\subsubsection{Individual Correlation Properties and Structural Features of Periodic Complementary Sequences}
abstract:Complementary sequences (CS) were considered to be used in pairs, although their property to reduce the crest factor in OFDM and MC-CDMA systems employing CS-based spreading is widely known. Their individual properties have hardly ever been studied, with one exception for the Golay sequences. In this paper, we study the individual properties of periodic CS (PCS), which are a superclass of Golay sequences. We show that PCS have remarkable correlation characteristics and unique features at their own, acting as single sequences. Although PCS are somewhat inferior to the Gold and Kasami sequences in terms of peak correlations, they are similar, and sometimes even perform better, in terms of RMS correlation values, and outnumber them by orders of magnitude. The structure of PCS enables efficient processing in applications requiring high data rates. We have also identified the unique feature of PCS which is possibility to use them to construct sets of orthogonal signals that lead to processing advantages of both complementary sequences and cyclic codes.
\subsubsection{FPGA Implementations of Layered MinSum LDPC Decoders Using RCQ Message Passing}
abstract:Non-uniform message quantization techniques such as reconstruction-computation-quantization (RCQ) improve error-correction performance and decrease hardware complexity of low-density parity-check (LDPC) decoders that use a flooding schedule. Layered MinSum RCQ (L-msRCQ) enables message quantization to be utilized for layered decoders and irregular LDPC codes. We investigate field-programmable gate array (FPGA) implementations of L-msRCQ decoders. Three design methods for message quantization are presented, which we name the Lookup, Broadcast, and Dribble methods. The decoding performance and hardware complexity of these schemes are compared to a layered offset MinSum (OMS) decoder. Simulation results on a (16384, 8192) protograph-based raptor-like (PBRL) LDPC code show that a 4-bit L-msRCQ decoder using the Broadcast method can achieve a 0.03 dB improvement in error-correction performance while using 12% fewer registers than the OMS decoder. A Broadcast-based 3-bit L-msRCQ decoder uses 15% fewer lookup tables, 18% fewer registers, and 13% fewer routed nets than the OMS decoder, but results in a 0.09 dB loss in performance.
\subsubsection{Two-Hop Network with Multiple Decision Centers under Expected-Rate Constraints}
abstract:The paper studies distributed binary hypothesis testing over a two-hop relay network where both the relay and the receiver decide on the hypothesis. Both communication links are subject to expected rate constraints, which differs from the classical assumption of maximum rate constraints. We exactly characterize the set of type-II error exponent pairs at the relay and the receiver when both type-I error probabilities are constrained by the same value \(\epsilon>0\). No tradeoff is observed between the two exponents, i.e., one can simultaneously attain maximum type-II error exponents both at the relay and at the receiver. For \(\epsilon_1 \neq \epsilon_2\), we present an achievable exponents region, which we obtain with a scheme that applies different versions of a basic two-hop scheme that is optimal under maximum rate constraints. We use the basic two-hop scheme with two choices of parameters and rates, depending on the transmitter's observed sequence. For \(\epsilon_1=\epsilon_2\), a single choice is shown to be sufficient. Numerical simulations indicate that extending to three or more parameter choices is never beneficial.
\subsubsection{Nonlinear Distortion in Distributed Massive MIMO Systems: A Indoor Channel Measurement Analysis}
abstract:In this paper, we experimentally analyze the spatial distribution of nonlinear distortion in massive MIMO systems with various array topologies and user locations. With an indoor channel measurement, we reveal the spatial distortion distribution of the in-band (IB) and out-of-band (OOB) power leakage in a real-life scenario. We further investigate the power leakage under different antenna array topologies: including uniform linear array (ULA), uniform rectangular array (URA), distributed linear subarrays (DIS). The impact of user location on the per antenna distortion is also visualized. The results indicate that the DIS array configuration achieves the lowest in-band and out-of-band power leakage, which renders the distributed array a potential to reduce the linearity requirement of PAs when scaling-up a practical massive MIMO system.
\subsection{Communication Theory for Time-Sensitive Applications}
\subsubsection{Age of Information Analysis of Multi-user Mobile Edge Computing Systems}
abstract:In this paper, we analyze the age of information (AoI) performance of a multi-user mobile edge computing (MEC) system where a base station (BS) generates and transmits computation-intensive packets to user equipments (UEs). In this MEC system, we consider two computing schemes, namely, the local computing scheme and the edge computing scheme. In the local computing scheme, each packet is transmitted to the UE and then computed by the local server at the UE. In the edge computing scheme, each packet is computed by the edge server at the BS and then transmitted to the UE. Considering exponentially distributed transmission time and computation time and adopting the first come first serve queuing policy, we derive the closed-form expressions for the average AoI of these two computing schemes. Simulation results corroborate our analysis and examine the impact of system parameters on the average AoI.
\subsubsection{A Spatio-temporal Analysis of Cellular-based IoT Networks under Heterogeneous Traffic}
abstract:In this paper, we consider a cellular-based Internet of things (IoT) network consisting of IoT devices that can communicate directly with each other in a device-to-device (D2D) fashion as well as send real-time status updates about some underlying physical processes observed by them. We assume that such real-time applications are supported by cellular networks where cellular base stations (BSs) collect status updates over time from a subset of the IoT devices in their vicinity. We characterize two performance metrics: i) the network throughput which quantifies the performance of D2D communications, and ii) the Age of Information which quantifies the performance of the real-time IoT-enabled applications. Concrete analytical results are derived using stochastic geometry by modeling the locations of IoT devices as a bipolar Poisson Point Process (PPP) and that of the BSs as another Independent PPP. Our results provide useful design guidelines on the efficient deployment of future IoT networks that will jointly support D2D communications and several cellular network-enabled real-time applications.
\subsubsection{Minimizing Age of Incorrect Information for Unreliable Channel with Power Constraint}
abstract:Age of Incorrect Information (AoII) is a newly introduced performance metric that is adaptable to a variety of communication goals. It has advantages over both the traditional performance metrics and the recently introduced metric - Age of Information (AoI). However, the fundamental nature of AoII has been elusive so far. In this work, we consider the AoII in a system where a transmitter sends updates about a multi-state Markovian source to a remote receiver through an unreliable channel. The communication goal is to minimize AoII subject to a power constraint. We cast the problem into a Constrained Markov Decision Process (CMDP) and prove that the optimal policy is a mixture of two deterministic threshold policies. Afterward, by leveraging the notion of Relative Value Iteration (RVI) and the structural properties of threshold policy, we propose an efficient algorithm to find the threshold policies as well as the mixing coefficient. Lastly, numerical results are laid out to highlight the effects of system parameters on the performance of AoII-optimal policy.
\subsubsection{Reliable Two-Timescale Scheduling in a Multi-User Downlink Channel with Hard Deadlines}
abstract:Ultra-Reliable Low-Latency Communications (URLLC) is an important part of emerging 5G and 6G networks which enables mission-critical applications like autonomous driving. These novel applications depend on the error-free delivery of short messages before an application-specific deadline, which is challenging in a fast-changing environment. In this work, we consider a wireless fading downlink channel shared for the transmission of periodically arriving messages for multiple mobile units (MUs). The message sizes, deadlines and the period of message arrival are MU-specific. The message for a MU can be split into smaller data packets, so that multiple unreliable transmissions can be combined to achieve a reliable transmission. We formulate an infinite time horizon Markov Decision Process (MDP) for the average timely throughput, and show that the MDP is periodic. We propose a novel two-timescale scheduling solution, which incorporates the uncertainty of the channel in an inter-frame problem and errors caused by short-packet coding in an intra-frame problem. Through numerical simulations, we show that the proposed approach outperforms State-of-the-Art scheduling algorithms in terms of timely throughput.
\subsubsection{On Joint Detection and Decoding in Short-Packet Communications}
abstract:We consider a communication problem in which the receiver must first detect the presence of an information packet and, if detected, decode the message carried within it. We present general nonasymptotic upper and lower bounds on the maximum coding rate that depend on the blocklength, the probability of false alarm, the probability of misdetection, and the packet error probability. The bounds, which are expressed in terms of binary-hypothesis-testing performance metrics, generalize finite-blocklength bounds derived previously for the scenario when a genie informs the receiver whether a packet is present. The bounds apply to detection performed either jointly with decoding on the entire data packet, or separately on a dedicated preamble. The results presented in this paper can be used to determine the blocklength values at which the performance of a communication system is limited by its ability to perform packet detection satisfactorily, and to assess the difference in performance between preamble-based detection, and joint detection and decoding. Numerical results pertaining to the binary-input AWGN channel are provided.
\subsection{Counter Fading Communication Theory}
\subsubsection{LDL-precoded FTN Signaling with Power Allocation in The Block Fading Channel}
abstract:In this paper, to obtain a higher information rate in the block fading channel, we propose an LDL-decomposition-based linear precoded faster-than-Nyquist (FTN) signaling with power allocation. We further propose an LDL-decomposition-based linear precoded FTN signaling with truncated power allocation (LDL-TPA-FTN) to extend the application range of the acceleration factor. Moreover, we derive the corresponding maximum average information rate, outage probability, and outage capacity of the proposed schemes. Also, the complexity of the proposed schemes is analysed theoretically and compared with classic ones, which shows the proposed schemes have lower complexity. Numerical results show that the proposed schemes have higher outage capacity and achievable rate than Nyquist signaling in the block fading channel. Also, the proposed schemes can reduce the incurred inter-symbol-interference (ISI) and have the same application range of the acceleration factors as the singular value decomposition-based FTN. Moreover, a trade-off should be made between the side-lobe suppression of the power spectral density of the transmit signal and threshold selection in LDL-TPA-FTN to obtain higher performance gain for small acceleration factors.
\subsubsection{Capacity Optimization using Reconfigurable Intelligent Surfaces: A Large System Approach}
abstract:Reconfigurable Intelligent Surfaces (RISs), comprising large numbers of low-cost and passive metamaterials with tunable reflection properties, have been recently proposed as an enabler for programmable radio propagation environments. However, the role of the channel conditions near the RISson their optimizability has not been analyzed adequately. In this paper, we present an asymptotic closed-form expression for the mutual information of a multi-antenna transmitter-receiver pair in the presence of multiple RISs, in the large-antenna limit, using the random matrix and replica theories. Under mild assumptions, asymptotic expressions for the eigenvalues and the eigenvectors of the channel covariance matrices are derived. We find that, when the channel close to an RIS is correlated, for instance due to small angle spread, the communication link benefits significantly from the RIS optimization, resulting in gains that are surprisingly higher than the nearly uncorrelated case. Furthermore, when the desired reflection from the RIS departs significantly from geometrical optics, the surface can be optimized to provide robust communication links. Building on the properties of the eigenvectors of the covariance matrices, we are able to find the optimal response of the RISs in closed form, bypassing the need for brute-force optimization.
\subsubsection{Robust Design of Secure IRS-aided MISO Broadcasting for SWIPT and Spectrum Sharing}
abstract:We consider an intelligent reflecting surface (IRS)-aided secondary multiple-input single-output (MISO) broadcast system for simultaneous wireless information and power transfer (SWIPT) in a spectrum underlay setup. The secondary transmitter (ST) regards the primary receivers (PR) as possible eavesdroppers. We propose an inter-system coordination protocol that enables acquisition at the ST of control information to facilitate interference management. We assume availability of imperfect channel state information (CSI) regarding the relevant direct and IRS-cascaded links at the ST. We aim at jointly optimizing the transmit precoding, artificial noise (AN) covariance, and reflect beamforming matrices, so that the transmit power of the ST is minimized subject to the quality-of-service (QoS) requirements of the information decoding and energy harvesting secondary receivers (IDR/EHR), the security and interference constraints of the PRs, and the unit-modulus constraints of the IRS phase shifts. We obtain convex approximations of the probabilistic constraints by employing Bernstein-type and first-order Taylor inequalities. We derive a robust outage-constrained design by developing an alternating minimization algorithm that makes use of the semi-definite relaxation (SDR) method and the penalty convex-concave procedure (CCP). Our design takes into account the additional interference incurred at the PRs by the IRS-reflected transmissions of the primary transmitter (PT) itself, which serves its users in an IRS-blind manner. Numerical simulation results reveal the performance gains of the proposed scheme over benchmark strategies and highlight the impact of the system parameters on the performance.
\subsubsection{How is Time Frequency Space Modulation Related to Short Time Fourier Signaling?}
abstract:We investigate the relationship between Orthogonal Time Frequency Space (OTFS) modulation and Orthogonal Short Time Fourier (OSTF) signaling. OTFS was recently proposed as a new scheme for high Doppler scenarios and builds on OSTF. We first show that the two schemes are unitarily equivalent in the digital domain. However, OSTF defines the analog-digital interface with the waveform domain. We then develop a critically sampled matrix-vector model for the two systems and consider linear minimum mean-squared error (MMSE) filtering at the receiver to suppress inter-symbol interference. Initial comparison of capacity and (uncoded) probability of error reveals a surprising observation: OTFS under-performs OSTF in capacity but over-performs in probability of error. This result can be attributed to characteristics of the channel matrices induced by the two systems. In particular, the diagonal entries of OTFS matrix exhibit nearly identical magnitude, whereas those of the OSTF matrix exhibit wild fluctuations induced by multipath randomness. It is observed that by simply replacing the unitary matrix, relating OTFS to OSTF, by an arbitrary unitary matrix results in performance nearly identical to OTFS. We then extend our analysis to orthogonal frequency division multiplexing (OFDM) and also consider a more extreme scenario of relatively large delay and Doppler spreads. Our results demonstrate the significance of using OSTF basis waveforms rather than sinusoidal ones in OFDM in highly dynamic environments, and also highlight the impact of the level of channel state information used at the receiver.
\subsection{Detection and Estimation Theory}
\subsubsection{A New Channel Estimation Strategy in Intelligent Reflecting Surface Assisted Networks}
abstract:Channel estimation is the main hurdle to reaping the benefits promised by the intelligent reflecting surface (IRS). Recently, a breakthrough was made in reducing the channel estimation overhead by revealing that the IRS-BS (base station) channels are common in the cascaded user-IRS-BS channels of all the users, and if the cascaded channel of one typical user is estimated, the other users' cascaded channels can be estimated very quickly based on their correlation with the typical user's channel. One limitation of this strategy is the waste of user energy, because many users need to keep silent when the typical user's channel is estimated. In this paper, we reveal another correlation hidden in the cascaded user-IRS-BS channels by observing that the user-IRS channel is common in the cascaded channels of all BS antennas. Building upon this finding, we propose a novel two-phase channel estimation protocol in the uplink communication. Specifically, in Phase I, the correlation coefficients between the channels of a typical BS antenna and those of the other antennas are estimated; while in Phase II, the cascaded channel of the typical antenna is estimated. Under this strategy, it is theoretically shown that the minimum number of time instants required for perfect channel estimation is the same as that of the aforementioned strategy in the ideal case without BS noise. Then, in the case with BS noise, we show by simulation that the channel estimation error of our proposed scheme is significantly reduced thanks to the full exploitation of the user energy.
\subsubsection{MSE-Optimaized Linear Transform for Noisy Fronthaul Channels in Distributed MIMO C-RAN}
abstract:A novel linear transform is presented which is suitable for noisy fronthaul channels in distributed MIMO C-RAN systems. A linear transform based on the Karhunen-Loeve transform is optimized in terms of mean squared error (MSE). Using MSE as the objective function enables us to derive a concise closed MSE formula. By using this MSE formula and the Lagrange multiplier method, the optimal linear transform can be derived in a closed form and no optimization processes are required for designing this transform. We also present a dimension expansion method with a Vandermonde matrix as a noise mitigation method. The MSE formula for the Vandermonde dimension expansion indicates an explicit tradeoff relation between spectral efficiency and energy efficiency of the fronthaul channel. Results from numerical experiments and evaluations support our theoretical arguments.
\subsubsection{Detection of Abrupt Change in Channel Covariance Matrix for Multi-Antenna Communication}
abstract:The knowledge of channel covariance matrices is of paramount importance to the estimation of instantaneous channels and the design of beamforming vectors in multi-antenna systems. In practice, an abrupt change in channel covariance matrices may occur due to the change in the user location. Although several works have proposed efficient algorithms to estimate the channel covariance matrices after any change occurs, how to detect such a change accurately and quickly is still an open problem. In this paper, we focus on channel covariance change detection in a downlink communication system, where a multi-antenna base station (BS) serves a single-antenna user equipment (UE). To provide theoretical performance limit, we first propose a genie-aided change detector based on the log-likelihood ratio (LLR) test assuming the channel covariance matrix after change is known, and characterize the corresponding missed detection and false alarm probabilities. Then, this paper considers the practical case where the channel covariance matrix after change is unknown. The maximum likelihood (ML) estimation and shrinkage estimation techniques are respectively used to predict the covariance matrix based on the received pilot signals over a certain number of coherence blocks, building upon which the LLR-based change detector is proposed. Numerical results show that our proposed scheme can detect the change with low error probability even when the number of samples is small such that the estimation of the covariance matrix is not that accurate. This result indicates that it is possible to detect the channel covariance change both accurately and quickly in practice.
\subsubsection{Temporal Detection of Anomalies via Actor-Critic Based Controlled Sensing}
abstract:We address the problem of monitoring a set of binary stochastic processes and generating an alert when the number of anomalies among them exceeds a threshold. For this, the decision-maker selects and probes a subset of the processes to obtain noisy estimates of their states (normal or anomalous). Based on the received observations, the decision-maker first determines whether to declare that the number of anomalies has exceeded the threshold or to continue taking observations. When the decision is to continue, it then decides whether to collect observations at the next time instant or defer it to a later time. If it chooses to collect observations, it further determines the subset of processes to be probed. To devise this three-step sequential decision-making process, we use a Bayesian formulation wherein we learn the posteriorprobability on the states of the processes. Using the posterior probability, we construct a Markov decision process and solve it using deep actor-critic reinforcement learning. Via numerical experiments, we demonstrate the superior performance of our algorithm compared to the traditional model-based algorithms.
\subsection{Distributed Computing}
\subsubsection{Optimization-Based GenQSGD for Federated Edge Learning}
abstract:Optimal algorithm design for federated learning (FL) remains an open problem. This paper explores the full potential of FL in practical edge computing systems where workers may have different computation and communication capabilities, and quantized intermediate model updates are sent between the server and workers. First, we present a general quantized parallel mini-batch stochastic gradient descent (SGD) algorithm for FL, namely GenQSGD, which is parameterized by the number of global iterations, the numbers of local iterations at all workers, and the mini-batch size. We also analyze its convergence error for any choice of the algorithm parameters. Then, we optimize the algorithm parameters to minimize the energy cost under the time constraint and convergence error constraint. The optimization problem is a challenging non-convex problem with non-differentiable constraint functions. We propose an iterative algorithm to obtain a KKT point using advanced optimization techniques. Numerical results demonstrate the significant gains of GenQSGD over existing FL algorithms and reveal the importance of optimally designing FL algorithms.
\subsubsection{Over-the-Air Statistical Estimation of Sparse Models}
abstract:We propose schemes for minimax statistical estimation of sparse parameter or observation vectors over a Gaussian multiple-access channel (MAC) under squared error loss, using techniques from statistics, compressed sensing and wireless communication. These "analog" schemes exploit the superposition inherent in the Gaussian MAC, using compressed sensing to reduce the number of channel uses needed. For the sparse Gaussian location and sparse product Bernoulli models, we derive expressions for risk in terms of the numbers of nodes, parameters, channel uses and nonzero entries (sparsity). We show that they offer exponential improvements over existing lower bounds for risk in "digital" schemes that assume nodes to transmit bits errorlessly at the Shannon capacity. This shows that analog schemes that design estimation and communication jointly can efficiently exploit the inherent sparsity in high-dimensional models and observations, and provide drastic improvements over digital schemes that separate source and channel coding in this context.
\subsubsection{Federated Learning over Time-Varying Channels}
abstract:We study distributed machine learning (ML) systems where independent workers compute local gradients based on their local datasets and send them to a parameter server (PS) through a time-varying multipath fading multiple access channel (MAC) via orthogonal frequency-division multiplexing (OFDM). We assume that the workers do not have channel state information (CSI), hence the PS employs multiple antennas to remove the fading effects. The variation in the wireless channel results in inter-carrier interference (ICI), which has a detrimental effect on the performance of OFDM systems, especially when the channel variations are rapid. To examine the effects of channel time variations on distributed learning systems, we perform an analysis of the interference term due to Doppler, and show that the undesired interference terms caused by time variations do not prevent the convergence of the learning algorithms. Specifically, the ICI term becomes insignificant for slow to moderate time variations. We also validate our theoretical results via simulations.
\subsubsection{Blind Federated Learning with Low-Cost Analog-to-Digital Converters}
abstract:We study federated learning over wireless channels where a massive dataset is distributed across independent workers which compute their local gradients based on their own datasets. Workers send their gradients through a multipath fading multiple access channel with orthogonal frequency division multiplexing to mitigate the frequency selectivity of the channel. We assume that there is no channel state information (CSI) at the workers, and the parameter server (PS) employs multiple antennas to align the received signals. To reduce the power consumption and hardware costs, we employ complex-valued low-resolution analog-to-digital converters (ADCs) at the receiver side, and study the effects of practical low-cost ADCs on the learning performance. Our theoretical analysis shows that the impairments caused by low-resolution ADCs, including those of one-bit ADCs, do not prevent the convergence of the federated learning algorithm, and the multipath channel effects vanish when a sufficient number of antennas are used at the PS. We also validate our theoretical results via simulations.
\subsubsection{A Practical Algorithm Design and Evaluation for Heterogeneous Elastic Computing with Stragglers}
abstract:Our extensive real measurements over Amazon EC2 show that the virtual instances often have different computing speeds even if they share the same configurations. This motivates us to study heterogeneous Coded Storage Elastic Computing (CSEC) systems where machines, with different computing speeds, join and leave the network arbitrarily over different computing steps. In CSEC systems, a Maximum Distance Separable (MDS) code is used for coded storage such that the file placement does not have to be re-defined with each elastic event. Computation assignment algorithms are used to minimize the computation time given computation speeds of different machines. While previous studies of heterogeneous CSEC do not include stragglers -- the slow machines during the computation, we develop a new framework in heterogeneous CSEC that introduces straggler tolerance. Based on this framework, we design a novel algorithm using our previously proposed approach for heterogeneous CSEC such that the system can handle any subset of stragglers of a specified size while minimizing the computation time. Furthermore, we establish a trade-off in computation time and straggler tolerance. Another major limitation of existing CSEC designs is the lack of practical evaluations using real applications. In this paper, we evaluate the performance of our designs on Amazon EC2 for applications of the power iteration and linear regression. Evaluation results show that the proposed heterogeneous CSEC algorithms outperform the state-of-the-art designs by more than 30%.
\subsection{Feedback and Decision Making in Communication Systems}
\subsubsection{Harnessing Random Receiver Cache in Erasure Interference Channels with Feedback}
abstract:We study the capacity region of two-user erasure interference channels with random receiver-end side-information and delayed channel state knowledge at the transmitters. We present a new set of outer-bounds on the achievable rates when each receiver has access to a random fraction of the message intended for the other receiver. The outer-bounds reveal the significant potential rate boost associated with even a small amount of side-information at each receiver. The key in deriving the bounds is to quantify the baseline entropy that will always become available to the unintended receiver given the intermittent connectivity, random available side-information, and causal feedback. We will also present the achievability of these outer-bounds under certain conditions.
\subsubsection{Instantaneous Feedback-based Opportunistic Symbol Length Adaptation for Reliable Communication}
abstract:It is well known that although feedback cannot increase the channel capacity of memoryless channels, it can enhance reliability or shorten codeword length. This work is based on an early result by Viterbi in 1965 that utilizes instantaneous feedback for reliable communications. We build on this work by incorporating (tail-biting) convolutional codes and designing a system where the decoder interacts with the transmitter by sending feedback during the decoding process. The proposed system is called Opportunistic Symbol Length Adaptation (OSLA), in which the symbol length opportunistically adapts to noise realization of each symbol to ensure that the target reliability is achieved. It is shown that, combined with tail-biting convolutional codes, the proposed scheme outperforms state-of-the-art non-feedback codes, as well as a recently proposed deep learning-based feedback scheme with up to 1.5 dB gain in noise-less and noisy feedback channels.
\subsubsection{Improving Energy-Efficiency Using Successively Reordered Transmissions and Feedback}
abstract:For the binary hypothesis testing problem, we propose a novel feedback-enhanced successively reordered transmissions scheme (FE-SRTS), in which the nodes change the order in which they transmit based on the feedback from the fusion node (FN) in each step. It can be implemented in a distributed manner using the timer scheme without any node knowing the measurement of any other node. We derive novel decision rules for it that enable the FN to decide on a hypothesis after receiving only a subset of measurements. For the Bayesian detection framework, FE-SRTS achieves the same optimal error probability as the unordered transmissions scheme (UTS), in which all the nodes transmit their log-likelihood ratios to the FN. However, it requires far fewer nodes to transmit, on average, than UTS, which leads to a much higher energy-efficiency. As the signal-to-noise ratio increases, the average number of transmissions of FE-SRTS decreases to two. This is much lower than the average number of transmissions of the conventional ordered transmissions scheme, which does not employ feedback and does not update the order in which the nodes transmit.
\subsubsection{Decentralized Decision-Making for Multi-Agent Networks: the State-Dependent Case}
abstract:A generalization of decentralized detection is considered herein. In particular, each agent in the network exists in a set of pre-specified states that affect the distribution of their observations. As such, observations are conditionally dependent. It is shown that the Bayes optimal detection rule for each agent is a likelihood ratio test with a state dependent threshold. Furthermore, this detection rule at each agent can be different and is person-by-person optimal. The thresholds are determined by a system of nonlinear equations. Under the assumption of statistically independent states, the error exponent overall detection system is computed.Finally, it is shown that as the number of agents increases it is asymptotically optimal for the agents to use the same rule, thus dramatically reducing the complexity of computing the decision rules for each agent.
\section{Communications Software, Services and Multimedia Apps}
\subsection{Communication and Network Security}
\subsubsection{Combined Forest: a New Supervised Approach for a Machine-Learning-based Botnets Detection}
abstract:Nowadays, botnet-based attacks are the most prevalent cyber-threats type. It is therefore essential to detect this kind of malware using efficient bots detection techniques. This paper presents our security anomalies detection system, based on a model that we named Combined Forest. Our approach consists of merging some pre-processed Decision Trees to highlight different kinds of botnet by detecting their intrinsic exchanges. Using a supervised data approach, each tree is built from a labelled dataset. In order to achieve this, we aggregate the IP-flows into Traffic-flows to extract key features and avoid over-fitting. Then, we tested different machine learning algorithms and selected the most suitable one. After that, many experiments have been done to determine the best parameters and design the most accurate, adaptative and efficient model.
\subsubsection{Offset Circular Polarization Modulation Scheme}
abstract:In this paper, a framework of a novel communication system based on circular polarization modulation called Offset Circular Polarization Modulation (OCPM) is proposed which not only increases the channel capacity but also provides data security against eavesdropping. In conventional circular polarization modulation (CPM), only one bit of data (1 or 0) can be transmitted at one time by using either left hand circular polarization (LHCP) or right hand circular polarization (RHCP), limiting it to only 2-states polarization modulation scheme. However, the proposed OCPM scheme provides eight-fold increase in the polarization states. In OCPM, typical CPM constellations (LHCP or RHCP) can be shifted to different quadrants using an offset vector in order to increase the number of polarization states. In addition to switching the swing of circular polarization (CP) in OCPM, quasi-dc offset vectors are added to the horizontal and/or vertical components of the CP wave with different combinations. The reason why quasi-dc offset vectors have been used is because pure dc offsets cannot be wirelessly transmitted. The quadrant of the modulation is determined by the offset vectors. For instance, in order to shift a CP constellation (LHCP or RHCP) in the first quadrant, two positive offset vectors are required: one along horizontal-axis and the other along vertical-axis of the EM wave. Thus, OCPM is a dual dimension CPM scheme with polarization state as well as offset state. The eavesdropper would require both dimensions (i.e. Polarization state and offset state) to correctly determine the overall state of the OCPM scheme.
\subsubsection{Towards a Secure and Reliable Federated Learning using Blockchain}
abstract:Federated learning (FL) is a distributed machine learning (ML) technique that enables collaborative training in which devices perform learning using a local dataset while preserving their privacy. This technique ensures privacy, communication efficiency, and resource conservation. Despite these advantages, FL still suffers from several challenges related to reliability (i.e., unreliable participating devices in training), tractability (i.e., a large number of trained models), and anonymity. To address these issues, we propose a secure and trustworthy blockchain framework (SRB-FL) tailored to FL, which uses blockchain features to enable collaborative model training in a fully distributed and trustworthy manner. In particular, we design a secure FL based on the blockchain sharding that ensures data reliability, scalability, and trustworthiness. In addition, we introduce an incentive mechanism to improve the reliability of FL devices using subjective multi-weight logic. The results show that our proposed SRB-FL framework is efficient and scalable, making it a promising and suitable solution for federated learning.
\subsubsection{A Novel Machine Learning Framework for advanced Attack Detection Using SDN}
abstract:Recently, software defined networks (SDN) has emerged as novel technology that leverages network programmability to facilitate network management. SDN provides a global view of the network, through a logically centralized component, called SDN controller, to strengthen network security. SDN separates control plane from data plane, which allows for a more control over the network and brings new capabilities to cope with the new emerging security threats ($i.e.,$ zero-day attacks). Existing attack detection schemes are facing obstacles due to high false positive rates, low detection performances, and high computational costs. To address these issues, we propose a multi-module Machine Learning (ML) framework that combines unsupervised ML techniques with a scalable feature collection and selection scheme to effectively/timely detect network security threats in the context of SDN. In particular, our proposed framework consists of: (1) a data flow collection module (DFC) to gather the features of network data in a scalable and efficient way using sFlow protocol; (2) an Information gain Feature Selection (IGF) module to select the most informative/relevant features to reduce training and testing time complexity; and (3) a novel unsupervised ML module that uses a novel outlier detection scheme, called Isolation Forest (ML-IF), to effectively/timely detect network security threats in SDN. The experimental results using the well-known public network security dataset UNSW-NB15, show that our proposed framework outperforms state-of-the-art contributions in terms of accuracy and detection rate while significantly reducing computational complexity; making it a promising framework to mitigate the new emerging network security threats in SDN.
\subsubsection{Architecture and Performance Comparison of Permissioned Blockchains Platforms for Smart Contracts}
abstract:Blockchain and Smart Contracts ensure security and automation in trustless scenarios, leading to innovative solutions in various industry branches. The Hyperledger open-source project adopts these technologies in the corporate business, providing platforms for developing distributed applications.This paper analyses and compares two widely used platforms to develop applications based on permissioned blockchains: Hyperledger Sawtooth and Hyperledger Fabric. We implement two prototypes based on the same smart contract to evaluate the performance of each tool. The results show that: i) Sawtooth parallel transaction execution performs up to 30% better than serial execution only if the number of conflicting transactions remains low, and ii) Fabric consensus protocol presents a much higher transaction throughput than Sawtooth consensus protocol in equivalent scenarios.
\subsection{Communications and Mobility}
\subsubsection{Three-Tier Fuzzy-based Orchestration in MEC}
abstract:Handing over highly demanding tasks to remote or nearby computing units helps accommodate the service Quality of Service (QoS) requirements, and compensates for the limited computational capabilities of User Equipment (UE) such as smartphones and tablets. Task offloading is a promising technique being proposed for Virtualized Edge (VE) environments to solve a wide range of issues, frequently with the aim of enabling resource-intensive low-latency services. However, the volatile nature of 5G and B5G networks, as they continuously change due to dynamic policies, optimization processes, and users mobility, formalizes a major obstacle facing offloading and overall resource orchestration. To cope with such a challenge, under the scope of Multi-access Edge Computing (MEC), a three-tier fuzzy-based orchestration strategy is proposed with the aim of offloading the users workload to the optimum computing units to support stricter QoS requirements and reduce the perceived service delay. To evaluate our solution, we compare the proposed workload orchestrator with different employed algorithms. The evaluation shows that our orchestrator achieves nearly ideal performance, and outperforms the state-of-the-art approaches considered.
\subsubsection{IoTRoam - Design and implementation of an open LoRaWAN roaming architecture}
abstract:IoT technologies currently operate as independent silos, and roaming is possible only if there are prior interconnection agreements. To our knowledge, there are no standardised procedures for interconnecting different IoT networks for roaming. The focus of IoTRoam is to set up an operational roaming model that scales, seamlessly embed to existing IoT infrastructures and interconnects on a global basis with minimum initial configuration requirement. As a Proof-of-Concept, we have designed, implemented and tested a roaming LoRaWAN architecture using time-tested infrastructures in the Internet such as PKI and the DNS. The IoTRoam experience has helped us to propose changes to the LoRaWAN backend specifications that have been accepted. We also evaluated whether the proposed mechanisms satisfy the constrained IoT requirements.
\subsubsection{Radio Link Failure Prediction in 5G Networks}
abstract:Radio Link Failure (RLF) is a challenging problem in 5G networks. It reduces Indeed, RLF decreases the communication reliability and increases the latency. This is against the objectives of 5G, particularly for the ultra-Reliable Low Latency Communications (uRLLC) traffic class. RLF can be predicted using radio measurements reported by User Equipment (UE)s, such as Reference Signal Receive Power (RSRP), Reference Signal Receive Quality (RSRQ), Channel Quality Indicator (CQI), and Power HeadRoom (PHR). However, it is very challenging to derive a closed-form model that derives RLF from these measurements. To fill this gap, we propose to use Machine Learning (ML) techniques, and specifically a combination of Long Short Term Memory (LSTM) and Support Vector Machine (SVM), to find the correlation between these measurements and RLF. The RLF prediction model was trained with real data obtained from a 5G testbed. The validation process of the model showed an accuracy of 98% when predicting the connection status (i.e., RLF). Moreover, we illustrate the usage of the RLF prediction model, we introduced two use-cases: handover optimization and UAV trajectory adjustment.
\subsubsection{A Genetic Algorithm for the Placement of Latency-Sensitive Multiplayer Game Servers in the Fog}
abstract:Fog computing can be a promising paradigm, for enabling online, multiplayer games with stringent delays. An important problem to be addressed, within this context, deals with the placement of game servers. It consists of selecting suitable Fog nodes, for hosting the servers, inline with the Quality of Service (QoS) requirements of the respective games, while offering the needed computing capacities, at optimal costs. The problem becomes even more challenging, when games with geographically distributed player groups, are considered. In such situations, each server placement aims at achieving an acceptable quality of experience (QoE) for each individual player, with an overall good QoE for the group. This paper provides an efficient approach for the placement of game servers, satisfying, at the same time, QoS game requirements, capacity issues, while providing good player QoE and balancing costs. The problem being investigated deals with restricted network delays while minimizing server allocation costs.Our approach is based on an integer linear programming model formulation, with complex constraints dealing with player network delays, server capacities and hosting cots. An efficient and timely solution is provided, using a novel multi-penalty genetic algorithm (GA) heuristic. Results show that our approach outperforms others, in terms of solution quality and with computations speeds enabling its use in real-time settings.
\subsubsection{Reinforcement Learning for Task Placement in Collaborative Cloud- Edge Computing}
abstract:With the advantage of being close to the network, edge cloud- enabled computing mode brings flexibility to task scheduling. However, with the heterogeneity of computing resources between cloud and edge cloud, and the complexity of computing and communication processes between multi- edge cloud, challenges have been brought to the deployment and computing of tasks in cloud- edge collaborative environments. In order to solve this challenge, firstly a deep reinforcement learning controller based cloud- edge collaborative computing framework has been proposed. Then a system QoS model has been established considering both the user benefits and the service provider benefits. By using deep Q- network, a collaborative task placement deep reinforcement learning algorithm has been proposed for dynamically optimizing the target system utility. Finally, the experimental results show that the proposed method has a good learning ability for the computing cost of cloud and edge cloud as well as the communication cost between multi- edge cloud. In addition, compared with Q- table learning, random computing and cloud computing, a 10% improvement of system utility has been achieved with the proposed method.
\subsection{Forensics and Tracking}
\subsubsection{A Common but Flexible Method for IoT DeviceForensics}
abstract:IoT devices are becoming prevalent and changing the world from many different aspects. From a digital forensic perspective, IoT devices are rich-evidence sources for criminal and civilian investigations because of their ubiquity. However, safely retrieving forensically sound evidence from IoT devices has been facing challenges due to their diversities in shape, size, function, internal circuit density, etc.. Forensic investigators are looking forward to a common, cost-effective, easy-to-use process or method for their IoT evidence acquisition. In this paper, we reveal the potential big issue of traditional evidence acquisition methods when they are applied in IoT devices forensics. Then, we propose a common forensic process for most of the IoT devices and utilize the up-to-date 3D print technique combined with the PoGo pins and Epoxy to customize the safe evidence acquisition tool for a specific IoT device. Our experiment result proved the generality, flexibility, feasibility of our process and method.
\subsubsection{Understanding Digital Forensic Characteristics of Smart Speaker Ecosystems}
abstract:With a built-in intelligent personal voice assistant providing Q&A services, smart speaker ecosystems combine multiple compatible components, including the internet of things (IoT) technology, mobile devices, and cloud computing. However, as it is closely related to people's daily lives, security and privacy issues have gained worldwide attention. Components in the ecosystem are interconnected and chained together to enable the ecosystem to perform increasingly diverse operations. By collecting meaningful data from smart speaker ecosystems, we can reconstruct user behavior and provide a holistic explanation for finding the root cause of an observable symptom. This highlights the need for digital forensic research to enhance the security and privacy of smart speaker ecosystems. In this paper, we first discuss the digital forensic characteristics of a smart speaker ecosystem. Then, we propose a proof-of-concept digital forensic tool based on data provenance, that supports the identification, acquisition, and analysis of client-side artifacts from local devices.
\subsubsection{Meta-Pose: Environment-adaptive Human Skeleton Tracking with RFID}
abstract:Human pose estimation has attracted great interestrecently. Considerable efforts have been made in Radio-Frequency (RF) sensing techniques for human pose estimationwithout using a video camera. Although the existing RF basedschemes can well protect user privacy, they are usually sensitiveto the RF environment and are hard to generalize to new environments.In this paper, we analyze the challenges of generalizationof Radio-Frequency Identification (RFID) based human posetracking systems. We then present an RFID based 3D humanskeleton tracking system, termed Meta-Pose, which incorporatesmeta-learning and few-shot fine-tuning to achieve high adaptabilityto new environments. The proposed system is implementedwith commodity RFID devices and extensive experiments areconducted for performance evaluation. The experiment resultsvalidate the superior human skeleton estimation performanceand high adaptability of the proposed Meta-Pose system.
\subsubsection{MOTrack: Real-time Configuration Adaptation for Video Analytics through Movement Tracking}
abstract:Video analytics has many applications in traffic control, security monitoring, action/event analysis, etc. With the adoption of deep neural networks, the accuracy of video analytics in video streams has been greatly improved. However, deep neural networks for performing video analytics are compute-intensive. In order to reduce processing time, many systems switch to the lower frame rate or resolution. State-of-the-art switching approaches adjust configurations by profiling video clips on a large configuration space. Multiple configurations are tested periodically and the cheapest one with a desired accuracy is adopted. In this paper, we propose a method that adapts the configuration by analyzing past video analytics results instead of profiling candidate configurations. Our method adopts a lower/higher resolution or frame rate when objects move slow/fast. We train a model that automatically selects the best configuration. We evaluate our method with two real-world video analytics applications: traffic tracking and pose estimation. Compared to the periodic profiling method, our method achieves 3%-12% higher accuracy with the same resource cost and 8-17x faster with comparable accuracy.
\subsubsection{Photonic sensors for non-invasive home monitoring of elders}
abstract:In this paper, we present an optical fiber based architecture for non-invasive home monitoring of elder citizens. The approach is based on a network of optical fiber sensors distributed along the space/room to be monitored. The sensing mechanism is based on optical fiber Bragg grating (FBG) sensors, produced by the phase mask method and integrated within an accelerometer structure. This type of sensing solution has high sensitivity allied with an extra resilience. Here we present the proposed architecture, the evaluation of different parameters that influence the accelerometer feedback, and the theoretical approach for indoor localization using this type of sensing mechanism. One advantage of the proposed solution is that it does not depend on wearables, which are considered a huge burden for elders.
\subsection{Network Virtualization}
\subsubsection{VAPNIC: A VersAtile shortest path-free VNF Placement using a divide-and-coNquer tactIC}
abstract:Orchestration mechanisms play a pivotal role in assisting service providers in deploying their increasingly complex virtual network services seamlessly thanks to Network Function Virtualization (NFV) and Software-defined networking (SDN) technology enablers. Unfortunately, existing state-of-the-art orchestration techniques suffer from non-scalability and time-efficiency aptitude when the services require VNFs to be distributed across cloud and edge environments with complex dimensions. Furthermore, they provide competitive solutions in good execution time only for small-scale scenarios (e.g., in seconds for 50 nodes) but generally require an exorbitant amount of time to converge towards feasible solutions for medium-scale or even large-scale schemes.This paper proposes VAPNIC: an innovative approach that solves the VNF placement and chaining problem with lower algorithmic complexity using a disjoint-set data structure aided divide-and-conquer strategy. Our method's unique design is the non-use of any existing shortest path search algorithms to chain the virtual network functions. To the best of our knowledge, this is the first work that strives to tackle the placement and chaining of the VNFs from a distinctive perspective in the case of medium-and-large-scale scenarios with a fast and scalable heuristic that exploits the divide-and-conquer design paradigm based on multi-branched recursion.Experimental results indicate that VAPNIC outperforms existing approaches in acceptance rate, resource utilization, scalability, and time-efficiency.
\subsubsection{Fair Virtual Network Function Scheduling with Deep Reinforcement Learning}
abstract:Network function virtualization aims at deploying network functions on general-purpose hardwares, referred to as virtual network functions (VNFs), rather than the specialized devices. A network service can be implemented by scheduling different VNFs. In this paper, we study the VNF scheduling problem with the objective of satisfying the diversified end-to-end delay requirements while maintaining the fairness among different network services. We formulate the problem as a mixed integer linear program and propose a VNF scheduling algorithm based on deep reinforcement learning, in which proximal policy optimization is adopted to optimize the policy network. Numerical results show that the proposed scheduling algorithm outperforms other canonical ones and the designed policy network can be scaled up to fit different problem sizes.
\subsubsection{Engineering and Experimentally Benchmarking a Serverless Edge Computing System}
abstract:Thanks to the latest advances in containerization, the serverless edge computing model is becoming close to reality. Serverless at the edge is expected to enable low latency applications with fast autoscaling mechanisms, all running on heterogeneous and resource-constrained devices. In this work, we engineer and experimentally benchmark a serverless edge computing system architecture. We deploy a decentralized edge computing platform for serverless applications providing processing, storage, and communication capabilities using only open-source software, running over heterogeneous resources (e.g., virtual machines, Raspberry Pis, or bare metal servers, etc). To achieve that, we provision an overlay-network based on Nebula network agnostic technology, running over private or public networks, and use K3s to provide hardware abstraction. We benchmark the system in terms of response times, throughput and scalability using different hardware devices connected through the public Internet. The results show that while serverless is feasible on heterogeneous devices showing a good performance on constrained devices, such as Raspberry Pis, the lack of support when determining computational power and network characterization leaves much room for improvement in edge environments.
\subsubsection{Placement, Routing and Scheduling Optimizations in Cloud-RAN}
abstract:The density increasing in Radio Access Networks (RAN) caused the migration of traditional base stations to the cloud to meet huge traffic of end-users' demands. In this context, virtualization techniques can add more flexibility and programmability to scale in/out virtual storage, network and computing resources. However, Cloud-RAN (C-RAN) requires real-time processing and scheduling of its demands represented as service chains. In this paper, we formulate the joint assignment and scheduling problem in C-RAN using linear programming approach. Placement and scheduling algorithms allowing to allocate efficiently computing resources for C-RAN Virtual Network Functions (VNFs) with respect to the RAN services chaining are introduced and their behavior is quantified through real traces. We illustrate and highlight the feasibility and efficiency of our proposed algorithms through different scenarios in various considered network instances. Metrics such as cpu cores occupancy, network throughput, and successful subframe decoding rate are used to illustrate our algorithms' efficiency.
\subsubsection{Evaluating Softwarization Gains in Drone Networks}
abstract:Unmanned Aerial Systems (UASs) or drones are becoming increasingly dependable tools for many industrial and commercial applications. Due to the increasing usage and capabilities of drones coupled with advances in innovative technologies and algorithms for managing and conducting tasks, drones are expected to crowd low-altitude airspace in urban areas. This brings many opportunities for service providers to provide drone-related services. Hence, efficient use of drones is required. In this paper, we investigate the benefits of reconfigurable softwarized drones operated by an entity or a service provider to perform tasks for its operations or for interested customers. We model a system of reconfigurable drones that can conduct multiple tasks per flight using capable drone-mounted computing systems with Virtual Network Functions (VNFs). We compare our proposed system with alternatives with limited and no softwarization capabilities. Results show that softwarization allows drones to perform a variety of tasks using a limited number of reconfigurable drones and in a shorter time.
\subsection{Networking and Learning}
\subsubsection{Taxonomy of Machine learning techniques and elastic resource optimization in sliced networks}
abstract:With the diversification of 5G services especially the emergence of applications related to the Internet of Things (IoT), slicing is considered as the best way to offer the optimal Quality of Services (QoS) and Quality of Experience (QoE) using the same shared infrastructure.In this paper, we focus on the optimal resource reservation using recent methods and techniques in order to meet the highly variable and dynamic demands. That's why, we propose a taxonomy of different approaches using Artificial Intelligence/Machine Learning (AI/ML) for slicing resource optimization and orchestration. In addition, a comparison study of different solution is proposed. Finally, we highlight future challenges and discuss future research directions.
\subsubsection{An Approach to Network Service Placement using Intelligent Search Strategies over Branch-and-Bound}
abstract:Network Function Virtualization (NFV) has been a significant shift from traditional dedicated hardware devices towards reusable software modules running over lightweight virtualized environments. While it brings many promising opportunities, it introduces several unprecedented complexities that need further considerations. Efficient placement of services is essential for achieving NFV expectations. We propose a highly reliable solution for systematically placing network services, touching the optimal results while maintaining the scalability, making it suitable for online scenarios with strict time constraints. We organized our solution as a Branch and Bound search structure, which leverages Artificial Intelligence (AI) search strategies (Especially A-Star) to address the placement problem, following the popular objective of Service Acceptance (SA). Extensive empirical analysis has been carried out and the results confirm significant improvements.
\subsubsection{Machine Learning based Root Cause Analysis for SDN Network}
abstract:Nowadays, the rapid growth of the Internet makes network management more complex due to the various and complicated network problems. In the past, network administrators implemented troubleshooting approaches (e.g., ping, traceroute, etc.) manually to identify the root cause of problems. However, it is not effective due to a human intervention and an increase of network devices. Consequently, the root cause analysis is considered by the research community. There are existing studies for the root cause analysis without human intervention (e.g., statistical approaches, heuristic algorithms, etc.). However, these approaches shows limited performance (e.g., due to complex threshold identification, etc.). The emerging of machine learning (ML) and deep learning is a potential solution to overcome this obstacle, offering opportunity to develop an effective root cause analysis approach. Therefore, in this paper, we propose a root cause analysis approach using ML and time-series network parameters to identify the root cause of problems in the network. In this approach, we consider a balance between accuracy and time complexity of ML algorithms to select an appropriate ML technique. Moreover, we contribute troubleshooting datasets to identify three kinds of root causes including link failure, switch failure and buffer overloaded. The experimental results show that the proposal can achieve approximately 97 percent of precision, recall and f1-score in considered scenarios and require less processing time (0.00143 ms for a sample) in comparison with other ML algorithms.
\subsubsection{Dynamic CU-DU Selection for Resource Allocation in O-RAN Using Actor-Critic Learning}
abstract:Recently, there has been tremendous efforts by network operators and equipment vendors to adopt intelligence and openness in the next generation radio access network (RAN). The goal is to reach a RAN that can self-optimize in a highly complex setting with multiple platforms, technologies and vendors in a converged compute and connect architecture. In this paper, we propose two nested actor-critic learning based techniques to optimize the placement of resource allocation function, and as well, the decisions for resource allocation. By this, we investigate the impact of observability on the performance of the reinforcement learning based resource allocation. We show that when an NF is dynamically relocated based on service requirements, using reinforcement learning techniques, latency and throughput gains are obtained.
\subsubsection{SCHEMA: Service Chain Elastic Management with Distributed Reinforcement Learning}
abstract:As the demand for Network Function Virtualization accelerates, service providers are expected to advance the way they manage and orchestrate their network services to offer lower latency services to their future users. Modern services require complex data flows between Virtual Network Functions, placed in separate network domains, risking an increase in latency that compromises the offered latency constraints. This shift requires high levels of automation to deal with the scale and load of future networks. In this paper, we formulate the Service Function Chaining (SFC) placement problem and then we tackle it by introducing SCHEMA, a Distributed Reinforcement Learning (RL) algorithm that performs complex SFC orchestration for low latency services. We combine multiple RL agents with a Bidding Mechanism to enable scalability on multi-domain networks. Finally, we use a simulation model to evaluate SCHEMA, and we demonstrate its ability to obtain a 60.54% reduction of average service latency when compared to a centralised RL solution.
\subsection{Service and Mobility Management}
\subsubsection{Blockchain and FL-based Network Resource Management for Interactive Immersive Services}
abstract:The vision of Beyond 5G (B5G) and 6G networks is based on the design of novel advanced services leveraged for future smart cities. Interactive immersive applications are an example of those enabled services, which allow for the interaction between multiple users in a 3D environment created by virtual presentations of real objects and participants using various concepts such as Virtual Reality (VR), Augmented Reality (AR), Extended Reality (XR), digital twin and holography. These applications require advanced computing models which allow for the processing of massive gathered amounts of data. Indeed, each motion, gesture and object modification should be captured, added to the virtual environment, and shared with all the participants. Two key factors are then required: synchronization and precision. Relying only on the cloud to process this data can cause significant delays. Therefore, a hybrid cloud/edge architecture is to be adopted. The openness of B5G and 6G networks enable various stakeholders to participate in this platform, incorporating servers with various capabilities. Therefore, an intelligent resource orchestration mechanism, able to allocate the available capacities efficiently is highly required. In this paper, a blockchain and federated learning-enabled predicted edge-resource allocation (FLP-RA) algorithm is introduced to manage the allocation of computing resources in B5G networks. It allows for smart edge nodes to train their local data and to share it with other nodes to create a global estimation of the future system load. Then, these nodes are able to make accurate decisions to distribute the available resources to provide the lowest computing delay.
\subsubsection{RevOPT: An LSTM-based Efficient Caching Strategy for CDN}
abstract:In order to face the rise in data consumption and network congestion, caching structures like Content Delivery Networks (CDNs) are being more and more used and integrated into the network infrastructure. Knowing that the capacities of caching resources are most often limited due to their large operational cost, it has become very important that these entities are managed efficiently. Especially, at the caching operations level, the question that arises is what content should be cached or evicted from the cache when it becomes full. Having these in mind, we introduce a lightweight Artificial Intelligence-based caching scheme called Reversed OPT (RevOPT). In our proposal, we use a Long Short-Term Memory (LSTM) encoder-decoder model to learn future requests patterns from the past and exploit its outcome with a Counting Bloom Filter (CBF) structure to manage efficiently the caching decisions and to keep in the cache only contents expected to be reused in the near future. The conducted simulations show promising results of RevOPT in terms of the cache hit ratio compared to existing caching algorithms.
\subsubsection{Joint Node-Link Algorithm for Embedding Virtual Networks with Conciliation Strategy}
abstract:Network virtualization (NV) has widely envisioned as a crucial factor for the success of the future networks by enabling a flexible, cost-effective and on-demand deployments of multiple network service requests on a shared physical infrastructure. The major challenge of NV is to efficiently and effectively embed heterogeneous virtual network requests (VNRs), consisting of a set of virtual nodes connected by virtual links, onto a shared substrate network meeting various stringent resource constraints. Most of the research papers in this field have merely focused on separate virtual node mapping (VNoM) or virtual link mapping (VLiM) with scalable heuristic algorithms. The lack of a coordination between node and link mapping stages results in low acceptance ratio as well as network revenues. In this paper, we propose a new approach based on Genetic Algorithm (GA) that jointly coordinates node and link mappings where the link mapping is relied on a path ranking method. A novel heuristic conciliation mechanism is introduced to handle a possible set of infeasible link mappings during generating virtual embedding solutions in GA's operations. Extensive evaluation results show that our proposed GA-based algorithm outperforms state-of-the-art virtual embedding algorithms in all performance metrics we adopted.
\subsubsection{Node Localization in WSN and IoT Using Harris Hawks Optimization Algorithm}
abstract:During the last decade Wireless Sensor Networks and IoT have taken an overwhelming place in engineering and environmental applications. Thus, with growing interest in large size and complex networks, some critical challenges have to be taken care of such as energy consumption and nodelocalization, especially when dealing with real-time applications. Such factors affect strongly the system performance in sensitive fields like e-health or military applications.The primary purpose is to determine the location of the sensor node that triggers the event. Energy consumption of each node is a serious problem for WSN when it comes to extend the life time of the whole network.In this paper, we propose a multicriteria optimization scheme based on a bio-inspired algorithm called Harris Hawks Optimization Algorithm (HHOA).It is shown that the proposed paradigm is capable of increasing the localization rate, as well as minimizing the energy consumption of the nodes. HHOA populations are able to share information in a multi-agent fashion to compute the position of the trigger sensors. Developing this algorithm on a huge WSN with millions of nodes reveals a particularly high performance. To assess this, several experiments in different scenarios are carried out in adecentralized environment of WSN. Finally, a comparative study is carried out also to some recent bio-inspired algorithms.
\subsubsection{Resource Allocation in THz-based Subcarrier Index Modulation Systems for Mobile Users}
abstract:Subcarrier Index Modulation (SIM) has recently received a significant research efforts analyzing its different performance aspects. In this paper, performance analysis of SIM over TeraHertz (THz) frequency band is conducted considering mobile users. The mobility model adopted is Random WayPoint (RWP) model with different numbers of mobility dimensions. Moreover, different resource allocation schemes are considered, including fixed, random and distance-aware resource allocation schemes. Closed form expression for the average bit error rate are derived for THz-based SIM considering mobile users and all considered resource allocation schemes. Simulation results with the molecular absorption effect on THz-based system validate the accuracy of the derived mathematical formulas.
\subsection{Services and Applications}
\subsubsection{Federated Learning for Energy-Efficient Thermal Comfort Control Service in Smart Buildings}
abstract:Energy efficiency and occupant thermal comfort are considered as high-interest topics in a smart building. Internet of Things (IoT) technology enables smart building management and operation to improve building energy efficiency and occupant thermal comfort. In this paper, we use IoT-generated data to derive accurate thermal comfort and electricity load forecast model for smart building control. Due to privacy concerns and high accuracy targets, we take advantage of the use of edge computing and federated learning. Federated learning is a decentralized machine learning scheme that permits data volume increase and data diversity in a training model while preserving privacy. Different households contribute to the training process without revealing privacy. A deep neural network is used to model the relationship between the environmental variables, controllable building operations, and thermal comfort. The Long-Short Term Memory is used as well to forecast the energy load using previous observations of the household electrical load and real-time environmental variables. Finally, the derived thermal comfort model is used to control the smart building environment by searching for the optimal cooling set-point, which results in the desired comfort while decreasing the total energy consumed. Results demonstrate the performance of federated learning settings in terms of accuracy and prediction. The control proposition results as well in less energy consumption within a comfort zone.
\subsubsection{ServEx: Service Exchange Among Multiple SCSPs in Sensor-Cloud for IoT Applications}
abstract:This paper introduces an autonomous service exchange scheme, ServEx, among multiple Sensor-Cloud Service Providers (SCSPs) in a sensor-cloud (SC) platform for Internet of Things (IoT) applications. Typically, SC offers Sensors-as-a-Service (Se-aaS) using the concept of sensor virtualization for serving different IoT applications seamlessly in real-time. On the other hand, an SC platform diminishes the strain of sensor deployment and management from the user by employing SCSP. In an SC platform, the presence of a single SCSP may be incapable of serving an entire IoT application requested by an end-user due to the lack of sufficient sensor nodes present in the application region. However, the presence of multiple SCSPs in an SC platform plays a complementary role in serving an IoT application entirely by implementing the idea of service exchange among them. The proposed scheme, ServEx, enables service exchange among multiple SCSPs in an SC platform. In ServEx, we apply a 2-phase approach -- in the first phase, we incorporate a data structure to store the service profile of end-users and SCSPs, whereas, in the second phase, we design a profile matching mechanism for enabling SCSPs to find desired sensor nodes registered to different SCSPs. We evaluate ServEx through extensive experiments and observe that it reduces the average delay by 57% and the average energy consumption by 74% and increases the capability of complete service provisioning for each SCSPs.
\subsubsection{Edge Computing-Assisted Multimedia Service Energy Optimization based on Deep Reinforcement Learning}
abstract:With the development of communication technology, emerging multimedia (e.g. virtual reality) can provide users with more immersive service experience. However, due to the ultrahigh rendering and splicing requirements of multimedia content, the higher demand for computing resources is put forward for the playback device. The anomalies of energy consumption and latency caused by such computationally intensive tasks hinder the practical application of emerging multimedia technology in mobile networks. In this regard, this paper proposes an edge computing assisted multimedia service optimization scheme (ECMSO) to broaden the computing capacity of the viewer(i.e. requester), so as to ensure that content can be served in time and reduce the energy cost of computation from the perspective of executor and requester, respectively. First, a computational offloading scheme based on deep reinforcement learning is designed. It optimizes intelligently the energy consumption while meeting the latency requirements of the requester. Secondly, a heuristic algorithm to allocate power, bandwidth, and computing resources for candidate executors is proposed. Finally, a series of simulation experiments are conducted to demonstrate the effectiveness of our proposed scheme.
\subsubsection{A Bayesian Game Model for Dynamic Channel Sensing Intervals in Internet of Things}
abstract:A Bayesian game theoretic model is developed to dynamically select channel sensing intervals in a massively dense network of Internet of Things.In such networks, the core objective is to minimize every node's energy consumption while having incomplete information about other nodes actively communicating in the network. Selecting channel sensing intervals in a medium access control (MAC) protocol is absolutely crucial, especially in massively dense networks, and selecting intelligently these intervals can optimize the overall network energy consumption while also minimizing latency during the information transfer.In the proposed model, a sensing interval chosen by a node is dynamically derived using current and previous incoming traffic patterns at other nodes in the vicinity.This paper shows that formulating the problem of channel sensing intervals as a Bayesian game model can extensively improve the performance of a MAC protocol when incorporating information from other nodes within the network.
\subsubsection{Semi-Deterministic Deployment based Area Coverage Optimization in Mobile WSN}
abstract:K-coverage by a minimum number of wireless sensor nodes is a major problem when using homogeneous and heterogeneous wireless sensor networks. Solutions proposed in the literature guaranteeing k-coverage in WSN affect either the energy, the network lifetime, or the quality of service. In this paper, we proposed a minimal semi-deterministic deployment model in a mobile wireless sensor network. This model is based on Pick's theorem to guarantee a maximum 1-coverage. To do so, the area of interest is divided into square sub-areas according to a pre-established grid, then the sensor nodes are initially randomly deployed around each sub-area center. The main strengthen of our proposed deployment approach is the minimization of the nodes' movement from their initial positions. In fact, each node moves once and remains active until it is exhausted. The results show a high efficiency in terms of increasing the network lifetime and the coverage compared to some well-known approaches.
\subsection{Vehicular and 5G Communications}
\subsubsection{Adaptive Vehicle Platooning With Joint Network-Traffic Approach}
abstract:The intelligent transport system (ITS) has become one of the most globally researched topics with Connected Autonomous Vehicles (CAV) at its core. The CAV applications can be improved by study of vehicle platooning immune to real time traffic and vehicular network losses. In this work, we explore the need of integration of Network model and Platooning system model to bridge the research gap. The proposed platoon model is adaptive in length with provision for independent ego vehicle to merge and exit thus dropping the assumption of common source and destination for the whole platoon. The upper bound on this platoon length is analysed using the Joint Network Traffic model based on SINR, reaction time delay and headway distance between two following vehicles in a platoon. The paper also discusses challenges in inter and intra-platoon communication and graph based network for relay selection and optimization of the resource allocation. The simulated cases test the performance for short contact time events like road intersections and lane merging. The reaction time delay introduces instability in the platoon which can be reduced by tuning the correction parameters, thus maintaining the headway distance within safety bounds. The research of this paper can provide new ideas for high QoS inter platoon communication as a response to unforeseen traffic events.
\subsubsection{A Personalized Learning Scheme for Internet of Vehicles Caching}
abstract:The emergence of Internet of Vehicles (IoV), as a large-scale distributed system, introduces new research and development challenges for supporting resource-constrained devices. In fact, the latency of retrieving contents and performing the desired tasks may increase dramatically and failures may occur when resource limits are exceeded.In this paper, we assess the use of advanced machine learning paradigms to achieve more accurate personalized edge caching and replacement decisions, while supporting data privacy, vehicle mobility, time-varying and location-aware content popularity. Firstly, we show that popular federated learning based schemes fail in maintaining acceptable performance under the above settings. Secondly, we propose a scalable-by-design edge caching scheme for IoV, leveraging decentralized region-to-region Road Side Units (RSUs) exchanges, while enhancing region local models. Finally, simulation results show that our scheme achieves higher performance in terms of average delay and edge hit ratio while keeping the cost and privacy risks at the minimum.
\subsubsection{Video Service-Oriented Vehicular Collaboration: A Multi-Agent Proximal Policy Optimization Approach}
abstract:To guarantee heterogeneous performance requirements of diverse vehicular services, it is necessary to design a full cooperative policy for both vehicle to infrastructure (V2I) links and vehicle to vehicle (V2V) links.This paper investigates how to improve the quality of experience (QoE) of the V2I users for video services while satisfying the delay requirements of both V2I and V2V links.In specific, a QoE maximization problem is formulated with consideration of vehicular collaboration where task offloading decision, channel reuse decision and power allocation of V2V users are all included.A multi-agent reinforcement learning (MARL) framework is then designed, where a new reward function is proposed to evaluate the utilities of the considered network.Thereafter, a proximal policy optimization approach is proposed to enable each V2V user to learn policy individually with the shared global network reward.The effectiveness of the proposed approach is finally validated with comparison of other baseline approaches through extensive simulation experiments.
\subsubsection{Frame-Level Integrated Transmission for Extended Reality over 5G and Beyond}
abstract:Extended reality (XR) is one of the most important media applications enabled by 5G and beyond. Providing XR services over 5G networks is very challenging due to high requirements in terms of data rate, reliability, and latency. Different from eMBB and URLLC services, XR has a unique characteristic that each XR video frame can be segmented into several correlated packets. Any one of the packets loss would result in frame decoding error, and thus bad quality of experience. In this paper, we investigate such characteristic of frame-level integrity and propose a new concept called frame-level integrated transmission. Unlike conventional wireless communication with the goal of transmitting one packet successfully, the proposed frame-level integrated transmission takes "multiple packets belonging to one video frame" as an "entity". Specifically, we formulate a resource allocation problem by considering the frame-level integrity to maximize the number of satisfied UEs under the data rate, reliability, and latency requirements for each UE. This problem is highly challenging due to the discontinuous and non-convex objective function and intractable reliability constraint. We propose to solve it approximately by jointly considering the user admission control and a frame-level integrated transmission problem to maximize the total number of frames that are successfully transmitted, which is efficiently solved via smooth function approximation and convex-concave procedure. Simulation results demonstrate the superiority of the proposed frame-level integrated transmission scheme, which can achieve much better performance than the conventional proportional fairness scheduler in terms of various performance metrics.
\subsection{Video Streaming}
\subsubsection{Cybersickness-aware Tile-based Adaptive 360 Video Streaming}
abstract:In contrast to traditional videos, the imaging in virtual reality (VR) is 360, and it consumes larger bandwidth to transmit video contents. To reduce bandwidth consumption, tile-based streaming has been proposed to deliver the focusedpart of the video, instead of the whole one. On the other hand, the techniques to alleviate cybersickness, which is akin to motion sickness and happens when using digital displays, have not been jointly explored with the tile selection in VR. In this paper, we investigate Tile Selection with Cybersickness Control (TSCC)in an adaptive 360 video streaming system with cybersickness alleviation. We propose an m-competitive online algorithm with Cybersickness Indicator (CI) and Video Loss Indicator (VLI) to evaluate instant cybersickness and the total loss of video quality. Moreover, the algorithm exploits Sickness Migration Indicator (SMI) to evaluate the cybersickness accumulated over time and the increase of optical flow to improve the tile quality assignment. Simulations with a real network dataset show that our algorithm outperforms the baselines regarding video qualityand cybersickness accumulation.
\subsubsection{Quality Optimization of Live Streaming Services over HTTP with Reinforcement Learning}
abstract:Recent years have seen tremendous growth in HTTP adaptive live video traffic over the Internet. In the presence of highly dynamic network conditions and diverse request patterns, existing yet simple hand-crafted heuristic approaches for serving client requests at the network edge might incur a large overhead and significant increase in time complexity. Therefore, these approaches might fail in delivering acceptable Quality of Experience (QoE) to end users. To bridge this gap, we propose ROPL, a learning-based client request management solution at the edge that leverages the power of the recent breakthroughs in deep reinforcement learning, to serve requests of concurrent users joining various HTTP-based live video channels.ROPL is able to react quickly to any changes in the environment, performing accurate decisions to serve clients requests, which results in achieving satisfactory user QoE. We validate the efficiency of ROPL through trace-driven simulations and a real-world setup. Experimental results from real-world scenarios confirm that ROPL outperforms existing heuristic-based approaches in terms of QoE, with a factor up to 3.7x.
\subsubsection{EC-360: Speeding Up 360 Video Streaming Using Tile-based Online Erasure Coding}
abstract:360 video services require extremely high bitrate and frame rate videos for a good immersive experience. Traditional solutions for adaptive bitrate streaming are still limited by currently insufficient and fluctuating bandwidth. Besides, viewpoint-aware or tile-based solutions would lead more rebuffering due to the short viewpoint prediction window. In this paper, we present EC-360, a novel video streaming framework to speed up 360 video streaming. Specifically, it creatively integrates tile- based online erasure coding into multi-source content delivery to mitigate "cask effect" or impact of straggler node, which oftentimes leads to failure to improve delivery speed in multi-source streaming. We formulate the critical tile-based request scheduling problem in EC-360 based on the Combinatorial Multi-armed Bandit (CMAB) model and develop a low complexity online algorithm. We further theoretically verify the efficiency of the CMAB-based algorithm by deriving its regret upper bound. Extensive experiments based on prototype implementation and real network traces corroborate the efficiency, flexibility, and lightweight of proposed solution. EC-360 achieves superior performance improvement compared to state-of-the-art works in various network scenes and system settings.
\section{Green Communications Systems and Networks}
\subsection{Energy Efficient UAV and Vehicular Networks}
\subsubsection{Position Optimization and Resource Management for UAV-Assisted Wireless Sensor Networks}
abstract:In this paper, we focus on the energy saving problem for the wireless sensor networks (WSNs). Specifically, we propose a UAV-assisted wireless network architecture, where the celledge sensor devices (SDs) can access the aerial access points (AAPs) instead of the terrestrial access point (TAP). Since the transmitter-to-receiver distance is shortened and the ground-toair channel is usually line-of-sight, the SDs can use lower power to transmit data and thereby prolong their lifetime. To fully exploit the potential of the network architecture, we jointly optimize the AAPs' position, channel allocation, and power control to minimize the total transmission power of all SDs. In order to solve the complex joint optimization problem, we reformulate it as three tractable subproblems and use the methods in graph theory to design low-complex algorithms. Simulation results indicate that the proposed network architecture greatly outperforms the traditional WSNs, and the proposed algorithms can further reduce the total power consumption.
\subsubsection{Energy-aware Control Of UAV-based Wireless Service Provisioning}
abstract:Unmanned aerial vehicle (UAV)-assisted communications have several promising advantages, such as the ability to facilitate on-demand deployment, high flexibility in network reconfiguration, and high chance of having line-of-sight (LoS) communication links. In this paper, we aim to optimize the UAV control for maximizing the UAV's energy efficiency, in which both aerodynamic energy and communication energy are considered while ensuring the communication requirements for each ground terminal (GT) and backhaul link between the UAV and the terrestrial base station (BS). The mobility of the UAV and GTs lead to time-varying channel conditions that make the environment dynamic. We formulate a nonconvex optimization for controlling the UAV considering the practical angle-dependent Rician fading channels between the UAV and GTs, and between the UAV and the terrestrial BS. Traditional optimization approaches are not able to handle the dynamic environment and high complexity of the problem in real-time. We propose to use the Trust Region Policy Optimization (TRPO) method that can improve the performance of the UAV compared to the Deep Deterministic Policy Gradient (DDPG) method in such a dynamic environment as in this paper.
\subsubsection{Joint UAV Location and Resource Allocation for Air-Ground Integrated Federated Learning}
abstract:With the popularity of fifth generation (5G) networks and envision of sixth generation (6G), varied artificialintelligence (AI) services gradually develop from the networkcenter to the edge, which makes unmanned aerial vehicle (UAV)a hot spot to provide auxiliary services of machine learning (ML)to empower terrestrial users intelligence. However, due to thesensitive privacy and limited resources, traditional centralizedML may not be used directly in such networks. As a promisingdistributed collaborative ML, federated learning (FL) couldbe more suitable. Meanwhile, unlike conventional FL workingon terrestrial networks, applying FL in UAV-assisted networksshould strictly consider the impact of air-ground wireless channelcaused by the maneuverability of UAVs, as well as the allocationof various network resources. To address these challenges, wepropose to jointly optimize the UAV location and resourceallocation, subject to the constraints of learning accuracy andtraining latency to minimize the energy consumption of terrestrialusers. The formulated complicated non-convex problem is efficiently solved by an alternating optimization algorithm based onsuccessive convex approximation (SCA) approaches after problem decomposition. Simulations results show that our proposedalgorithm can reduce more overall users' energy consumptionthan three benchmarks while guaranteeing the learning accuracywithin the maximum training latency.
\subsubsection{Joint Vehicle Association and Power Allocation for Energy Efficient Connected Automated Vehicles}
abstract:Connected Automated Vehicle (CAV) is a promising paradigm for achieving safe and intelligent transportation systems. In CAV scenario, massive raw sensor data needs to be shared among vehicles under strict latency and high data rate constraints, which poses critical challenges on existing low data rate vehicular communication on 5.9 GHz band. To address thesechallenges, we propose a millimeter wave (mmWave) enabled CAV network with high capacity to support the raw sensor data sharing among vehicles. A joint vehicle association and power allocation (JVAPA) algorithm is proposed to maximize date rate while minimize energy consumption for CAVs. The one-to-many vehicle association problem is formulated as a swap matching model, and the stable matching states for CAVs and BSs association are achieved. The non-convex power allocation problem is transformed into a convex problem using the first order Taylor expansion, and the optimal power allocation results are achieved. Numerical results verify that the proposed JVAPA algorithm can significantly improve the energy efficiency compared with the benchmark algorithms.
\subsubsection{Cooperative Task Offloading in UAV Swarm-based Edge Computing}
abstract:Mobile edge computing (MEC) has been envisioned as a promising technology to meet ever-increasing demands on computational resources. Due to the fixed deployment and limited coverage of conventional MEC, unmanned aerial vehicle (UAV) edge computing began to receive attention with the advantage of flexibility and controllability. However, single UAV edge computing is not competent for complex scenarios with the limitations of computing capabilities and coverage.On the other hand, although multi-UAV edge computing could improve the situation, the long processing delay and insufficient utilization of resources still restrict the communication and cooperative computation among UAVs. Characterized by the unique swarm cooperative communication and computing, UAV swarm-based edge computing can realize more complex task computing and higher computational efficiency. Toward this end, we provide this paper to study the cooperative task offloading problem in UAV swarm-based edge computing, aiming to minimize the overall task processing delay. To solve this problem, we adopt an optimal cooperative computation offloading method. Experimental results demonstrate the importance and high performance of UAV swarm-based edge computation and the low complexity of our proposed method.
\subsection{Green Communication Networks-1}
\subsubsection{Energy Sharing based Cooperative Dual-powered Green Cellular Networks}
abstract:Solar enabled and grid connected "dual-powered" base stations (BSs) have developed as a cost effective solution to network operators. While these networks prevent energy outages and are effective in providing seamless operation for catering to the user quality of service (QoS), they still rely significantly on the power-grid, generating carbon footprint. In this paper, we propose a profitable energy sharing based cooperative framework to reduce the grid energy consumption and to facilitate better utilization of network energy. Traffic-energy imbalances in a dual-powered network often result in some BSs being energy deficient due to spatio-temporal variation of traffic. In such an event, it is proposed that the energy deficient BS, through the proposed energy cooperation (EC) framework, interact with the other networked BSs and requests them to share the deficient energy or a portion of it at a much lower price than the grid price via grid itself. We compare the proposed EC model with a without energy cooperation (WEC) model, where the BSs do not interact with one another and release energy above their storage capacity only to the power-grid for selling. Our results demonstrate that for a two BS network, the EC model provides a significant reduction in grid consumption up to 50% with a 38% gain in operator's revenue at 80% traffic skewness.
\subsubsection{User Scheduling in Federated Learning over Energy Harvesting Wireless Networks}
abstract:In this paper, the deployment of federated learning (FL) is investigated in an energy harvesting wireless network in which the base station (BS) is equipped with a massive multiple-input multiple-output (MIMO) system and a set of users powered by independent energy harvesting sources to cooperatively perform an FL algorithm. Since a certain number of users may not be served due to interference and energy constraints, a joint energy management and user scheduling problem is considered. This problem is formulated as an optimization problem whose goal is to minimize the FL training loss via optimizing user scheduling. To determine the effect of various wireless factors (transmit power and number of scheduled users) on training loss, the convergence rate of the FL algorithm is analyzed. Given this analytical result, the original user scheduling and energy management optimization problem can be decomposed, simplified and solved. Simulation results show that the proposed algorithm can reduce training loss compared to a standard FL algorithm.
\subsubsection{Energy Efficiency of Multi-Carrier Massive MIMO Networks: Massive MIMO Meets Carrier Aggregation}
abstract:The energy consumption of cellular networks, despitethe high energy efficiency of the fifth generation (5G) ofmobile technology, is still a challenge. The fundamental problemarises due to the complexity of optimising the operation ofthe available rich set of energy efficiency features in largescaledeployments. To assist such optimisation, a large body ofresearch -with the resulting understanding and algorithms-exists, particularly on the energy efficiency of single-cell massivemultiple-input multiple-output systems. However, other fundamentalcellular features, such as those relating to multi-carriersystems, remain largely unexplored. In this paper, we show howmulti-carrier features, such as carrier aggregation, can play asignificant role in energy savings, and question the need forhundreds of antennas and transceivers at the base stations asan urgent solution to increase the energy efficiency of nextgeneration networks.
\subsubsection{Power Usage of Energy Harvesting Sensors with a Drone Sink: A Reinforcement Learning Based Approach}
abstract:Wireless sensor networks (WSNs) can utilize radio frequency energy harvesting from ambient power sources for continuous operation, while drones acting as sink nodes increase the reliability of transmission and decrease a sensor's energy usage. In this paper, we attempt to optimize the transmit power of a sensor such that the overall outage probability is minimized. The sensors are assumed to have a finite-level battery and a buffer with discrete states. Energy is harvested from ambient energy arising from cellular base stations distributed according to a Poisson point process. The sensor's data is transmitted to a drone whenever its buffer becomes full. We consider two scenarios for the drone: i) hovering, and ii) moving on a fixed trajectory. Moreover, we utilize different path loss and fading models for the sensor-drone links due to their line-of-sight nature. An outage occurs due to both transmission outage and buffer overflow. We formulate the problem as a Markov decision process, and utilize a Q-learning based algorithm to learn the power control policy. Our numerical results show that the proposed policy significantly outperforms both the full and single-step energy usage policies. Moreover, it is robust enough to handle environmental/channel changes without a need for re-training.
\subsubsection{Reinforcement Learning Based Energy-Efficient Component Carrier Activation-Deactivation in 5G}
abstract:Carrier aggregation (CA) is considered a key enabler technology for delivering higher rates to users of LTE and 5G networks. However, the increased transmission rate comes with the price of higher energy consumption which stems from users continuously monitoring the control channel of the active CCs whether data transmission is ongoing or not. In order to reduce energy consumption, we exploit the activation-deactivation procedure at the medium access control (MAC) layer of LTE/5G network. In this paper, we propose a reinforcement learning-based algorithm to improve energy-efficiency by dynamically activating-deactivating secondary component carriers (SCCs) with awareness of the user traffic profiles. The proposed algorithm aims to predict the arrival of data and identify SCCs to activate for each user. In addition, a traffic splitting approach and an intelligent exploration strategy are proposed to balance users' load among CCs and improve the convergence of the algorithm, respectively. Results of the proposed algorithm are compared with three baseline algorithms. The first baseline always activates all CCs for each user, the second baseline activates one carrier only (i.e., the primary carrier) and the third baseline algorithm relies on a reactive method, where the activation-deactivation decision is performed after observing the arrival of data. Results show that Q-learning outperforms the baseline algorithms by achieving the highest sum throughput (and lowest average delay) with the lowest number of activated SCCs, which is obtained by learning to dynamically activate SCCs according to the traffic pattern. Hence, Q-learning is considered the most energy-efficient compared to baseline algorithms.
\subsection{Green Communication Networks-2}
\subsubsection{Mobile Traffic Forecasting for Green 5G Networks}
abstract:The energy consumption and carbon footprint of the fifth-generation (5G) of mobile technology is a current concern to mobile network operators (MNOs). These are currently attempting to lower both their carbon emissions and electricity bills by investigating new schemes that allow adapting the network transmission capabilities to the end-users' quality of service (QoS) requirements. Many of such schemes rely on accurate traffic forecasting, and as a consequence, there is a large effort on investigating novel machine learning (ML) algorithms, which fed by network measurement data and empowered by the computing capabilities of dedicated hardware, can help modelling and predicting user behaviours. Most of the works in the literature, however, focus on predicting the traffic when energy saving features, e.g. carrier shutdown, are not implemented or activated. However, the prediction task becomes much more challenging when energy saving features are adopted due to their impact to the actual measured traffic. In this paper, we consider a scenario in which part of the base stations implement energy saving schemes, which allow them to dynamically switch off part of the hardware to reduce their power consumption. Then, we present a ML framework based on graph convolutional networks (GCN) for traffic forecasting in such dynamic scenarios, and compare its performance with other statistical and ML prediction algorithms. The proposed GCN framework provides significant accuracy gains. Moreover, we provide an analysis of the impact of spatial correlation -captured by the GCN model- on the achieved performance.
\subsubsection{Dynamic Ensemble Inference at the Edge}
abstract:We propose a dynamic resource allocation algorithm in the context future wireless networks endowed with edge computing, to enable accurate energy efficient classification with end-to-end delay guarantees. In our scenario, sensor devices upload data to an Edge Server (ES) for classification purposes. Merging Lyapunov stochastic optimization and ensemble inference, we propose DEsIreE, a low-complexity method that dynamically selects the data quantization level, the transmit power, and the ES's CPU scheduling, without any prior knowledge of the statistics of wireless channels and data arrivals. Numerical simulations run on two real datasets assess the effectiveness of our algorithm in optimizing sensors' energy consumption and classification accuracy, with the ensemble yielding considerable gain.
\subsubsection{Peak AoI-Aware Network Lifetime Maximization in Underwater Acoustic Sensor Networks}
abstract:Underwater acoustic sensor networks (UASNs) have been proposed to monitor underwater environment for various purposes, e.g., scientific, commercial, military, and so on. However, high dynamic of underwater acoustic links, long propagation delays, and limited bandwidth challenge the application of UASNs for underwater state monitor, particularly for the states that sensitive to information freshness. In addition, in battery-limited UASNs, the lifetime of underwater sensor nodes (USNs) would significantly affect the sustainability of underwater monitor and information delivery. To prolong the lifetime of UASNs constrained by information freshness, a distributed path selection problem is studied. In special, to reliably deliver the underwater state update information to the monitor center deployed on the surface, a link-level handshake and stop-and-wait mechanism is used. Considering the high dynamic and long propagation delay of the underwater environment, the path decision is made between directly transmitting from the source to the sink node, or relaying through a neighbor USN. We formulate the path selection problem in UASNs as a peak age of information (AoI)-aware network lifetime maximization problem. Then, a distributed and cooperative scheme, including the energy-consumption-variance minimization (ECVM) and partial particle swarm optimization (PPSO) algorithms, is proposed to solve the problem. Simulation studies have been conducted to illustrate the efficiency of the proposal.
\subsubsection{Age of Information Minimization in Energy Harvesting Sensors with Non-Ideal Batteries}
abstract:Many emerging Internet of Things (IoT) sensorsdeployed for sampling and delivering status update packets maybe powered by energy harvesting (EH) sources. In this work, weconsider a sensor-monitor pair, where the EH-powered sensordelivers timely updates to the monitor over a fading channel.We deem a status update to be successful if it contains at leastR 0 bits. The sensor is equipped with a finite-capacity non-idealbattery whose charging and discharging losses are proportionalto charging and discharging powers. The sensor consumes non-negligible circuit power for its operation. Under this setting, wequantify timeliness of information by the age of information (AoI)metric and consider an AoI minimization problem for optimallychoosing charging and discharging powers, transmission durationand transmit power at each communication slot. We cast thisproblem as a Markov decision process (MDP), in which the actiontuple can take a wide range of values, and solve it using the valueiteration algorithm. We reduce the computational complexity ofthe value iteration, by showing that the optimal action takesone of the two values, which result in either no transmission ordelivery of R 0 bits, and that it has a threshold structure in AoIwhen all other variables are fixed. Via numerical simulations,we illustrate the threshold structure of the optimal action andcompare the performance of the MDP-based policy with othersimpler heuristic policies.
\subsubsection{Statistical Delay and Error-Rate Bounded QoS for SWIPT in CF M-MIMO 6G Wireless Networks Using FBC}
abstract:Taking advantage of the broadcast nature of radiofrequency (RF) wave propagation, simultaneous wireless informationand power transfer (SWIPT) has recently gained significantresearch attention since it can prolong the battery-life of energyconstrainedand low-power-supported mobile devices. In addition,due to the potential benefits of favorable propagation and channelhardening, cell-free (CF) massive multi-input multi-output (m-MIMO) can significantly enhance the QoS performance of SWIPTin terms of the achievable data rate and energy efficiency. On theother hand, finite blocklength coding (FBC) has been proposedto support various massive access techniques while reducingthe access latency and guaranteeing stringent QoS requirements.However, how to efficiently integrate SWIPT with CF m-MIMOusing FBC based statistical delay-bounded QoS theory has imposedmany new challenges not encountered before. To overcome thesedifficulties, in this paper we propose and develop statisticaldelay and error-rate bounded QoS provisioning schemes overSWIPT-enabled CF m-MIMO 6G wireless networks in the finiteblocklength regime. In particular, we establish SWIPT-enabled CFm-MIMO based system models by using FBC. We also formulateand solve the optimization problems for the tradeoff between the -effective capacity and harvested energy for our proposed statisticaldelay and error-rate bounded QoS provisioning mechanisms. Theobtained simulation results validate and evaluate our developedschemes.
\subsection{Green IoT}
\subsubsection{Efficient Multiple Charging Base Stations Assignment for Far-Field Wireless-Charging in Green IoT}
abstract:Owing to the development of Internet of Things (IoT) and Artificial Intelligence (AI) technology, powering IoT devices has become a dire problem that mobile IoT devices need a more portable way to be charged. Based on our previous research on green IoT, the far-field Wireless Power Transfer (WPT) powered by green energy can alleviate this problem. Although many existing works on Multi-Base Station Joint Charging Schemes have gained remarkable results, the aggregation of multiple power waves cannot be explicitly described by the traditional 1-dimensional model suggested by Friis Formula. The 2-dimensional model called vector model can solve this problem by clearly indicating how the multiple power waves aggregate at an IoT device in the form of a 2-dimensional vector. In this work, an Adjusting Phase (AP) method based on the vector model is designed to enhance the value of aggregated power waves. In addition, we propose the Greedy chArging Grouping Algorithm (GAGA) to ensure that the charging mission will be completed on time and the risk of running out of power can be reduced. Finally, we validate the performance of the proposed algorithm in comparison with the state-of-the-art solutions through extensive simulations.
\subsubsection{Cloud-Edge Collaboration with Green Scheduling and Deep Learning for Industrial Internet of Things}
abstract:As a key technology of the sixth generation (6G), cloud-edge collaboration has attracted attention in the industrial Internet of Things (IIoT). However, the delay-sensitive and resource-intensive intelligent services in IIoT not only require a large number of computing resources to reduce the delay cost and energy consumption of devices but also require fast and accurate intelligent decisions to avoid service congestion. In this paper, we design an offloading scheme based on cloud-edge collaboration and edge collaboration, including four computing modes, which jointly consider the delay and energy optimization of devices. We propose a parallel deep learning-driven cooperative offloading (PDCO) algorithm, which weighs the real-time and accuracy of offloading scheme. To deal with the difficulty of obtaining labels, a low-complexity hybrid label processing method is designed to reduce the cost of labeling data, and then multiple parallel deep neural networks (DNNs) are trained to generate the best offloading decision timely. Simulation results show that the proposed algorithm can generate offloading decisions with more than 90% accuracy in 0.1s while considering green scheduling.
\subsubsection{Energy Efficient Data Recovery from Corrupted LoRa Frames}
abstract:High frame-corruption is widely observed in Long Range Wide Area Networks (LoRaWAN) due to the coexistence with other networks in ISM bands and an Aloha-like MAC layer. LoRa's Forward Error Correction (FEC) mechanism is often insufficient to retrieve corrupted data. In fact, real-life measurements show that at least one-fourth of received transmissions are corrupted. When more frames are dropped, LoRa nodes usually switch over to higher spreading factors (SF), thus increasing transmission times and increasing the required energy. This paper introduces ReDCoS, a novel coding technique at the application layer that improves recovery of corrupted LoRa frames, thus reducing the overall transmission time and energy invested by LoRa nodes by several-fold. ReDCoS utilizes lightweight coding techniques to pre-encode the transmitted data. Therefore, the inbuilt Cyclic Redundancy Check (CRC) that follows is computed based on an already encoded data. At the receiver, we use both the CRC and the coded data to recover data from a corrupted frame beyond the built-in Error Correcting Code (ECC). We compare the performance of ReDCoS to (i) the standard FEC of vanilla-LoRaWAN, and to (ii) RS coding applied as ECC to the data of LoRaWAN. The results indicated a 54x and 13.5x improvement of decoding ratio, respectively, when 20 data symbols were sent. Furthermore, we evaluated ReDCoS on-field using LoRa SX1261 transceivers showing that it outperformed RS-coding by factor of at least 2x (and up to 6x) in terms of the decoding ratio while consuming 38.5% less energy per correctly received transmission.
\subsubsection{Green NOMA M2M}
abstract:With the advent of next-generation networks, such as 5G, and their support for spectrum hungry applications, Industry has become more ambitious in finding new methods of exploiting existing spectrum. Methods to exploit existing spectrum include network densification and spectrum reuse, both of which attempt to increase spectral efficiency. Reuse of underutilized spectrum offers many opportunities for Service Providers (SPs) and a means of alleviating network congestion. In this paper we present Green Non-Orthogonal Multiple Access (NOMA) Machine-to-Machine (M2M), where underutilized NOMA power-domain resources are re-purposed for non-QoS (Non-Quality of Service) delay tolerant network flows in a non-performance impacting manner. Based on the set of users allocated NOMA resources, small cells determine the minimum Signal-to-Interference-Noise-Ratio (SINR) required and allocated the remaining power for green M2M (delay tolerant data flows). Using game theory we design an incentive compatible resource determination and allocation mechanism. In the mechanism, delay tolerant flows compete for resources via a non-QoS Variable Resource Block (VRB) auction. An inherent benefit of this model is that it enables information dissemination using existing standardized methods and does not adversely impact system performance. We show that our auction approximates the profit and allocative efficiency performance characteristics of the Vickrey Clarke Groves (VCG) auction, yet runs in P-time. In addition, we show that via NOMA and statistical SINR the underutilized spectrum can be estimated and exploited without sacrificing the SP's available spectrum, while also enabling the SP the ability to increase their revenue without impacting the QoS guarantees of primary cellular users.
\subsubsection{A Distributed Approach to Energy Efficiency in Seamless IoT Communications}
abstract:The rapid growth of Internet-of-things (IoT) devices modernizes our society and lives. Such a growth leads to the proliferation of the wireless technologies, most of which occupies the unlicensed/open industrial, scientific and medical (ISM) band. Since each of these wireless technologies adheres to different standards, they give rise to challenges like cross technology interference, incompatible security protocols and high energy consumption. In this work, we envision to achieve an energy-efficient communication on Seamless IoT platform. In specific, a game theoretical approach is proposed to formulate a multi-device IoT communication system. The existence and uniqueness of the Nash equilibrium is proven to achieve the optimal transmit power for the IoT devices. Moreover, a distributed scheme is developed for practical implementations depending on receiving signal strength indicators. Simulation results demonstrate that optimal transmit power can be achieved through the proposed scheme. We further implement and demonstrate the distributed approach on a USRP software-defined radio testbed.
\subsection{Green Mobile Edge Networks}
\subsubsection{MEC and Blockchain-Enabled Energy-Efficient Internet of Vehicles with A3C Approach}
abstract:Recently, the rise of the Internet of Vehicles (IoV) has led to the rapid development of intelligent transportation. In order to improve the computing capacity of mobile vehicles and reduce the content delivery latency of suppliers, mobile edge computing (MEC) is widely regarded as a promising paradigm. However, there are some essential issues to be considered: 1) security and privacy of data transmission, and 2) reasonable resource allocation for collaborative computing and caching. In this paper, to solve above issues, blockchain technology is adopted to ensure reliable transmission and interaction of data. Meanwhile, we develop an intelligent resources framework about computing and caching for blockchain-enabled MEC systems in the IoV. By jointly optimizing offloading decision of computation task carried by vehicle, caching decision, the number of offloaded consensus nodes, block interval and block size, the energy consumption and computation overheads of the system can be decreased, and the transaction throughput of the blockchain system can be improved. Moreover, we adopt a Markov decision process to formulate the joint optimization problem. Facing the dynamics and complexity of the system, the asynchronous advantage actor-critic (A3C) approach is introduced to solve the formulated problem. Simulation results show that the proposed scheme has better performance than other existing schemes.
\subsubsection{Dynamic Task Offloading in MEC-Enabled IoT Networks: A Hybrid DDPG-D3QN Approach}
abstract:Mobile edge computing (MEC) has recently emerged as an effective paradigm to support computation-intensive and delay critical applications at energy-constrained and computation-limited Internet of Things (IoT) devices. Due to the time-varying channels and dynamic task patterns, there exist many challenges to make efficient and effective computation offloading decisions, especially in the multi-server multi-user IoT networks, where the decisions involve both continuous and discrete actions. In this paper, we consider computation task offloading in a dynamic environment and formulate a task offloading problem to minimize the average long-term service cost in terms of power consumption and buffering delay. To enhance the estimation of the long-term cost, we propose a deep reinforcement learning algorithm, where deep deterministic policy gradient (DDPG) and dueling double deep Q networks (D3QN) are invoked to tackle continuous and discrete action domains, respectively. Simulation results verify that the proposed DDPG-D3QN algorithm has a better stability and faster convergence than the existing methods, and show that the proposed algorithm can also effectively decrease the average system service cost.
\subsubsection{Caching Assisted Correlated Task Offloading for IoT Devices in Mobile Edge Computing}
abstract:The rapid development of Internet of Things (IoT) has generated a vast number of tasks which need to be performed efficiently. Owing to the drawback of the sensor-to-cloud computing paradigm in IoT, mobile edge computing (MEC) has generated enormous attention recently. Against this backdrop, in this paper, we focus on the offloading of tasks characterized by intrinsic correlations, which have not been considered in most of existing works. For the sequential arrival of such correlated tasks, the future workload can be efficiently reduced by caching the current computational result. Specifically, we apply the Lyapunov optimization technology to tackle the long-term constraint on energy consumption. Simulation results reveal that our approach has a great advantage over other approaches in terms of response latency optimization and energy consumption reduction.
\subsubsection{FedGreen: Federated Learning with Fine-Grained Gradient Compression for Green Mobile Edge Computing}
abstract:Federated learning (FL) enables devices in mobile edge computing (MEC) to collaboratively train a shared model without uploading the local data. Gradient compression may be applied to FL to alleviate the communication overheads but current FL with gradient compression still faces great challenges. To deploy green MEC, we propose FedGreen, which enhances the original FL with fine-grained gradient compression to efficiently control the total energy consumption of the devices. Specifically, we introduce the relevant operations including device-side gradient reduction and server-side element-wise aggregation to facilitate the gradient compression in FL. According to a public dataset, we investigate the contributions of the compressed local gradients with respect to different compression ratios. After that, we formulate and tackle a learning accuracy-energy efficiency tradeoff problem where the optimal compression ratio and computing frequency are derived for each device. Experiments results demonstrate that given the 80% test accuracy requirement, compared with the baseline schemes, FedGreen reduces at least 32% of the total energy consumption of the devices.
\subsubsection{Distributed Resource Management for Licensed and Unlicensed Integrated Mobile Edge Computing}
abstract:This paper addresses a joint radio and computational resources allocation problem for mobile edge computing (MEC) networks. To alleviate the shortage of licensed spectrum resources, computing tasks can be offloaded to the MEC server through not only the licensed channels but also the unlicensed channels, where the adaptive duty-cycle-muting (DCM) mechanism is employed at the user terminals (UTs) to guarantee the fair coexistence with the WiFi networks. Moreover, Stackelberg game formulation is used to build up a decentralized radio and computational resources allocation framework, where the MEC server is modeled as a leader to set the price of the licensed spectrum, while UTs as followers compete for the radio and computational resources as a non-cooperative game. The objective of each UT is to minimize the long-term energy consumption with the guarantee of task buffer stability. Accordingly, we develop a distributed algorithm to achieve the equilibrium solution for the formulated Stackelberg game. Numerical results are presented to demonstrate that the proposed scheme is effective with respect to the reduction on energy consumption of UTs with limited signaling overheads.
\subsubsection{Contrastive Learning based Intelligent Skin Lesion Diagnosis in Edge Computing Networks}
abstract:In recent years, automatic skin lesion diagnosis methods based on artificial intelligence (AI) have achieved great success. However, the lack of rich available training data and visual similarity of various skin diseases remain the major challenges in intelligent skin lesion diagnosis. In this paper, we propose a contrastive learning based intelligent skin lesion diagnosis (CL-ISLD) scheme in edge computing networks. Specifically, an edge computing based intelligent skin lesion diagnosis network is constructed, which can provide the convenient and quick online diagnosis service to users nearby. Meanwhile, a contrastive learning based dual encoder network is designed to overcome training sample scarcity by fully leveraging unlabeled samples for performance promotion. Subsequently, we devise a maximum mean discrepancy (MMD) based supervised contrastive loss function, it can efficiently explore complex intra-class and inter-class variances of samples. Finally, the simulation results demonstrate that the proposed CL-ISLD obtains competitive diagnosis accuracy compared with existing representative works and achieves relatively more balanced performance among classes in inadequate and imbalanced dataset.
\subsection{Green RIS and Computing Systems}
\subsubsection{Secure Performance Analysis of RIS-aided Wireless Communication Systems}
abstract:Since Internet of Things (IoT) is suggested as the fundamental platform to adapt massive connections and secure transmission, we study physical-layer authentication in the point-to-point wireless systems relying on reconfigurable intelligent surfaces (RIS) technique. Due to lack of direct link from IoT devices (both legal and illegal devices) to the access point, we benefit from RIS by considering two main secure performance metrics. As main goal, we examine the secrecy performance of a RIS-aided wireless communication systems which show secure performance in the presence of an eavesdropping IoT devices. In this circumstance, RIS is placed between the access point and the legitimate devices and is designed to enhance the link security. To specify secure system performance metrics, we firstly present analytical results for the secrecy outage probability. Then, secrecy rate is further examined. We find that the number of metasurface elements of the RIS and the average signal-to-noise ratio at the source lead to main impacts on system performance enhancement. We verify derived expressions by matching Monte-Carlo simulation and analytical results.
\subsubsection{Reconfigurable Intelligent Surface Based Uplink Massive MIMO Symbiotic Radio System}
abstract:In this paper, we investigate a reconfigurable intelligent surface (RIS)-based uplink massive multi-input multi-output symbiotic radio system, where each RIS, as an IoT device, enhances the primary transmission from a nearby user to the base station (BS) and simultaneously transmits its own information to the BS by backscattering modulation. By embedding environmental sensors on the RISs, the proposed system enables the IoT transmission of locally collected environmental data to the BS while assisting the primary transmission. Assuming imperfect channel state information (CSI), we jointly design the active beamforming at the BS and the passive beamforming at the RISs to maximize the weighted sum-rate of both the primary and IoT transmissions. An algorithm based on the block coordinate descent method is proposed to solve it. Simulation results show that the proposed system achieves significant performance gain compared with different baseline schemes. Besides, when the channel estimation error is small, the performance loss due to imperfect CSI is insignificant.
\subsubsection{Reconfigurable Intelligent Surface for Small Cell Network}
abstract:Small cell network (SCN) is a promising solution to meet the demand for increasing data traffic for the sixth generation and beyond wireless networks. However, the power consumption and two-tier interference issues are two bottlenecks that hinder its further development. In this paper, we propose a novel intelligent reflecting communication (IRC) system in which a reconfigurable intelligent surface (RIS) is used to serve multiple micro users in an SCN while assisting the transmission from a macro base station (MBS) to a macro user. Compared to the conventional SCN, the RIS can achieve significant power reduction as it transmits the information by passively reflecting the incident signals. In addition, the reflected signal can be regarded as a multipath component instead of an interference to the macro user. We are interested in minimizing the total power consumption by jointly designing the phase shift matrix at the RIS and the beamforming vector at the MBS under the user rate constraints and the practical phase shift constraints. The solution is obtained by alternating optimization to iteratively solve two subproblems, one to optimize the phase shift matrix, and the other to optimize the beamforming vector. A TDMA transmission scheme is also proposed as an alternative to serve multiple users. Simulation results demonstrate that the total power consumption can be reduced significantly by deploying the RIS in the SCN when the number of reflecting elements is sufficiently large.
\subsubsection{Joint Optimization for Data-Offloading/Resource-Allocation in RF-Powered Backscatter Mobile Networks}
abstract:We propose to develop the joint optimizationschemes for traffic-offloading and resource-allocation over radiofrequency(RF) powered backscatter-based mobile wireless networks,where a macro-cell base station (BS), several smallcellaccess points (SAPs), and multiple energy harvesting (EH)mobile users (MUs) coexist. By optimizing MUs' network access(through traffic-offloading) and system resource-allocation, weaim to minimize the energy consumption of MUs by usingthe low-energy consumption of the backscatter communications.First, we propose a distributed traffic-offloading and resourceallocationscheme, when the short-range ambient backscatter(AB) communication (generally with communication range beingseveral meters) is employed to assist MUs' data transmission.Based on the proposed scheme, MUs choosing to access nearbySAPs can backscatter ambient RF signals for data transmissionto reduce energy consumption. Then, we employ the long-rangebistatic backscatter (BB) communication (with communicationup to 270 meters) to support data transmission between MUsand the BS, and thus MUs can convey data by backscattering RFsignals emitted from dedicated carrier emitters. Accordingly, wepropose a joint traffic-offloading and resource-allocation schemefor the scenarios when the AB and BB communications areadopted by MUs accessing the SAPs and the BS, respectively.Finally, we validate and evaluate the performance of our developedschemes through numerical analyses.
\subsubsection{Leveraging LEO assisted Cloud-Edge Collaboration for Energy Efficient Computation Offloading}
abstract:Mobile edge computing (MEC) has been widely considered as an effective technology to handle computationally intensive tasks generated by mobile devices. However, the computation resources at an edge node is usually several orders of magnitude smaller than that of a cloud. Thus, it is rather vital to take an investigation into the collaboration between the cloud and the edge. In this paper, to fully exploit the computation power of the cloud server and achieve energy efficient task offloading, we propose an LEO-assisted terrestrial-satellite network (TSN) architecture for cloud-edge collaborative computation offloading. We formulate the collaborative cloud-edge computing problem that minimizes the energy consumption of the whole TSN under the quality-of-service (QoS) constraints. The optimization problem is further decomposed into two subproblems which are solved by deep neural networks (DNN) and successive convex approximation (SCA) algorithm, respectively. Simulation results show the effectiveness of our proposed cloud-edge collaborative computation offloading architecture on achieving a lower energy cost.
\subsection{Green Communication Systems}
\subsubsection{Approaches for Improving Efficiency: Adaptive Predistortion of Dual-Level Voltage Power Amplifiers}
abstract:A stochastic approximation technique is presented as a low-complexity approach for improving efficiency by linearizing the power amplifier (PA) with dual-level supply voltage. The coefficients of the RF predistorter are updated by using the simultaneous disturbance random approximation (SPSA) algorithm to minimize the adjacent channel power ratio (ACPR) under the transmit power constraints. The PA with a low supply voltage of 2 V can improve the efficiency by reducing DC power consumption in the lower output power range compared with the PA with a fixed supply voltage of 4 V. The efficiency of the PA can be further improved by using a combination of RF predistortion technique and dual-level supply voltage PA when operating close to a saturated region without significantly causing nonlinear distortion. The simulation results show that the RF predistorter using the SPSA algorithm has a faster convergence speed in the PA nonlinear compensation and a lower complexity calculation in the predistorter coefficient adaptation.
\subsubsection{A Coverage Area-Based Cooperation Technique for SWIPT-Enabled Systems with Mobility}
abstract:We investigate the performance of the simultaneous wireless information and power transfer (SWIPT)-enabled mobile networks, based on a novel coverage area-based coordinated multipoint transmission (CA-CoMP) technique. In particular, the proposed scheme exploits the cooperation technique by appropriately defining the set of cooperative base stations, aiming at reducing the handover rate of the SWIPT-enabled battery-operated mobile users (MUs). Using stochastic geometry tools, we first establish an analytical and tractable framework to investigate the achieved information decoding and energy harvesting performance of the proposed scheme, in terms of the coverage probability, the average spectral efficiency and the average harvested energy. Moreover, analytical expressions for the inter- and intra-cell handover rates are derived. Our results indicate the ability of our proposed scheme to significantly reduce the handover rate of MUs. Finally, simulation results verify that the CA-CoMP scheme achieves a higher average spectral efficiency and a higher average harvested energy compared with the conventional cooperative technique.
\subsubsection{Unified Wireless Power and Information Transfer Using a Diplexed Rectifier}
abstract:Despite the recent interest in Wireless Information and Power Transmission (WIPT) - in which information and energy flow together through the wireless medium - there remains key challenges for practical implementations. So far, it is assumed that it is not possible to perform Wireless Power Transfer (WPT) and Wireless Information Transmission (WIT) operations on the same received signal using one antenna. In this paper, we propose a receiver circuit implementation that extract both RF power and data from the same signal. The key to our implementation is the use of RF diplexers that implements frequency-domain multiplexing. The proposed sustainable unified receiver use low-power components and can hence be entirely powered by RF wave. We also establish a theoretical performance foundation for the proposed receiver by deriving achievable rate to serve as guideline for the optimization of energy-rate bi-objective function.The simulation results prove that the proposed sustainable unified architecture always out performs the existing low-complexity integrated receiver architecture.
\subsubsection{Performance Optimization in Heterogeneous WiFi and Cellular Mobile Edge Computing Systems}
abstract:Mobile edge computing (MEC) is a promising paradigm for alleviating the computation burden of resource-constrained mobile devices. Nevertheless, the majority of existing efforts concentrate on offloading computations from mobile devices to an edge computing server through the cellular networks only. With the development of wireless connectivity technologies, WiFi networks over unlicensed spectrum provide a "green" (i.e., cost-efficient and economical) alternative for computation offloading. In this paper, we investigate the problem of computation offloading in a heterogeneous WiFi and cellular MEC system, where both the WiFi and the cellular networks are possible for offloading the arriving computation tasks at a mobile user (MU). The objective of an MU is to minimize the long-term cost, which can be described as a single-agent Markov decision process (MDP) by accounting for the inherent system dynamics in the MU mobility, sporadic computation task arrivals and wireless connectivity variations. To solve the optimal strategy for the formulated MDP with a high-dimensional state space but without the statistical knowledge of system dynamics, we resort to a model-free deep reinforcement learning algorithm. Numerical experiments verify that the proposed algorithm is able to significantly reduce the average computation offloading cost compared with other baselines.
\subsubsection{Traffic-Aware Compute Resource Tuning for Energy Efficient Cloud RANs}
abstract:Cloud Radio Access Network (C-RAN) disaggregates the base stations in a way that some of the radio processing tasks are centralized and virtualized on a pool of general-purpose processors on a cloud platform so that its computational resources could be utilized efficiently based on spatio-temporal traffic fluctuations at cell sites. To further reduce the consumption of computation resources by C-RAN on the cloud platform, in this paper, we first profile the energy consumed in an OAI based C-RAN system and report variation in the energy consumed when using the existing Linux CPU frequency scaling governors. We then identify optimal CPU clock frequencies for different Modulation and Coding Schemes (MCS) to reduce energy consumption in the cloud platform. Based on the observations, we propose a traffic-aware compute resource tuning (CRT) scheme to reduce the energy consumption of C-RANs. The CRT scheme opportunistically lowers MCS used while serving users by utilizing all of the available radio resources in every scheduling interval during non-peak hours. This reduction in MCS helps in reducing energy consumption (due to usage of lower CPU clock frequency) and fronthaul bandwidth requirements. Another benefit of the CRT scheme is its ability to work with any MAC scheduler. The extensive simulation results show how CRT outperforms the existing frequency scaling governors in energy consumption while reducing fronthaul bandwidth requirements.
\subsubsection{Low-Cost mmWave Transmission: Wireless Beam Modulation based Phase-less Interference Cancellation}
abstract:The millimeter wave (mmWave) communication is envisioned as a key essence of the 6G network to supply high data rate transmission. Nevertheless, both the power consumption and the hardware cost of the mmWave communication system are far beyond the capability of Internet of Things (IoT) devices. The emerging of over the air modulation (OTAM) based mmWave communication technology, i.e., wireless beammodulation (WBM), provides a new perspective to support the low-cost mmWave transmission for IoT devices. However, since the OTAM-based transmission is energy-based detection without phase information, the interference cancellation will be a major challenge for the phase-less receiver. In this paper, we propose three phase-less interference cancellation algorithms named Time Division Multiplexing based Interference Cancellation (TDM-IC), Code Division Multiplexing based Interference Cancellation (CDM-IC), and Code Matching Interference Cancellation(CM-IC) according to the multiplexing methods of the pilot. The three methods perform different characteristics in terms of complexity, robustness, supporting different accessing methods. By employing the proposed interference cancellation methods, the error floor of WBM BER performance is eliminated, which paves the way for the massive connectivity of OTAM based systems.
\section{IOT and Sensor Networks}
\subsection{Technical Papers}
\subsubsection{WIAGE: A Gait-based Age Estimation System Using Wireless Signals}
abstract:With recent advances in the study of biometrics, gait analysis has drawn much attention for its potential use in forensics, surveillance, and legal systems. In this paper, we present WIAGE, a contactless and non-intrusive gait-based age estimation system, which leverages wireless sensing to perform gait analysis to infer the age of individuals. Traditional age estimation systems either require users to carry wearable devices that are inconvenient or rely on image processing that is computationally intensive and sensitive to lighting conditions and occlusion. In contrast, WIAGE utilizes the incumbent WiFi infrastructure to infer the age of users with minimal interference to their activities. We adopt a series of signal processing techniques to recover clear gait patterns from the noisy WiFi signals and extract the most relevant features from steps that can be used for robust age estimation. The experimental results show that WIAGE can achieve an age estimation accuracy of 95.2% for 23 users, which demonstrates the feasibility and effectiveness of our proposed system.
\subsubsection{Passive Unsupervised Localization and Tracking using a Multi-Static UWB Radar Network}
abstract:The indoor localisation and tracking of objects and humans with high accuracy is becoming increasingly important in a number of applications including healthcare, ambient assistant-living, surveillance, among others. Since Ultrawideband (UWB) systems have a large operating bandwidth, they can provide centimetre (cm) level localisation accuracy in Line-of-Sight (LoS) scenarios. However, current commercial UWB systems require the target to carry an active device (tag) so that it can be precisely located and this may be impractical in applications such as security and surveillance. In this work, we process experimental data obtained from a multi-static UWB radar network for the passive indoor localisation of a moving target. Current fingerprinting-based passive localisation techniques require a substantial radio-map survey in the offline training phase and labour-intensive fingerprint updates when there are changes in the environment. In this work, on the other hand, we propose to use the fine-grained physical layer information, known as Channel Impulse response (CIR), that is exchanged between UWB modules and capitalise on the fact that a moving person induces variations in the CIR that stand out against the background signal. Our results show that a walking target can be passively located with a median distance error as low as 0.55m in an indoor environment.
\subsubsection{Channel State Information Compression based on Projection Transformation and Curve Fitting}
abstract:In the Internet of Things (IoT), with the development of channel state information (CSI) based wireless sensing technologies, such as intrusion detection, activity recognition, and indoor localization, a tremendous amount of CSI data need to be transmitted simultaneously, resulting in unacceptable time delay and requirement for additional bandwidth. Therefore, this paper proposes a CSI compression algorithm based on projection transformation and curve fitting (PCFIT), which realizes high-compression-ratio transmission and high-accuracy reconstruction. Concretely, projection matrixes are firstly constructed to perform projection transformation on the original CSI.Next, an adaptive weighted average fitting order judgment algorithm is designed to get the fitting order. And then, the Levenberg-Marquardt (LM) algorithm is used to perform curve fitting on the CSI based on the linear combination of a few sine waves to obtain the fitting parameters. Finally, CSI is reconstructed with these fitting parameters.By analyzing compression indicators such as residual, compression ratio, and parameter estimation accuracy, experimental results show thatthe PCFIT compression algorithm achieves a better compression ratio than other existing compression algorithms under the same residual and parameter estimation accuracy.
\subsubsection{QoS-aware Routing Optimization Algorithm using Differential Search in SDN-based MANETs}
abstract:In Mobile Ad hoc Networks (MANETs), the mobility of nodes causes frequent changes in the network topology, which directly affects user's Quality of Service (QoS) performance. Therefore, it is crucial for network operator to implement efficient routing optimization (RO) algorithms for diverse traffic flows. Based on the characteristics of centralized control routing in Software-Defined Networking (SDN), in this paper, we propose a routing optimization problem, and formulate the problem as a integer linear programming (ILP) problem. Then to solve the problem efficiently, we propose the QoS-aware routing optimization algorithm (QoS_ROA), which solves the problem in two stages. In the first stage, we use the Wavelet neural network (WNN) to predict the link quality at the next moment. In the second stage, we transform the proposed routing optimization problem into a 0-1 knapsack problem, and use differential search (DS) to solve it. The simulation results verify that, compared with the traditional routing algorithms, our algorithm can achieve high throughput, low packet drop rate, low delay in SDN-based MANETs.
\subsubsection{Wake-up Control with Kernel Density Estimation for Top-k Query in Wireless Sensor Networks}
abstract:This paper aims to improve the efficiency of wake-up control for top-k query in wireless sensor networks (WSNs) by introducing a function to estimate the data distribution of sensors. In order to minimize the wasteful wake-up of nodes outside the top-k set, the sink gradually enlarges the search region of data from the highest value by employing countdown content-based wake-up (CDCoWu). In this case, the step size to enlarge the search region plays an important role: a larger step causes many nodes to simultaneously wake-up, which leads to severe congestion, while a smaller step leads to the failed wake-up trial with no replies. In this paper, in order for the sink to appropriately select the step size, we introduce kernel density estimation into wake-up control, by which the sink first estimates the probability density function (PDF) of data owned by sensor nodes. The efficiency of CDCoWu can be improved with more accurate estimation on PDF, which, however, requires larger estimation cost. In this paper, we analyze the trade off between the accuracy of estimation and efficiency of CDCoWu, and investigate whether the overall efficiency in terms of energy consumption and delay can be improved by the proposed wake-up control with density estimation. Our numerical results show that the proposed wake-up control achieves better energy efficiency and data collection delay than the conventional CDCoWu employing a fixed step size.
\subsubsection{An Efficient Multi-Model Training Algorithm for Federated Learning}
abstract:How to effectively organize various heterogeneous clients for effective model training has been a critical issue in federated learning. Existing algorithms in this aspect are all for single model training and are not suitable for parallelmulti-model training due to the inefficient utilization of resources at the powerful clients. In this paper, we study the issue of multi-model training in federated learning. The objective is to effectively utilize the heterogeneous resources at clients for parallel multi-model training and therefore maximize the overall training efficiency while ensuring a certain fairness among individual models. For this purpose, we introduce a logarithmic function to characterize the relationship between the model training accuracy and the number of clients involved in the training based on measurement results. We accordingly formulate the multi-model training as an optimization problem to find an assignment to maximize the overall training efficiency while ensuring a log fairness among individual models. We design a Logarithmic Fairness-based Multi-model Balancing algorithm (LFMB), which iteratively replaces the already assigned models with a not-assigned model at each client for improving thetraining efficiency until no such improvement can be found. Numerical results demonstrate the significantly high performance of LFMB in terms of overall training efficiency and fairness.
\subsubsection{Communication-Efficient Heterogeneous Federated Dropout in Cross-device Settings}
abstract:Federated Dropout has emerged as an elegant solution to conjugate communication-efficiency and computation-reduction on Federated Learning (FL) clients. We claim that Federated Dropout can also efficiently cope with device heterogeneity by exploiting a server that broadcasts custom and differently-sized sub-models, selected from a discrete set of possible sub-models, to match the computation capability constraints of FL clients. In addition, we further reduce the up-link communication cost by applying per-layer or traditional Sparse Ternary Compression (STC) to sub-model updates. We demonstrate the effectiveness of our solution by reporting results for a well-known CNN used for classification tasks considering the Federated EMNIST dataset.
\subsubsection{Deep Reinforcement Learning for URLLC in 5G Mission-Critical Cloud Robotic Application}
abstract:In this paper, we investigate the problem of robot swarm control in 5G mission-critical robotic applications, i.e., in an automated grid-based warehouse scenario. Such application requires both the kinematic energy consumption of the robots and the ultra-reliable and low latency communication (URLLC) between the central controller and the robot swarm to be jointly optimized in real-time. The problem is formulated as a nonconvex optimization problem since the achievable rate and decoding error probability with short blocklength are neither convex nor concave in bandwidth and transmit power. We propose a deep reinforcement learning (DRL) based approach that employs the deep deterministic policy gradient (DDPG) method and convolutional neural network (CNN) to achieve a stationary optimal control policy that consists of a number of continuous and discrete actions. Numerical results show that our proposed multi-agent DDPG algorithm achieves a performance close to the optimal baseline and outperforms the single-agent DDPG in terms of decoding error probability and energy efficiency.
\subsubsection{Packet-Loss-Tolerant Split Inference for Delay-Sensitive Deep Learning in Lossy Wireless Networks}
abstract:The distributed inference framework is an emerging technology for real-time applications empowered by cutting-edge deep machine learning (ML) on resource-constrained Internet of things (IoT) devices. In distributed inference, computational tasks are offloaded from the IoT device to other devices or the edge server via lossy IoT networks. However, narrow-band and lossy IoT networks cause non-negligible packet losses and retransmissions, resulting in non-negligible communication latency. This study solves the problem of the incremental retransmission latency caused by packet loss in a lossy IoT network. We propose a split inference with no retransmissions (SI-NR) method that achieves high accuracy without any retransmissions, even when packet loss occurs. In SI-NR, the key idea is to train the ML model by emulating the packet loss by a dropout method, which randomly drops the output of hidden units in a neural network layer. This enables the SI-NR system to obtain robustness against packet losses. Our ML experimental evaluation reveals that SI-NR obtains accurate predictions without packet retransmission at a packet loss rate of 60%.
\subsubsection{Long-short Term Prediction for Occlusion Multiple Object Tracking}
abstract:Online multiple object tracking (MOT) is a challenging problem in complex scenes due to frequent occlusions. Most of the existing MOT methods tend to focus on addressing an individual type of occlusion, which cannot meet the requirements of real complex scenes. In this paper, we propose a unified MOT framework that combines long- and short-term prediction models for online multiple object tracking. Basically, The short-term prediction model consists of an appearance-based model and a motion-based model, aiming at exploiting the appearance and motion of objects to handle different types of occlusions jointly. Furthermore, we adopt a cubic spline interpolation as a longterm prediction model to estimate the trajectory of the target in occluded frames. To handle different lengths of occlusions, an adaptive weighted fusion model is proposed to combine the short-term prediction model, and the long-term prediction model. Experimental results on several challenging datasets demonstrate that the proposed method outperforms state-of-the-art methods.
\subsubsection{Analyse or Transmit: Utilising Correlation at the Edge with Deep Reinforcement Learning}
abstract:Millions of sensors, cameras, meters, and other edge devices are deployed in networks to collect and analyse data. In many cases, such devices are powered only by Energy Harvesting (EH) and have limited energy available to analyse acquired data. When edge infrastructure is available, a device has a choice: to perform analysis locally or offload the task to other resource-rich devices such as cloudlet servers. However, such a choice carries a price in terms of consumed energy and accuracy. On the one hand, transmitting raw data can result in higher energy cost in comparison to the required energy to process data locally. On the other hand, performing data analytics on servers can improve the task's accuracy. Additionally, due to the correlation between information sent by multiple devices, accuracy might not be affected if some edge devices decide to neither process nor send data and preserve energy instead. For such a scenario, we propose a Deep Reinforcement Learning(DRL) based solution capable of learning and adapting the policy to the time-varying energy arrival due to EH patterns. We leverage two datasets, one to model energy an EH device can collect and the other to model the correlation between cameras. Furthermore, we compare the proposed solution performance to three baseline policies. Our results show that we can increase accuracy by 15% in comparison to conventional approaches while preventing outages.
\subsubsection{Evolutionary Multitasking for Cross-domain Task Optimization via Vehicular Edge Computing}
abstract:Efficient optimization is a key enabler for emerging intelligent applications in Internet of Vehicles (IoV). However, existing studies in IoV only focused on solving a single domain-specific optimization problem at a time, which renders their efficiency on tackling various cross-domain optimization tasks in IoV. In this paper, we make the first effort on investigating a novel optimization framework in IoV for cross-domain tasks via vehicular edge computing. Specifically, two typical cross-domain tasks in IoV are presented, namely, the data dissemination (DD) task and the computing offloading (CO) task. Then, a cross-domain problem called DD-CO is formulated to facilitate the sharing of task features and knowledge during the solution searching. On this basis, we propose an evolutionary multitasking approach named EMA, which consists of an integer based unified representation scheme for encoding both the DD and CO tasks in a single solution, a corresponding decoding operator for task-specific solution evaluation, and a new population evolution mechanism for better adapt to the cross-domain problem optimization. Finally, we build the simulation model and give a comprehensive performance evaluation, which have demonstrated the advancement of the new optimization framework via vehicular edge computing and the effectiveness of the proposed EMA method.
\subsubsection{A Comparative Study of Deep-Learning-Based Semi-Supervised Device-Free Indoor Localization}
abstract:Real-time device-free indoor localization is a key technology for many Internet of Things (IoT) applications. Fingerprinting-based localization schemes rely on the database constructed from the offline site survey. Constructing a fully labeled database is expensive, and therefore a fingerprinting-based method requiring only a small amount of labeled data and a large amount of unlabeled data (i.e., semi-supervised) is highly useful. In this paper, we consider semi-supervised fingerprinting techniques based on the classic, generative model-based variational auto-encoder (VAE) and generative adversarial network (GAN). We conduct a comparative study of VAE and GAN in three real-world environments. Experimental results reveal that GAN generally outperforms VAE with various amounts of labeled data. Insights into how different generative mechanisms of these schemes, as well as environmental effects, affect the performance are provided.
\subsubsection{Age-of-Task Aware Sampling Rate Optimization in Edge-Assisted Industrial Network Systems}
abstract:Multivariate-information and computation-intensive tasks play an important role in Wireless Sensor Network Systems (WSNSs), where information freshness has an important impact on system performance of state analysis. Recently, the Age of Information (AoI) has been studied extensively as a promising metric to evaluate the freshness of state packets. However, most of the existing research focuses on optimizing the average AoI of a single information source, which can not be directly applied to the scenario with multi-source tasks. In this paper, we firstly present a novel definition of the Age of Task (AoT) for edge-assisted industrial WSNSs. Furthermore, the expressions of the sensing time and arrival time that determine the AoT tail distribution are given in detail. Then, we propose an AoT tail violation probability minimization problem to find the optimal sampling rate and give the feasible region of the sampling rate. Since it is difficult to obtain the exacted expression of the formulated problem, the Upper Bound Minimization Problem (UBMP) and the more tractable -relaxed UBMP are proposed to obtain the near-optimal sampling rate. Finally, simulation results show that the sampling rate obtained by -relaxed UBMP is nearly optimal for the AoT tail violation probability minimization problem.
\subsubsection{Mean-Field Game and Reinforcement Learning MEC Resource Provisioning for SFC}
abstract:In this paper, we address the resource provisioning problem for service function chaining (SFC) in terms of the placement and chaining of virtual network functions (VNFs) within a multi-access edge computing (MEC) infrastructure to reduce service delay. We consider the VNFs as the main entities of the system and propose a mean field game (MFG) framework to model their behavior for their placement and chaining. Then, to achieve the optimal resource provisioning policy without considering the system control parameters, we reduce the proposed MFG to a Markov decision process (MDP). In this way, we leverage reinforcement learning with an actor-critic approach for MEC nodes to learn complex placement and chaining policies. Simulation results show that our proposed approach outperforms benchmark state-o-the-art approaches.
\subsubsection{Robust Distributed Intrusion Detection System for Edge of Things}
abstract:The edge computing paradigm has been adopted in many Internet-of-Things (IoT) applications to improve responsiveness and conserve communication resources. However, such high agility and efficiency come with increased cyber threats. Intrusion detection systems (IDS) have been the primary means for guarding networked computing assets against hacking attempts. The popular design methodology for IDS relies on the application of machine learning (ML) techniques that use intelligence data to classify malicious activities. However, in the realm of IoT, insufficient data is available to build IDS; hence a distributed intrusion system with continual data collection is primordial to refine the detection model. Such IDS is also subject to privacy constraints and should sustain robustness against data manipulation from internal attackers that degrade the ML model. This paper opts to fulfill these requirements by proposing a novel distributed IDS for IoT. The proposed system employs federated learning to enable privacy preservation and diminish the communication overhead. Our system promotes a reinforcement mechanism to ensure resiliency to data manipulation attacks by single or colluding internal actors. The validation results using recently released datasets demonstrate the effectiveness of our approach.
\subsubsection{A Novel Deep Reinforcement Learning-based Approach for Task-offloading in Vehicular Networks}
abstract:Next-generation vehicular networks will impose unprecedented computation demand due to the wide adoption of compute-intensive services with stringent latency requirements. Vehicular edge or fog computing has been adopted to enhance the computational capacity of vehicular networks; however, the growing popularity and massive adoption of novel services make the edge resources insufficient. To address this challenge, the onboard computation resources of neighboring vehicles that are not resource-constrained can be utilized to enhance the computational capacity of the vehicular network along with the edge computing resources. To fill the gaps, in this paper, we propose to solve the problem of task offloading by jointly considering the communication and computation resources in a mobile vehicular network. We formulate a non-linear problem to minimize the energy consumption subject to the network resources. Furthermore, we consider a practical vehicular environment by taking into account the dynamics of mobile vehicular networks. The formulated problem is solved via a deep reinforcement learning (DRL) based approach. Finally, we perform numerical evaluations to demonstrate the effectiveness of our proposed scheme.
\subsubsection{Adaptive Edge Caching in UAV-assisted 5G Network}
abstract:Unmanned aerial vehicles (UAVs) with communication, computing, and storage capabilities have high mobility. Based on this advantage, it can push the service closer to the user. Our research group is concerned with implementing the Internet of Things (IoT) enabled massive crowd management platform that employs 5G to facilitate network connectivity among the UAV and sensory networks. In such a highly dynamic environment, IoT devices, users, and UAVs are the key factors to determine the caching strategies. Due to the limitations of drone batteries and changes in UAV cluster density, the environment is characterized as highly dynamic. However, the existing UAV caching strategy does not consider both the changes of the users and UAVs. Therefore, this paper proposes a three-layer UAV cache architecture in 5G network to achieve hierarchical adaptation to the dynamic changes of users and UAVs. Based on this architecture, we propose a dual dynamic adaptive caching(DDAC) algorithm. The DDAC algorithm is divided into two parts: user adaptation and UAV adaptation. For user adaptation, we designed a user-adaptive UAV trajectory model, which ensures the transmission efficiency of the UAV. For UAV adaptation, we designed and deployed a UAV-adaptive cache model based on a greedy algorithm in the cognitive center layer. The UAV can dynamically adjust the caching strategy according to the cluster density. Finally, the results of the experiment prove that our proposed UAV adaptive cache model has better performance in the cache hit ratio compared with the existing UAV cache model.
\subsubsection{TTDeep: Time-Triggered Scheduling for Real-Time Ethernet via Deep Reinforcement Learning}
abstract:Schedule scheme is essential for real-time Ethernet. Due to the inevitable change of network configurations, the solution requires to be incrementally scheduled in a timely manner. Solver-based methods are time-consuming, while handcrafted scheduling heuristics require domain knowledge and professional expertise, and their application scenarios are usually limited. Instead of designing heuristic strategy manually, we propose TTDeep, a deep reinforcement learning schedule framework, to incrementally schedule Time-Triggered (TT) flows and adapt to various topologies. Our novel framework includes 3 key designs: a period layer to capture the periodical transmission nature of TT flows, the graph neural network to extract and represent topology features, and a 3-step selection paradigm to alleviate the huge action search space issue. Comprehensive experiments show that TTDeep can schedule TT flows much faster than solver-based methods and schedule nearly twice more TT flows on average compared to handcrafted heuristics.
\subsubsection{Collaborative Service Placement for Profit Maximization in Mobile Edge Computing}
abstract:Mobile Edge Computing (MEC) is a promising computing paradigm, which allows mobile devices to offload high-complexity computation tasks to base stations (BSs) with enhanced computation resources so that the high-complexity tasks can be completed in a timely manner. However, due to the limit on the amount of computation resources in a BS, only a few services can be placed on each BS and the number of users that each BS can serve is bounded. Therefore, service providers (SPs) should jointly consider service placement and user selection in MEC. In addition, since BSs tend to be densely deployed, users normally have the access to the services from multiple BSs. Consequently, BSs should cooperatively determine how services are deployed and which BS processes a specific task. Financially, a SP prefers to adopt a service placement and user selection strategy that maximizes its profit. In this paper, we investigate the service placement problem in MEC from the perspective of SP profit maximization. Technically, we propose a Profit-based Greedy Algorithm (PGA) to collaboratively deploy services on a cluster of BSs in order to maximize the SP profit. Our experimental results indicate that PGA outperforms the existing service placement methods in terms of SP profit.
\subsubsection{In-Network Processing Acoustic Data for Anomaly Detection in Smart Factory}
abstract:Modern manufacturing is now deeply integrating new technologies such as 5G, Internet-of-things (IoT), and cloud/edge computing to shape manufacturing to a new level - Smart Factory. Autonomic anomaly detection (e.g., malfunctioning machines and hazard situations) in a factory hall is on the list and expects to be realized with massive IoT sensor deployments. In this paper, we consider acoustic data-based anomaly detection, which is widely used in factories because sound information reflects richer internal states while videos cannot; besides, the capital investment of an audio system is more economically friendly. However, a unique challenge of using audio data is that sounds are mixed when collecting thus source data separation is inevitable. A traditional way transfers audio data all to a centralized point for separation. Nevertheless, such a centralized manner (i.e., data transferring and then analyzing) may delay prompt reactions to critical anomalies. We demonstrate that this job can be transformed into an in-network processing scheme and thus further accelerated. Specifically, we propose a progressive processing scheme where data separation jobs are distributed as microservices on intermediate nodes in parallel with data forwarding. Therefore, collected audio data can be separated faster with even less total computing resources. This solution is comprehensively evaluated with numerical simulations, compared with benchmark solutions, and results justify its advantages.
\subsubsection{Advanced Secure DNS Name Autoconfiguration with Authentication for Enterprise IoT Network}
abstract:Internet of Things (IoT) is an intelligent infrastructure and service technology that connects all objects based on information and communication technology to communicate information between people and things. The number of IoT devices is rapidly increasing in various environments. Although DNS is being applied to IoT networks for efficient unique identifiers, it is highly burdensome for users due to the manual DNS name configuration and the limitation only for local usage. DNS Name Autoconfiguration (DNSNA) was proposed to register the DNS name of IoT devices automatically and utilize IoT devices globally. However, DNSNA without secure authentication and authorization leads to potential threats, such as the registration of malicious IoT devices, and eventually IoT security attacks. In this paper, we propose an Advanced Secure DNS name autoconfiguration with Authentication and Authorization for enterprise IoT network (ASDAI). Especially, we provide the first model using the convergence of extended OAuth 2.0 and Kerberos v5. The proposed protocol supports the secure user authentication and service authorization, and the heterogeneity and scalability of the enterprise IoT networks.
\subsubsection{IoT-ID: Robust IoT Device Identification Based on Feature Drift Adaptation}
abstract:Internet of Things (IoT) devices deployed in publicly-accessible locations increasingly encounter security threats from device replacement and impersonation attacks. Unfortunately, the limited memory and poor computing capabilityon such devices make solutions involving complex algorithms or enhanced authentication protocols untenable. To address this issue, device identification technologies based on traffic characteristics fingerprinting have been proposed to prevent illegaldevice intrusion and impersonation. However, because of time-dependent distribution of traffic characteristics, these approaches often become less accurate over time. Meanwhile insufficient attention has beenpaid to the impact of possible changes on the accuracy of device identification. Therefore, we propose a novel feature selection method based on degree of feature drift and genetic algorithm to keep high accuracy and stability of device identification. The degree of feature drift-- relevance of features through time and gain ratio are combined as a composite metric to filter out stable features. Furthermore, in order to perform equally well in device identification, we use the genetic algorithm to select the most discriminate feature subset. Experiments showthat the accuracy of device recognition compared with other methods is increased from 86.4\% to 94.5\%, and the robustness of recognition is also improved.
\subsubsection{Meta-material Sensors based Internet of Things for 6G Communications}
abstract:In the coming 6G communications, the internet of things (IoT) serves as a key enabler to collect environmental information and is expected to achieve ubiquitous deployment. However, it is challenging for traditional IoT sensors to meet this demand because of their requirement of power supplies and frequent maintenance, which is due to their sense-then-transmit working principle. To address this challenge, we propose a meta-IoT sensing system, where the IoT sensors are based on specially designed meta-materials. The meta-IoT sensors achieve simultaneous sensing and transmission and thus require no power supplies. In order to design a meta-IoT sensing system with optimal sensing accuracy, we jointly consider the sensing and transmission of meta-IoT sensors and propose an efficient algorithm to jointly optimizes the meta-IoT structure and the sensing function at the receiver of the system. As an example, we apply the proposed system and algorithm in sensing environmental temperature and humidity levels. Simulation results show that by using the proposed algorithm, the sensing accuracy can be significantly increased.
\subsubsection{Open Set Mixed-Reality Human Activity Recognition}
abstract:Sensor-based human activity recognition (HAR) is a fundamental problem that can have a broad impact on many research/industrial fields. The deep learning methods pave the way for extracting robust and informative features from the non-stationary HAR data, achieving high-accuracy HAR. Most of the works in the literature consider the close set activity recognition, which assumes all classes of activities are known in both the training and test stages. However, it is a challenging way to apply deep learning models to real-world applications, which can contain unseen activities. These activities will sharply decrease the performance of the deep learning models. To this end, we introduce the problem of Open Set Mixed-Reality (OSM-) HAR, which aims to recognize unseen activities while classify seen samples. Furthermore, we propose a novel balanced open set backpropagation method to realize accurate and robust OSM-HAR. Lastly, we verify the effectiveness of the proposed method with a publicly available and our newly collected dataset.
\subsubsection{Performance Limit of Two-Agent Scheduling with Kinematic Constraints}
abstract:Intelligent multi-agent systems are prospective in various applications such as smart cities and smart factories, while the efficiency of scheduling problems of the agents often suffers from the kinematic constraints. In this paper, we establish the performance limit in the distributed scheduling of two agents with kinematic constraints. Specifically, the minimum achievable loss under full information is first analyzed, and then the robust loss is derived according to the available information. The robust analysis provides key insights on how to evaluate a situation, how to quantify the value of information for the efficient control, and how to characterize the benefit of cooperation between agents. Furthermore, a robust control policy is proposed based on the analysis, and simulation results are presented validating the efficiency of the policy. The robust analysis proposed in this paper provides a new perspective for handling the scheduling problems in intelligent multi-agent systems both theoretically and algorithmically.
\subsubsection{An Internet-of-Vehicles Powered Defensive Driving Warning Approach for Traffic Safety}
abstract:As a major type of driver assistance technologies, automated warning systems provide drivers and vulnerable road users with safety. These systems, such as forward collision warnings, can detect potential risks nearby and alert the drivers. One shortcoming of such warning systems is that their effectiveness and capability depend on the information collected from sensors existing in a single vehicle, which can be highly limited in the presence of occlusion, leading to irreversible consequences. To overcome this shortcoming, in this paper, we benefit from the vehicular sensing and communication technologies to propose a novel Internet-of-vehicles (IoV) powered framework for defensive driving warning, in which a vehicle can take advantage of other vehicles sensing data through V2V communications. We further evaluate the introduced framework in cyclist protection system scenarios. Simulation results demonstrate how the proposed IoV-based framework can improve warning systems by providing increased situational awareness.
\subsubsection{SRRM: Ranking-based Route Mutation Scheme for Software-Defined WSNs}
abstract:In WSNs, packets are delivered through mostly static shortest paths to their destination. However, static packet delivery makes WSNs highly vulnerable to traffic analysis attacks due to open area deployment. Existing defence proposals fail to achieve a balance between the protection level and the resource constraints. In this paper, we present a proactive SDN-based Route Mutation (SRRM) scheme that enables changing the routes of the multiple flows in WSNs simultaneously to defend against passive and stealthy reconnaissance and sniffer attacks while preserving reliable and energy-aware routing. Multiple routes are ranked for packet flow based on node reliability, energy consumption, link cost, and route overlapping. Our extensive simulation results show that these techniques can effectively provide route obfuscation for software-defined WSNs.
\subsubsection{Joint Optimization of UAVs 3-D Placement and Power Allocation in Emergency Communications}
abstract:The UAV deployment as well as power allocation is critical in emergency rescue with the practical diverse requirements of data applications. In this paper, a UAV 3-D deployment scheme driven by multi-level quality of service (QoS) is first presented. Specifically, to harvest more performance gain, a fuzzy clustering-based initialization method is proposed, which can achieve more reliable accuracy of clustering compared with traditional clustering methods. Further, considering more comprehensive effect in terms of particle diversity and iteration, a novel inertia weight update method is developed to accelerate convergence. Simulation results demonstrate that our proposed scheme outperforms other schemes.
\subsection{Technical Papers I}
\subsubsection{Blockchain-Secured Data Collection for UAV-Assisted IoT: A DDPG Approach}
abstract:Internet of Things (IoT) can be conveniently deployed while empowering various applications, where the IoT nodes can form clusters to finish certain missions collectively. In this paper, we propose to employ unmanned aerial vehicles (UAVs) to assist the IoT data collection with blockchain-based security provisioning, towards efficient and safeguarded IoT operations. In particular, a blockchain with proof-of-stake (PoS) consensus mechanism is constructed among the UAVs with the collected IoT data. Correspondingly, we optimize the IoT communication and the UAV deployment for the maximum blockchain throughput considering the PoS procedure. The problem is solved with a deep deterministic policy gradient-based approach, where the power allocation is obtained with closed-form solutions and the UAV deployment is learned with actor-critic networks. Simulation results are provided to show the deployment and performance, corroborating the effectiveness of our proposal.
\subsubsection{A Privacy-Preserving Vehicular Data Sharing Framework atop Multi-Sharding Blockchain}
abstract:Internet of Vehicles (IoV) has become an indispensable technology to bridge vehicles, persons and infrastructures, and is promising to make our cities smarter and more connected. It enables vehicles to exchange vehicular data (e.g., GPS, sensors, and brakes) with different entities nearby. However, sharing these vehicular data over the air raises concerns about identity privacy leakage. Besides, the centralized architecture adopted in existing IoV systems is fragile to single point of failure and malicious attacks. With the emergence of blockchain technology, it has the chance to solve these problems due to its features of tamper-proof, traceability and decentralization. In this paper, we propose a privacy-preserving vehicular data sharing framework based on blockchain. In particular, we design an anonymous and auditable data sharing scheme using Zero-Knowledge Proof (ZKP) technology so as to protect the identity privacy of vehicles while preserving the vehicular data auditability for Trusted Authorities (TAs). In response to high mobility of vehicles, we design an efficient multi-sharding protocol to decrease blockchain communication costs without compromising the blockchain security. We implement a prototype of our framework and conduct extensive experiments and simulations on it. Evaluation and analysis results indicate that our framework can not only strengthen system security and data privacy, but also increase the data authenticity verification efficiency by 5x comparing to existing privacy-preserving schemes.
\subsubsection{Robust 3D Trajectory Optimization for Secure UAV-Ground Communications}
abstract:The utilization of UAV may suffer severe security problems due to the inherent characteristics of wireless air-to-ground (A2G) channels. To this end, researchers have drawn much attention on utilizing physical layer security (PLS) techniques to maintain secrecy data transmission in UAV enablednetworks. Different from previous works, we jointly optimize user scheduling strategy, signal transmission power and 3D flying trajectory of the UAV to maximize the minimum system secrecy rate by considering UAV position and an eavesdropper with partial location information simultaneously. Because theformulated problem is intractable and non-convex, we in this paper develop an iteration approach to solve the problem based on the successive convex approximation (SCA) method. Finally, experimental results are further showed to validate the performance gains of our scheme.
\subsubsection{Effective interoperability and security support for constrained IoT networks}
abstract:The Internet of Things (IoT) paradigm brings together various applications and use cases, in several domains such as Smart City, e-Health, Industrial IoT, etc. The characteristics of these applications range from high-data rate streams to sporadic transmissions of small packets, including critical and low-latency traffic. Some of them such as telemetering, smart agriculture, asset tracking and environment monitoring also require an extended coverage and a long battery life (up to 10 years). Both non-3GPP and 3GPP technologies, such as Sigfox, LoraWAN, NB-IoT and LTE-M, were specifically designed to meet these specific requirements,by reducing for instance the bandwidth and the achievable data rate. These networks are so constrained that it is very challenging for them to support a standard, interoperable network stack, including security protocols.Nevertheless, these features would be highly desirable in order to ease and accelerate the deployment of IoT solutions involving various types of devices, from very constrained low-power sensors to high data rate cameras and critical actuators.In this paper, we study standard interoperable and secure solutions and we show how SCHC, a generic compression and fragmentation mechanism, can be used to enable their support over constrained IoT networks.With our implementation, typical messages are shrunk from 80-100 bytes down to as little as 20-30 bytes.
\subsubsection{Trade-offs in large blockchain-based IoT system design}
abstract:The well known Practical Byzantine Fault Tolerance (PBFT) consensusalgorithm is not well suited to blockchain-based Internet of Things(IoT) systems which cover large geographical areas. To reduce queuingdelays and eliminates a permanent leader as a single point of failure,we use a multiple entry, multi-tier PBFT architecture and investigatethe distribution of orderers that will lead to minimization of thetotal delay from the reception of a block of IoT data to the moment itis linked to the global blockchain. Our results indicate that thetotal number of orderers for given system coverage and total load aremain determinants of the block linking time. We show that, given thedimensions of an area and the number of orderers, partitioning theorderers into a smaller number of tiers with more clusters will leadto lower block linking time. These observations may be used in theprocess of planning and dimensioning of multi-tier clusterarchitectures for blockchain-enabled IoT systems.
\subsection{Technical Papers II}
\subsubsection{Vulnerability Analysis of Road Network under Information Pollution Attacks in VANET}
abstract:As an application of the Internet of Things in the automotive field, Vehicular Ad-hoc NETworks (VANETs) are developed to facilitate traffic safety and traffic flow optimization. VANET consists of the communication network and the underlay road network. Due to the characteristics of open access, the communication network is vulnerable to various attacks, especially, the information pollution attack which is highly risky and concealed. Such an attack can lead the vehicles to react to the false messages from attackers, even cause failure cascades. However, the impact of the information pollution attack on the road network has not gained enough attention. To assess such impact, we build a traffic model with polluted information and assess the vulnerability of the road network under different pollution scenarios in terms of attack percentage, attack types, and road network topologies. Experimental results demonstrate that VANETs are vulnerable to the information pollution attack, and transportation performance drops by half when only 15% of edges are attacked in the worst case.
\subsubsection{Underwater Localization using Airborne Visible Light Communication Links}
abstract:Localization of underwater networks has received lots of attention. However, existing scheme focuses on establishing a relative topology where a node's position is defined in relation to one another. Provisioning global coordinates is achieved only through the inclusion of a surface node that can serve as a reference. This paper opts to tackle the global localization problem in setting where surface nodes cannot be deployed or should be avoided. Our approach is to establish visible light communication (VLC) links across the air-water interface. An airborne unit is to transmit its GPS position using VLC. Upon receiving such a message, the underwater node will factor in the light intensity and coverage to determine its own position relative to the airborne node and consequently its own GPS coordinates. The simulation results demonstrate the effectiveness of our approach and high positioning accuracy.
\subsubsection{Toward The Design of An Efficient Transparent Traffic Environment Based on Vehicular Edge Computing}
abstract:In recent years, with the continuous increasing number of vehicles, how to solve the frequent traffic accidents, the increasing traffic congestion, and the corresponding exhaust pollution in the transportation system is the problem that must be solved to ensure people's safe, efficient, and green travel needs. As one of the core components of future intelligent transportation systems (ITS), autonomous vehicles have become a common area of interest for academia and industry because they can strictly follow traffic laws and regulations while avoiding traffic accidents caused by improper driving behavior of human drivers. However, the current autonomous driving technology often relies on a single vehicle to independently detect its surrounding traffic environment. Under the impression of the detection range of the corresponding detector and the occlusion of different types of objects in the traffic system, a single-vehicle often has a large detection blind spot. As a result, it may not be possible to develop an effective driving strategy in complex environments. For addressing this issue, in this article, we will introduce a collaborative object detection and warning method based on the vehicular edge computing (VEC) to achieve a transparent traffic environment. In other words, by exploiting the computing power provided by the VEC, we will eliminate the detection blind spots of traffic system participants as much as possible. The efficiency of the proposed method will be evaluation by the physical experiments.
\subsubsection{A Blockchain-based Energy Trading Scheme for Dynamic Charging of Electric Vehicles}
abstract:Dynamic charging is a promising technology for Electric Vehicles (EVs) since it allows EVs to replenish its energy supply while on the move. The popular technology for such dynamic recharging utilizes magnetic induction by placing a large number of special charging pads on the roads that EVs pass over while traveling. Unlike the traditional stationary systems, dynamic charging introduces several challenges in how to handle billing, conduct EV authentication, and sustain privacy. The main issue is attributed to the high motion speed of EVs which allows a very short contact time between the resource constrained charging pads and the EVs. Therefore, we propose a lightweight and fast authentication protocol for EV-to-charging-pads; the protocol is incorporated in an energy trading scheme for the dynamic charging of EVs that is based on blockchain technology. We utilize Physically Unclonable Function (PUF) in the creation of a charging ticket in order to prevent double-spending of the ticket without incurring additional overhead. Furthermore, we leverage pseudonyms to preserve the privacy of EVs. Our analysis demonstrates that the proposed protocol is secure and allows a charging pad to authenticate EV in less than 13 sec.
\subsubsection{Efficient In-Network Caching in NDN Based Connected Vehicles}
abstract:Advancements in communication technologies such as Beyond 5G have made it possible for on-demand services in vehicular networks. Novel advancements in the transportation area are needed to meet these feature requirements. Most of the communications in vehicular networks are based on names of content and thus caching inside the network makes Named Data Networking (NDN) a suitable option for content dissemination for connected vehicles. To achieve NDN maximum benefits, a new effective data forwarding scheme is needed as the size of the cache is limited to store all of the requested content, and cache replacement scheme to replace the data which is not required. In this paper, the challenge of in-network caching is addressed in terms of efficient cache utilization by considering the cache hit ratio and the challenge of vehicles' mobility. The volume of data cached during communication is called cache utilization. Many in-network caching strategies are proposed in the literature, but very few of the proposed schemes consider the efficient utilization of cache resources while considering the mobility of vehicles. Default cache replacement technique is Least Recently Used (LRU) whereas Leave Copy Everywhere (LCE), Probabilistic Caching (PC), Always Cache (AC), Leave Copy Down (LCD), Reactive Caching (RC), and Edge Caching (EC) are used as data forwarding schemes. In this paper, existing in-network caching strategies with important role in cache utilization such as data forwarding schemes, and cache replacement schemes, are implemented, evaluated and a new caching scheme is proposed which will address the issues faced by current schemes.
\subsection{Technical Papers III}
\subsubsection{Dynamic Switching in LoRaWAN under multiple Gateways and Heavy Traffic Load}
abstract:The classic ADR method used by LoRa for optimizing data rates, airtime and energy consumption in the network has proven to be not very efficient in scenarios characterized by a high number of transmitting nodes. There is therefore a need to implement alternative mechanisms to reduce collisions and packet transmission delay in high dense IoT network scenario with increasing traffic load. The research presented in this paper contributes to meeting this need by proposing a new mechanism aimed at traffic load balancing that leverages the switching of LoRa devices between Gateways in a multi-Gateway scenario. From the comparison with the classic ADR it clearly emerges how the proposed technique can obtain significantly better performance levels in terms of number of collisions, energy consumption and packet delay.
\subsubsection{Coded Relaying in LoRa Sensor Networks}
abstract:To enhance the reliability of LoRa sensor networks, we propose and analyze two forwarding protocols in which relays overhear sensor-to-gateway messages and periodically forward them to the gateway. A form of coded forwarding is employed in which the relays transmit XOR sums of recently overheard messages. One protocol employs a single relay, while two relays cooperate in the other. With a single relay, our proposed protocol incurs lower relay energy expenditure to achieve similar loss rates as conventional uncoded forwarding in which every received message is immediately forwarded. We demonstrate energy reductions of up to 35% at the relay. The cooperative protocol has better loss and delay performance than the single-relay protocol with slightly higher energy overhead. We provide a mathematical analysis of the schemes, which can be used to choose protocol parameters to optimize delivery performance.
\subsubsection{M-ary Aggregate Spread Pulse Modulation in LPWANs for IoT applications}
abstract:In low-power wide-area networks (LPWANs), various trade-offs among the bandwidth, data rates, and energy per bit have different effects on the quality of service under different propagation conditions (e.g. fading and multipath), interference scenarios, multi-user requirements, and design constraints. Such compromises, and the manner in which they are implemented, further affect other technical aspects, such as system's computational complexity and power efficiency. At the same time, this difference in trade-offs also adds to the technical flexibility in addressing a broader range of IoT applications. This paper addresses a physical layer LPWAN approach based on the Aggregate Spread Pulse Modulation (ASPM) and provides a brief assessment of its properties in additive white Gaussian noise (AWGN) channel. In the binary ASPM the control of the quality of service is performed through the change in the spectral efficiency, i.e., the data rate at a given bandwidth. Implementing M-ary encoding in ASPM further enables controlling service quality through changing the energy per bit (in about an order of magnitude range) as an additional trade-off parameter. Such encoding is especially useful for improving the ASPM's energy per bit performance, thus increasing its range and overall energy efficiency, and making it more attractive for use in LPWANs for IoT applications.
\subsubsection{Receiving Colliding LoRa Packets with Hard Information Iterative Decoding}
abstract:This paper presents symbol querying and symbol SIC, two techniques which allow LoRa receivers to recover colliding packets. A symbol querying receiver allows the demodulator and channel decoder to jointly search for the correct set of symbols during a collision. By operating in the frequency domain, both symbol querying and symbol SIC greatly limit the search space of possible packets, allowing for efficient implementations. Experimental results show that these techniques allow LoRa to elevate error detection to correction and outperform a BICM-ID receiver, receiving 3.8x more frames than a traditional LoRa receiver in a low SINR setting.
\subsection{Technical Papers IX}
\subsubsection{Enabling Mobile Edge Computing for Battery-less Intermittent IoT Devices}
abstract:Intermittent computing enables battery-less systems to support complex tasks such as face recognition through energy harvesting, but without an installed battery. Nevertheless, the latency may not be satisfied due to the limited computing power. Integrating mobile edge computing (MEC) with intermittent computing would be the desired solution to reduce latency and increase computation efficiency. In this work, we investigate the joint optimization problem of bandwidth allocation and the computation offloading with multiple battery-less intermittent devices in a wireless MEC network. We provide a comprehensive analysis of the expected offloading efficiency, and then propose Greedy Adaptive Balanced Allocation and Offloading (GABAO) algorithm considering the energy arrival distributions, remaining task load, and available computing/communication resources. Simulation results show that the proposed system can significantly reduce the latency in a multi-user MEC network with battery-less devices.
\subsubsection{Optimal Deployment of Fog Nodes, Microservices and SDN Controllers in Time-Strict IoT Scenarios}
abstract:The application of Internet of Things (IoT)-based solutions to intensive domains has enabled the automation of real-world processes. The critical nature of these domains requires for very high Quality of Service (QoS) to work properly. These applications often use computing paradigms such as fog computing and software architectures such as the Microservices Architecture (MSA). Moreover, the need for transparent service discovery in MSAs, combined with the need for network scalability and flexibility, motivates the use of Software-Defined Networking (SDN) in these infrastructures. However, optimizing QoS in these scenarios implies an optimal deployment of microservices, fog nodes, and SDN controllers. Moreover, the deployment of each of the different elements affects the optimality of the others, which calls for a joint solution. In this paper, we motivate the joining of these three optimization problems into a single effort and we present Umizatou, a holistic deployment optimization solution. Finally, we evaluate Umizatou over a healthcare case study, showing its scalability in topologies of different sizes.
\subsubsection{PWPAE: An Ensemble Framework for Concept Drift Adaptation in IoT Data Streams}
abstract:As the number of Internet of Things (IoT) devices and systems have surged, IoT data analytics techniques have been developed to detect malicious cyber-attacks and secure IoT systems; however, concept drift issues often occur in IoT data analytics, as IoT data is often dynamic data streams that change over time, causing model degradation and attack detection failure. This is because traditional data analytics models are static models that cannot adapt to data distribution changes. In this paper, we propose a Performance Weighted Probability Averaging Ensemble (PWPAE) framework for drift adaptive IoT anomaly detection through IoT data stream analytics. Experiments on two public datasets show the effectiveness of our proposed PWPAE method compared against state-of-the-art methods.
\subsubsection{PFCC: Predictive Fast Consensus Convergence for Mobile Blockchain over 5G Slicing-enabled IoT}
abstract:As the security requirements increases, 5G slicing-enabled Internet of things needs to adopt end-to-end standalone networking, which limits the consensus convergence of mobile blockchain. Although the rise of FIBRE (Fast Internet Bitcoin Relay Engine) gives a huge promotion to block propagation, existing approaches cannot consider the influence of link outages among 5G slices on mobile blockchain. In this paper, we focus on decreasing the block propagation time among blockchain peers under the scenario where link outages among 5G slices exist. A predictive fast consensus convergence (PFCC) scheme is proposed for mobile blockchain over 5G slicing-enabled internet of things. In PFCC, federated semi-supervised learning is used to learn the features of withdraw packets, reroutes the packets of blockchain peers, and ultimately reduces the scale of link outages quickly. With PFCC, different blockchain peers located in standalone 5G slices of IoT can transact local sensing data more efficiently. To the best of our knowledge, this is the first work to improve the consensus convergence speed of mobile blockchain by optimizing communications between 5G slices. Experiments shows the feasibility of proposed scheme.
\subsubsection{Exploring Tradeoffs between Energy Consumption and Network Performance in Cellular-IoT: a Survey}
abstract:Recent growth in the Internet of Things (IoT) has been remarkable. Among the solutions to accommodate such a growth is Cellular IoT (C-IoT), comprising a group of technologies extended from legacy cellular infrastructures. One of the key goals of C-IoT technologies is to extend the battery life of UEs in the network. However, this often comes at the cost of degrading network performance. This work attempts to identify, categorize, and analyze the available literature on this problem. The literature is broadly categorized into three sections: scheduling, data processing, and sleep modes. In each of these sections, the literature is further sub categorized. Finally, a direction for future research is identified and discussed.
\subsection{Technical Papers V}
\subsubsection{Wi-Adaptor: Fine-grained Domain Adaptation in WiFi-based Activity Recognition}
abstract:Human activity recognition (HAR) has attractedsignificant attention during recent years due to its critical rolein a wide range of applications. Among existing recognitionalgorithms, most of them utilize domain adversarial neuralnetworks to achieve recognition between diverse domains, suchas DANN. However, these methods try to fully align the featuredistributions while each domain has specific characteristics,which leads to different decision boundaries and substantiallydegrade the recognition accuracy.In this paper, we propose a fine-grained method called Wi-Adaptor to tackle these problems. Wi-Adaptor utilizes twoclassifiers to match distributions of source and target samplesby considering the decision boundaries. In order to detect targetsamples that are far from the support of the source, we train theclassifiers to maximize the discrepancy between their outputsand train the feature generator to generate target featuresthat minimize the discrepancy. Our experiments show thatWi-Adaptor outperforms other traditional domain adversarialadaptation models and show robustness as we limit the sourcesamples. Especially in the case of reducing the source samplesto a half, Wi-Adaptor achieves more than 30% accuracy gain indifferent domain adaptation experiments.
\subsubsection{Fire Detection Using Commodity WiFi Devices}
abstract:WiFi Sensing has received tremendous attention in Recent Literature, demonstrating the ability to leverage ubiquitous commercial WiFi devices to sense Human activities and environmental occupancy. We identify that in all environments fire-safety is vital, and this paper demonstrates the suitability for using WiFi to sense fire. Using commodity Raspberry Pi devices on the 5GHz WiFi band we demonstrate a temporal shift in WiFi Channel State Information (CSI) Amplitude, before, during, and after the ignition of a flame. We further emphasise the presence of fire by observing the spread of CSI Amplitudes, noting that CSI takes much more diverse values in the presence of fire. This result is exacerbated by the frequency selective behaviour of OFDM subcarriers, where some subcarriers displayed larger variation in CSI amplitude due to the fire. The WiFi Fire sensing model was evaluated in an ideal setup with a gas flame to remove material deformation as a variable, and subsequently in a real-world scenario with the ignition of building cladding.
\subsubsection{Aerial Data Collection with Coordinated UAV and Truck Route Planning in Wireless Sensor Network}
abstract:Unmanned aerial vehicle (UAV) is a promising way to collect data generated by wireless sensor network, nevertheless, the battery capacity of the UAV restricts its application on many occasions, e.g., the network deployed in the wild. In this paper, we propose a coordinated route planning scheme to deal with the energy issue of the UAV, where a truck carrying backup batteries moves together with the UAV as a "mobile recharging station".Our optimization task is to minimize the total mission time for collecting data from all the sensor nodes.We develop an efficient clustering algorithm to divide the entire mission area into multiple subregions in a load-balanced way to minimize the number of movements of the UAV, and formulate the trajectory planning task as a coordinated traveling salesman problem which is heuristically solved by a three-step route planning algorithm. Numerical results show that our proposed scheme provide an effective and cost-efficient way for the data collection of wireless sensor networks in practical application scenarios.
\subsubsection{Semi-Supervised Learning for Channel Charting-Aided IoT Localization in Millimeter Wave Networks}
abstract:In this paper, a novel framework is proposed for channel charting (CC)-aided localization in millimeter wave networks. In particular, a convolutional autoencoder model is proposed to estimate the three-dimensional location of wireless user equipment (UE), based on multipath channel state information (CSI), received by different base stations. In order to learn the radio-geometry map and capture the relative position of each UE, an autoencoder-based channel chart is constructed in an unsupervised manner, such that neighboring UEs in the physical space will remain close in the channel chart. Next, the channel charting model is extended to a semi-supervised framework, where the autoencoder is divided into two components: an encoder and a decoder, and each component is optimized individually, using the labeled CSI dataset with associated location information, to further improve positioning accuracy. Simulation results show that the proposed CC-aided semi-supervised localization yields a higher accuracy, compared with existing supervised positioning and conventional unsupervised CC approaches.
\subsubsection{Unlabeled Detection via Header-Free Communication in a Wireless Sensor Network}
abstract:The traditional problem of distributed detection is well understood, but what solutions can be derived if the observer is unaware of the origins of the data? This problem, referred to as unlabeled detection in the literature, has several interesting applications. Motivated by the need to improve energy efficiency in internet-of-things and wireless sensor networks, this work presents two novel approaches to the unlabeled detection problem, both utilizing the generalized likelihood ratio test. The first approach exploits tools from traditional detection and convex optimization theory, while the second takes a deep learning approach to the problem. Numerical simulations reveal that both detectors are able to provide robust detection in the presence of reordered measurements. In conditions of extreme reordering, a simultaneous increase in probability of detection and decrease in probability of false alarm of up to 20% and 10%, respectively, is achieved.
\subsection{Technical Papers X}
\subsubsection{A Reliable Real-Time Slow DoS Detection Framework for Resource-Constrained IoT Networks}
abstract:Slow DoS attacks have proven to pose a significantsecurity threat to low-resource IoT devices and networks, because they can be launched by nodes which consume nominalbandwidth and have limited resource capability. This makes suchmalicious attacks easy to initiate, but difficult to mitigate. Therealso exists the recurrent likelihood of misclassifying legitimatenodes, which are incurring slow or poor network connectivity,as malicious activity. Existing intrusion detection systems (IDS)for detecting Slow DoS attacks often require the creation of largedatasets for post event analysis. A functional disadvantage of thisdataset-driven approach is the sheer volume of data required, dueto the high number of network attributes and events collated,which precludes an in-line, real-time IDS detection solutionfor live IoT networks. This paper presents an innovative IDSdetection framework for resource constrained IoT networks.Using a set of only four attributes, a two-step analysis of liveIoT network events enables Slow DoS attacks, in the form ofSlowloris, to be both efficiently and reliably detected in real-time. In addition, this lightweight IDS framework can accuratelydistinguish between malicious and genuine nodes encounteringslow or intermittent network connections.
\subsubsection{STEPS - Score Table based Evaluation and Parameters Surfing approach of LoRaWAN}
abstract:LoRaWAN (LOng RAnge radio Wide Area Network) belongs to the LPWAN (Low Power Wide Area Network) category, it aims to provide a wide area, long range and low power consumption communication network solution. However, with the large number of connected devices to the base-station and a pure Aloha MAC protocol, the packet lost and collisions become an important issue in the network related to the transmission parameters of the devices. For this issue, STEPS is proposed in this study. The goal of the proposed method is to establish a score table based evaluation and parameters surfing approach. STEPS provides an approach of evaluating and changing the parameters based on probability and score table when transmission failure appears. It can also update the table while transmitting. STEPS is compatible with pure Aloha, and doesn't need any change on the MAC protocol and is easy to be implemented. In this study, the spreading factor is chosen as the parameter to establish the table, that will be evaluated and to be surfed. Simulation results show that STEPS provides a remarkable improvement on bi-directional transmission of the packet and a capability of decreasing the packet lost and collisions even with higher data rate than classical LoRaWAN scenario.
\subsubsection{Direct Analytics of Generalized Deduplication Compressed IoT Data}
abstract:Given the ever increasing volume of data generated by the Internet of Things, data compression plays an essential role in reducing the cost of data transmission and storage. However, it also introduces a barrier, namely decompression, between users and the data-driven insights they require. We propose techniques for direct analytics of compressed data based on the Generalised Deduplication compression algorithm. When applied to data clustering, the accuracy of the proposed method differs by merely 1-5% when compared to analytics performed upon the uncompressed data. However, it runs four times faster, accesses only 14% as much data and, since the data is always compressed, requires significantly less storage. These results show that it is possible to simultaneously reap the benefits of compression and accurate, high-speed analytics in many applications.
\subsubsection{Thermal Profiling by WiFi Sensing in IoT Networks}
abstract:Extensive literature has shown the possibility of using WiFi to sense large scale environmental features such as people, movement, and human gestures. As of yet, no work has been done on identifying microscopic changes in a channel, such as atmospheric temperature. We identify this as a real world use case, since there are scenarios such as Data Centres where WiFi traffic is omnipresent and temperature monitoring is important. We develop a framework for sensing temperature using WiFi Channel State Information (CSI), proposing that the increased kinetic energy of ambient gas particles will affect the wireless link. To validate this, our paper uses low wavelength 5GHz WiFi CSI from commodity hardware to measure how the channel changes as the ambient temperature is raised. Empirically, we demonstrate that the CSI amplitude value drops at a rate of 13 per degree Celsius rise in the ambient temperature based on the testing platform, and developed regressions models with 1 C accuracy in the majority of cases. Moreover, we have shown that WiFi subcarriers exhibit a frequency-selective behaviour in their varying responses to the rise in ambient temperature.
\subsubsection{Congestion-Aware Routing in Dynamic IoT Networks: A Reinforcement Learning Approach}
abstract:The innovative services empowered by the Internet of Things (IoT) require a seamless and reliable wireless infrastructure that enables communications within heterogeneous and dynamic low-power and lossy networks (LLNs). The Routing Protocol for LLNs (RPL) was designed to meet the communication requirements of a wide range of IoT application domains. However, a load balancing problem exists in RPL under heavy traffic-load scenarios, degrading the network performance in terms of delay and packet delivery. In this paper, we tackle the problem of load-balancing in RPL networks using a reinforcement-learning framework. The proposed method adopts Q-learning at each node to learn an optimal parent selection policy based on the dynamic network conditions. Each node maintains the routing information of its neighbours as Q-values that represent a composite routing cost as a function of the congestion level, the link-quality and the hop-distance. The Q-values are updated continuously exploiting the existing RPL signalling mechanism. The performance of the proposed approach is evaluated through extensive simulations and compared with the existing work to demonstrate its effectiveness. The results show that the proposed method substantially improves network performance in terms of packet delivery and average delay with a marginal increase in the signalling frequency.
\subsection{Technical Papers XIII}
\subsubsection{Reliable Target Positioning in Complicated Environments Using Multiple Radar Observations}
abstract:Accurate and reliable multi-target detection and localization in complicated environments is challenging, due to the existence of multiple reflectors that may distract the observations. In this paper, we try to adopt the linear frequency modulated continuous wave (LFMCW) signals, in addition to the antenna array, to perform target detection and positioning. Aiming at the ghost observations which may be generated during the measurement and data fusion phases, we try to properly combine the observations from multiple radar nodes using a probabilistic data fusion framework. The positions of targets could also be obtained straightforwardly using the minimum mean squared error (MMSE) estimator. The fundamental limits of target positioning are also provided in terms of the Cramer Rao Lower Bound (CRLB). Furthermore, we try to accurately identify the target of interest by characterizing its micro-Doppler effects. Both simulations and experiments are carried out to show that, the ghosts could be effectively eliminated by the proposed multi-radar-fusion framework. Meanwhile, the target positioning accuracy can also be obviously improved.
\subsubsection{Short Packet Communications with Random Arrivals: An Effective Bandwidth Approach}
abstract:Short packet transmission techniques have recently attracted substantial attention due to their potential of achieving low latency in emerging Industrial Internet of Things (IIoT). To this end, finite-blocklength coding is expected to play a central role in short packet communications. With the random arrival of short packets, there exists a fundamental tradeoff between reliability and latency in finite-blocklength coding based transmission systems. In this paper, we consider the reliability-latency tradeoff of short packet transmission over AWGN channels. More specifically, an effective bandwidth approach is adopted to obtain the delay violation probability given a hard delay constraint. We present an approximate but closed-form Quality-of-Service (QoS) exponent that bridges the delay violation probability of short packets and the error probability of finite-blocklength coding. Numerical results shall validate our theoretical analysis, which characterizes a performance limit of ultra-reliable and low latency communications (URLLC) over AWGN channels.
\subsubsection{Visible Forensic Investigation for Android Applications by Using Attack Scenario Reconstruction}
abstract:With the widespread use of Android devices, research on their security has attracted increasing attention. At present, digital forensics for investigating attacks, such as social engineering attacks and phishing, that target Android users remains a challenging and time-consuming task. To help discover the existence of an attack and conduct effective investigations, we propose a top-down digital forensic tool for Android applications, to reconstruct attack scenarios by considering both high-level user interface (UI) elements and low-level system events. Thus, we can explain the nature of an attack from a visual and global perspective. The tested evaluation results show that our tool can successfully reconstruct scenarios on Android devices for phishing attacks.
\subsubsection{Impacts of Soil and Antenna Characteristics on LoRa in Internet of Underground Things}
abstract:Long-range (LoRa) is a suitable candidate for underground wireless communications due to its capability of communicating over a long range. However, due to the uniqueness of soil properties at a given geographical location, and the varying nature of soil moisture, it is challenging to apply a universal approach to characterize LoRa in wireless underground channels. In this paper, the performance of LoRa in underground channels is studied both theoretically and empirically. The range and bit error rate (BER) formulation of LoRa is derived as a function of soil parameters based on statistical underground channel models. To validate the model, path loss measurements are conducted under different moisture levels in two soil types (sandy and silty clay loam soil). In addition, as underground communication is also dependent on the return loss of buried antennas, the path loss measurements are performed using two different types of underground antennas. Results show that the underground channel models agree well with empirical LoRa measurements, resulting in R-squared values of 0.87-0.89. The results suggest that the performance of LoRa in underground channels can be predicted using the models developed in this paper.
\subsubsection{Adversarial Attacks to Solar Power Forecast}
abstract:With the development of the photovoltaic industry, solar power generation forecasting using weather data has become an important problem. Various machine learning (ML) algorithms have been proposed to handle the random and massive weather data, with considerable recent interest on deep neural networks (DNN). Recent studies show that DNNs are vulnerable to adversarial examples, but most prior work has been focused on the classification problem. In this paper, we investigate the problem of adversarial attacks on solar power generation forecasting, which is a regression problem. We examine the impact of adversarial attacks on both the DNN model and a LASSO-based statistical model proposed in our prior work. Both white-box attack and black-box attack are examined, along with the effect of adversarial training.
\subsection{Technical Papers XIV}
\subsubsection{Maximum Receiver Harvesting Area of Backscatter Signals from Ambient Low-Frequency Mobile Networks}
abstract:The purpose of this paper is to estimate the maximum achievable range for ambient backscattering communications (AmBC) by utilizing one of the lowest available frequency bands for mobile networks. Long term evolution (LTE) networks operating at 700 MHz (LTE-700, also referred to as LTE band 28) have been proposed as one of the operating frequency bands for the fifth generation (5G) of mobile communications. LTE-700 networks use the frequency division duplexing (FDD) technique for communications and are utilised as the ambient signals to perform the simulations. The simulations are carried out in urban macro-cellular and suburban highway environments. For the simulations, the sensors are placed in the line-of-sight (LOS) path of the LTE-700 transmitter and receiver antenna as this ensures the maximum applicability of the AmBC technology. Two propagation models, the ray tracing approach and the radar equation are leveraged to determine the maximum range of communication when the signal is reflected by the sensor. A maximum range of 0.55 km and 0.95 km can be achieved by utilizing the ray tracing technique in urban and suburban environments, respectively. In an urban environment, a total distance of 1.59 km can be achieved using the radar equation. In a suburban macro-cellular environment, the maximum achievable range of communication is 2.83 km. The size of the sensor has a pivotal role in determining the maximum range of communication while utilising the radar equation. Therefore, a thorough analysis is performed using real-world sensor sizes deployed for the internet of things (IoT) wireless communication.
\subsubsection{Cooperative Dynamic Coverage Control in Wireless Camera Sensor Networks with Anisotropic Perception}
abstract:Coverage control is an essential problem in wireless camera sensor networks (WCSNs), and how to realize cooperative dynamic coverage control in WCSNs with anisotropic perception receives wide concern. In this paper, first we design a coverage metric integrating both the perception quality and the cover rate, in which the dynamic accumulation of coverage performance over time is also considered. To characterize the perception and the motion traits of the WCSNs, an anisotropic sensing model and the unicycle kinematic model are adopted. Then we propose a two-level cooperative dynamic coverage control scheme for the WCSNs, which incorporates both the time-domain cooperation among time instants and the spatial-domain cooperation among agents. Compared with the traditional area-oriented methods, our scheme achieves target-oriented coverage based on the density function within the region. Numerical results verify the performance of our scheme in terms of the total and the perception cover rates.
\subsubsection{OTFS-superimposed PRACH-aided Localization for UAV Safety Applications}
abstract:The adoption of Unmanned Aerial Vehicles (UAVs) for public safety applications has skyrocketed in the last years. Leveraging on Physical Random Access Channel (PRACH) preambles, in this paper we pioneer a novel localization technique for UAVs equipped with cellular base stations used in emergency scenarios. We exploit the new concept of Orthogonal Time Frequency Space (OTFS) modulation (tolerant to channel Doppler spread caused by UAVs motion) to build a fully standards-compliant OTFS-modulated PRACH transmission and reception scheme able to perform time-of-arrival (ToA) measurements. First, we analyze such novel ToA ranging technique, both analytically and numerically, to accurately and iteratively derive the distance between localized users and the points traversed by the UAV along its trajectory. Then, we determine the optimal UAV speed as a trade-off between the accuracy of the ranging technique and the power needed by the UAV to reach and keep its speed during emergency operations. Finally, we demonstrate that our solution outperforms standard PRACH-based localization techniques in terms of Root Mean Square Error (RMSE) by about 20% in quasi-static conditions and up to 80% in high-mobility conditions.
\subsubsection{An Analysis of Amazon Echo's Network Behavior}
abstract:With over 20 million units sold since 2015, Amazon Echo, the Alexa-enabled smart speaker developed by Amazon, is probably one of the most widely deployed Internet of Things consumer devices. Despite the very large installed base, surprisingly little is known about the device's network behavior. We modify a first generation Echo device, decrypt its communication with Amazon cloud, and analyze the device pairing, Alexa Voice Service, and drop-in calling protocols. We also describe our methodology and the experimental setup. We find a minor shortcoming in the device pairing protocol and learn that drop-in calls are end-to-end encrypted and based on modern open standards. Overall, we find the Echo to be a well-designed device from the network communication perspective.
\subsubsection{Data Freshness Optimization in Relaying Network Operating with Finite Blocklength Codes}
abstract:In this paper, we focus on a relaying network working with a decode-and-forward (DF) principle. A source reports latency-critical information updates to the destination with the help of the relay under periodic request, while this two-hop transmission is operating with finite blocklength (FBL) codes. To evaluate the data freshness at destination, we characterize the average age-of-information (AoI) of the two-hop relaying. Based on the characterization, we consider a problem minimizing the average AoI by jointly optimizing the blocklengths allocated to both hops. To address this non-convex problem, we construct a tight convex approximation for the average AoI at a feasible local point (values of the two blocklengths). Then, we propose an efficient algorithm which iteratively applies the convex approximation, solves the approximated convex problem and updates the local point until a convergence to a suboptimum. Via numerical results, we validate the convergence of the proposed iterative algorithm and confirm the high performance and high efficiency of the proposed solution. The performance advantage of relaying in improving the data freshness is also shown in comparison to direct transmission.
\section{Mobile and Wireless Networks}
\subsection{Beamforming and MIMO Communications}
\subsubsection{Investigation of Beamforming-NOMA in mmWave Mobile Communication Networks with Spectrum Sharing}
abstract:Orthogonal Multiple Access (OMA) schemes have been widely adopted in microwave mobile networks to reduce interference. In the sub-6GHz spectrum, current LTE and 5G systems exploit the orthogonal communication resources in terms of time, frequency or code in order to improve the Signal-to-Interference-and-Noise-Ratio (SINR) levels, leading to fast and reliable links. On the other hand, the low spectral usage of OMA schemes make them unsuitable for use in future generation mobile networks especially in the mmwave spectrum. The high propagation losses at mmwave frequencies and the use of M-MIMO (Massive Multiple-Input Multiple-Output) antennas allow experimentation with new techniques that allow a highly aggressive spectrum utilization. In this paper, a beam-based Non-orthogonal Multiple Access (NOMA) scheme is combined with Spectrum Sharing (SS) as a potential solution for successfully meeting the requirements set for the future generation mmwave networks. Single captures of a network were ray-traced under multiple access strategies and network performance was evaluated in terms of SINR and throughput.
\subsubsection{Deep Learning-based Coordinated Beamforming for Massive MIMO-Enabled Heterogeneous Networks}
abstract:Coordinated beamforming (CoBF) for multi-user massive multiple-input and multiple-output (MIMO) heterogeneous networks (HetNets) promises for capacity enhancement. However, challenges of energy efficiency (EE) and ultra-low latency are yet to be addressed due to the circuit power and calculation latency heavily depend on the number of transmit antennas. To solve these problems, a maximizing EE algorithm named coordinated beamforming based on convolutional neural networks (CoBFCNN) is proposed in which the advantages of convolutional neural networks and deep learning are fully exploited. Basing on the results of this study, an optimization problem of maximizing EE with lower complexity and lower calculation latency for the different constraints is formulated and exploited for multi-user massive MIMO HetNets. Simulation and analysis show that the proposed CoBFCNN algorithm can significantly satisfy the performance of maximizing EE for the multi-user massive MIMO HetNets with significantly lower complexity and ultra-low calculation latency, especially when the number of antennas is large.
\subsubsection{Antennas/PINs Selection and Joint Beamforming for High Rank LOS MmWave Communications}
abstract:The reconfigurable intelligent surface (RIS), which converts the wireless channel into an intelligent transmit entity, has been regarded as one of new paradigms for beyond fifth-generation (5G) wireless communication. One of the main functions for RIS is converting the none-line-of-sight (NLOS) channel to the line-of-sight (LOS) channel. Due to the LOS environment, RIS can be naturally combined with millimeter wave (mmWave) communications. However, due to the high correlation in the LOS channel, the combination of RIS and mmWave communication results in a low rank channel. To tackle this problem, we investigate the usage of uniform circular array (UCA) to leverage the potential circulant characteristic of two parallel UCAs. We develop the antennas/PINs selection and joint beamforming scheme to derive equivalent UCAs in traditional uniform planar array (UPA) systems. As the result, the low rank LOS channel can be converted to high rank channel due to the full rank feature of circulant matrix. Numerical results verify the superiority of our antennas/PINs selection and joint beamforming scheme.
\subsubsection{Joint Time Allocation and Beamforming Design for IRS-Aided Coexistent Cellular and Sensor Networks}
abstract:Internet of things (IoT) technology is an essential enabler to realize ubiquitous connections and pervasive intelligence for the future wireless communication system. The energy self-sustainability based on the wireless power transfer technique and the coexistence with heterogeneous networks will become two predominant attributes of IoT networks. In this paper we consider the system design in a context of coexistence of a wireless powered sensor network and a cellular system, both of which share common spectrum bandwidth and are assisted by intelligent reflecting surface (IRS). Specifically, the wireless sensors exploit the harvested energy from the cellular base station (BS) to transfer information to a data sink. We aim to design a cooperation scheme via jointly optimizing the time allocation of channel use, collaborative beamforming across networks and IRS phase-shifting control to improve the sensing network's throughput while guaranteeing the cellular users' quality of service. This design problem leads to a highly nonconvex and difficult mathematical optimization problem. Via utilizing the penalty-duality-decomposition (PDD) and successive convex approximation (SCA) methods, we have managed to develop an alternative optimization solution. Numerical results verify the effectiveness of our algorithm and demonstrate the benefits that come from the cooperative network design.
\subsubsection{Joint Resource Block Allocation and Beamforming with Mixed-Numerology for eMBB and URLLC Use Cases}
abstract:Mixed-numerology has been proposed in the Third Generation Partnership Project (3GPP) standard for the fifth generation (5G) wireless networks, where flexible subcarrier spacing (SCS) can be applied to support uses cases with different quality-of-service (QoS) requirements. In this paper, we study the joint design of resource block allocation and beamforming with mixed-numerology for enhanced mobile broadband (eMBB) and ultra-reliable low-latency communications (URLLC) use cases. We consider multiple multi-antenna base stations (BSs) cooperatively provide services to the users. By using beamforming, inter-user interference can be mitigated and a resource block can be utilized by more than one user. Short packet transmission is considered for URLLC users to satisfy their low-latency requirements. We formulate a mixed-integer nonlinear programming problem to maximize the aggregate throughput of eMBB users while guaranteeing the throughput, reliability, and latency of URLLC users. We propose a low-complexity algorithm, which leverages fractional programming and successive convex approximation (SCA), to obtain the solutions. Simulation results show that our proposed algorithm can improve the aggregate eMBB throughput by 30% compared with the fixed-numerology based approach.
\subsection{Blockchain and Age factor in Wireless Networks}
\subsubsection{Transmission policy design for critical services subject to reliability versus age minimization}
abstract:Critical services in wireless networks have different performance objectives depending on the application they convey. For instance, the 5G defined class of services Ultra-Reliable Low-Latency Communication (URLLC) is designed for traffic such as machine automation applications and aims to achieve high reliability, on the order of 99.999%, and small latency, as low as 1 ms. A second set of critical services, used generally in monitoring applications, values freshness of information at the receiver side, and aims thus to minimize the age of the received packets. In this paper, we study the transport of these two sets of services over unlicensed spectrum. We propose a transmission scheme which adapts the transmission power depending on the packet status, in order to achieve the target performance objectives while minimizing power consumption. We show that the differing objectives lead to opposite policies in terms of increasing or decreasing transmission power levels with respect to delay.
\subsubsection{Joint Link Rate Selection and Channel State Change Detection in Block-Fading Channels}
abstract:In this work, we consider the problem of transmission rate selection for a discrete time point-to-point block fading wireless communication link. The wireless channel remains constant within the channel coherence time but can change rapidly across blocks. The goal is to design a link rate selection strategy that can identify the best transmission rate quickly and adaptively in quasi-static channels. This problem can be cast into the stochastic bandit framework, and the unawareness of time-stamps where channel changes necessitates running change-point detection simultaneously with stochastic bandit algorithms to improve adaptivity. We present a joint channel change-point detection and link rate selection algorithm based on Thompson Sampling (CD-TS) and show it can achieve a sublinear regret with respect to the number of time steps T when the channel coherence time is larger than a threshold. We then improve the CD-TS algorithm by considering the fact that higher transmission rate has higher packet-loss probability. Finally, we validate the performance of the proposed algorithms through numerical simulations.
\subsubsection{Estimation Performance of Cyber-Physical Systems Attacked by False Data Injection}
abstract:This paper investigates the state estimation error covariances of cyber-physical systems (CPSs) when its physical system and transmission channels are attacked by false data injection (FDI). Specifically, sensors in CPSs measure the physical systems' state and then send these measurement information to a fusion center through transmission channels. Due to easy deployment and maintenance of sensors as well as unprotected wireless transmission medium, CPSs in sensing and communication are vulnerable to malicious attacks. To maliciously degrade the system estimation performance, an intruder attempts to modify the state and measurements by launching FDI attacks. To seek an optimal attack strategy from the perspective of an attacker, the corresponding estimate error covariances of CPSs are calculated. Based on this, we compare the estimation error covariances of different attacks on the physical system and transmission channels. Simulations demonstrate the effectiveness of the theoretical results.
\subsubsection{Age-Critical Frameless ALOHA Protocol for Grant-Free Massive Access}
abstract:In this paper, we analyze the freshness of information in grant-free massive access via a new metrics named age of information (AoI), and propose an age-critical frameless ALOHA (ACFA) random access protocol, where the average AoI (AAoI) is implicitly reduced by banning the transmission of activated user equipments (UEs) recovery successful in the last frame. In particular, in order to analyze the AAoI performance of the ACFA random access protocol, we define two metrics named the average channel load and packets recovery rate (PRR) of ACFA protocol, and tracking the evolution of the number of access-allowed UEs in each frame. Then we derive an analytical expression of AAoI as a function of the frame length and the PRR in the ACFA random access protocol. Simulation results validate the accuracy of our theoretical analysis and show the great potential of ACFA random access protocol in minimizing AAoI.
\subsubsection{On the Performance of Blockchain-enabled RAN-as-a-service in Beyond 5G Networks}
abstract:Blockchain (BC) technology can revolutionize the future of communications by enabling decentralized and open sharing networks. In this paper, we propose the application of BC to facilitate Mobile Network Operators (MNOs) and other players such as Verticals or Over-The-Top (OTT) service providers to exchange Radio Access Network (RAN) resources (e.g., infrastructure, spectrum) in a secure, flexible and autonomous manner. In particular, we propose a BC-enabled reverse auction mechanism for RAN sharing and dynamic users' service provision in Beyond 5G networks, and we analyze its potential advantages with respect to current service provisioning and RAN sharing schemes. Moreover, we study the delay and overheads incurred by the BC in the whole process, when running over both wireless and wired interfaces.
\subsection{Cellular Systems and Emerging Technologies}
\subsubsection{Benchmarking Framework for Reconfigurable Intelligent Surfaces}
abstract:From an application point of view, we use RIS that is made from a certain number of unit cells (meta-atoms) to generate some desired radiation patterns. As the number of unit cells on RIS is allowed to increase complex radiation patterns can be generated. Similarly, a unit cell having more phase control and maximum difference between phases also helps in the generation of more complex radiation patterns at the RIS level. However, the control circuit complexity and energy requirements increase. In many practical applications, we might not need every possible RIS radiation pattern and only a handful of them might be sufficient. Additionally, the overall problem of finding the appropriate unit cell control state to generate any desired radiation patterns has combinatorial nature. In this paper, we propose a benchmarking framework and suitable performance metrics that enable us to determine and compare the capabilities of RISs made from different unit cells. The proposed framework is numerically tested on three 40x40 RISs made from 1-bit optimized (0, 180) and 2-bit optimized (0, 90, 180, 270) unit-cells reported in the literature and another 1-bit unoptimized unit cell (50-degree phase difference). While the performance of RIS made from 1-bit unoptimized unit cell was overall poor, rather surprisingly, it was able to successfully generate some simple but useful benchmarking patterns. Thus, our framework provides a mechanism to identify the best candidate unit cell for a given application. This framework would also guide further research and development on unit cell and RIS design.
\subsubsection{On Evaluating Delegated Digital Signing of Broadcasting Messages in 5G}
abstract:In 5G networks, base stations (namely gNBs) periodically broadcast the system information messages including network identifiers to facilitate User Equipment (UE) to connect to the network. As in prior generations, the system information messages in 5G are transmitted in clear text without any security protection. Therefore, an adversary could spoof a legitimate gNB to become a man-on-the-side (MOTS) or man-in-the-middle (MITM) attacker. This vulnerability is being studied by 3GPP and a number of solutions have been proposed in the Technical Report (TR 33.809), including a promising solution namely Digital Signing Network Function (DSnF). In this paper, we provided an evaluation of DSnF, including the practicality of its assumption, feasibility of its certificate transmission within the system information message, and quantitative analysis of its performance. Our evaluation results show that DSnF is practical in general. Initial results from this paper have been provided to 3GPP and incorporated into TR 33.809.
\subsubsection{Signaling Overhead-Constrained Throughput Optimization for 5G Packet-Based Random Access with mMTC}
abstract:To reduce the signaling overhead for sporadic small packet transmission in massive Machine Type Communications (mMTC), Packet-Based Random Access (PBRA) scheme is introduced in 5G system, where devices can transmit data packets inthe random access procedure without connection establishment. Yet, even with PBRA, the signaling overhead may surge if the system parameters are configured improperly. This paper aims to address this issue by studying how to tune the Access Class Barring (ACB) factor to maximize the throughput while maintaining the signaling-to-throughput ratio below a certain level. Explicit expressions of maximum throughput and the corresponding optimal ACB factor in saturated and unsaturated cases are derived. It reveals that with a demanding requirementon signaling-to-throughput ratio, the throughput performance has to be sacrificed even with optimal tuning of ACB factor. To boost the throughput performance, the system should either loose the signaling constraint or enlarge the packet length. The analysis is verified by simulations and sheds important light on practical 5G network design for supporting mMTC with PBRA.
\subsubsection{Driver Distraction Impact on Road Safety: A Data-driven Simulation Approach}
abstract:Driver distraction identification is crucial to improve road safety. Through vehicular communications, vehicles can exchange driver behavior information, based on which distracted drivers can be identified, and all drivers can be notified, which can mitigate the impact of driver distraction on road safety. To build such systems, it is essential to understand the different types of distraction, and how they affect driver behavior and their relationship to crashes or near-crashes. This understating should be based on real datasets, which are very limited. Therefore, in this paper, we cover this gap by building a data-driven simulation model to quantify the impact of realistic driver distraction on traffic safety. In particular, we use the 2nd Strategic Highway Research Program Naturalistic Driving Study (SHRP2 NDS) dataset to develop a simulation framework for driver distraction. First, we pre-process and analyze the dataset for different types of distractions. The analysis shows that the data can not be fitted to any of the known distributions. Therefore, we use the Gaussian Mixture Model (GMM) to represent the distraction intervals for the different distraction types. We then use these GMM models and the statistics collected from the data to realistically simulate the driver distraction using the Simulation for Urban MObility (SUMO) software. Finally, we use this framework to simulate the driver distraction in a real network. The data analysis and simulation results revealed important and interesting conclusions, such as decreasing the crash ratio when roads become congested.
\subsubsection{A Collaborative Statistical Actor-Critic Learning Approach for 6G Network Slicing Control}
abstract:Artificial intelligence (AI)-driven zero-touch massive network slicing is envisioned to be a disruptive technology in beyond 5G (B5G)/6G, where tenancy would be extended to the final consumer in the form of advanced digital use-cases. In this paper, we propose a novel model-free deep reinforcement learning (DRL) framework, called collaborative statistical Actor-Critic (CS-AC) that enables a scalable and farsighted slice performance management in a 6G-like RAN scenario that is built upon mobile edge computing (MEC) and massive multiple-input multiple-output (mMIMO). In this intent, the proposed CS-AC targets the optimization of the latency cost under a long-term statistical service-level agreement (SLA). In particular, we consider the Q-th delay percentile SLA metric and enforce some slice-specific preset constraints on it. Moreover, to implement distributed learners, we propose a developed variant of soft Actor-Critic (SAC) with less hyperparameter sensitivity. Finally, we present numerical results to showcase the gain of the adopted approach on our built OpenAI-based network slicing environment and verify the performance in terms of latency, SLA Q-th percentile, and time efficiency. To the best of our knowledge, this is the first work that studies the feasibility of an AI-driven approach for massive network slicing under statistical SLA.
\subsubsection{Wireless Indoor Simultaneous Localization and Mapping Using Reconfigurable Intelligent Surface}
abstract:Indoor wireless simultaneous localization and mapping (SLAM) is considered as a promising technique to provide positioning services in future 6G systems. However, the accuracy of traditional wireless SLAM system heavily relies on the quality of propagation paths, which is limited by the uncontrollable wireless environment. In this paper, we propose a novel SLAM system assisted by a reconfigurable intelligent surface (RIS) to address this issue. By configuring the phase shifts of the RIS, the strength of received signals can be enhanced to resist the disturbance of noise. However, the selection of phase shifts heavily influences the localization and mapping phase, which makes the design very challenging. To tackle this challenge, we formulate the RIS-assisted indoor SLAM optimization problem and design a particle filter based localization and mapping algorithm. Simulations show that the RIS assisted SLAM system can achieve a higher accuracy compared with benchmark schemes.
\subsection{Cross Layer Design, Optimization, Mobile Users}
\subsubsection{Timing Side Information Aided Real-Time Monitoring of Discrete-Event Systems}
abstract:Industrial Internet of Things (IIoT) has attracted considerable attention recently due to its potential application in factory automation e.g., manufacturing or production systems. As most manufacturing operations can be modeled by discrete event systems (DESs), how to monitor a DES remotely and in a timely manner through sensors and communication links needs investigation in IIoT. In this paper, we present a lossless data compression method for real-time monitoring of DESs. In particular, we find that timing siding information (TSI) is available in delay-constrained communications. Based on the TSI, the data rate required to describe a DES can be substantially reduced. To this end, we derive the minimum data rate of a DES as a conditional entropy from an information-theoretic perspective. Low complexity compression algorithms are also developed. Both analytical and numerical results demonstrate the TSI-enabled compression gain in three typical scenarios.
\subsubsection{Federated Learning with User Mobility in Hierarchical Wireless Networks}
abstract:Recently, the implementation of federated learning (FL) in wireless networks becomes a hotspot due to its flexible collaborative learning methods and privacy-preserving benefits. However, most of the existing works overlook the impact of user mobility on the learning performance, which is critical. Specifically, the mobile users may roam among multiple edge access points (APs) during the local training procedures, leading to incompletion of inconsistent FL training. In this paper, we theoretically study the impact of user mobility on the Fl in hierarchical wireless networks. In our system model, the network consists of one cloud server, several edge APs, and multiple mobile users that have their positions vary over time. During the local training process, users may stay in or move out of the coverage area of the originally attached edge AP. In such a practical context, we analyze the convergence rate of the FL algorithm and provide experiments to evaluate the learning performance under different network parameters. Our results provide insights in further improvements of FL in hierarchical wireless networks.
\subsubsection{Energy-Efficient Device Assignment and Task Allocation in Multi-Orchestrator Mobile Edge Learning}
abstract:Mobile Edge Learning (MEL) is a decentralizedlearning paradigm that enables resource-constrained IoT devicesto either learn a shared model without sharing the data, or todistribute the learning task with the data to other IoT devices andutilize their available resources. In the former case, IoT devices(a.k.a learners) need to be assigned an orchestrator to facilitatethe learning and models' aggregation from different learners.Whereas in the latter case, IoT devices act as orchestrators andlook for learners with available resources to distribute the learningtask to. However, the coexistence of multiple learning problemsin an environment with limited resources poses the learners-orchestratorassignment problem. To this end, we aim to developan energy-efficient learner assignment and task allocation scheme,in which each orchestrator gets assigned a group of learners basedon their communication channel qualities and computationalresources. We formulate and solve a multi-objective optimizationproblem to minimize the total energy consumption and maximizethe learning accuracy. To reduce the solution complexity, wealso propose a lightweight heuristic algorithm that can achievenear-optimal performance. The conducted simulations show thatour proposed approaches can execute multiple learning tasksefficiently and significantly reduce energy consumption comparedto current state-of-art methods.
\subsubsection{Incentivizing Mobile Edge Caching and Sharing: An Evolutionary Game Approach}
abstract:Mobile Edge Caching is a promising technique to enhance the content delivery quality and reduce the backhaul link congestion, by storing popular contents at the network edge or mobile devices (e.g. base stations and smartphones) that are proximate to content requesters. In this work, we study a novel mobile edge caching framework, which enables mobile devices to cache and share popular contents with each other via device-to-device(D2D) links. We are interested in the following incentive problem of mobile device users: whether and which users are willing to cache and share what contents, taking the user mobility and cost/reward into consideration. The problem is challenging in a large-scale network with a large number of users. We introduce the evolutionary game theory, an effective tool for analyzing large-scale dynamic systems, to analyze the mobile users' content caching and sharing strategies. Specifically, we first derive the users' best caching and sharing strategies, and then analyze how these best strategies change dynamically over time, based on which we further characterize the system equilibrium systematically. Simulation results show that the proposed caching scheme outperforms the existing schemes in terms of the total transmission cost and the cellular load. In particular, in our simulation, the total transmission cost can be reduced by 42.5%55.2% and the cellular load can be reduced by 21.5%56.4%.
\subsubsection{Optimizing Federated Edge Learning on Non-IID Data via Neural Architecture Search}
abstract:To exploit the vast amount of distributed data across edge devices, Federated Learning (FL) has been proposed to learn a shared model by performing distributed training locally on participating devices and aggregating the local models into a global one. The existing FL algorithms suffer from accuracy loss due to that data samples across all devices are usually not independent and identically distributed (non-i.i.d.). Besides, devices might lose connection during the training process inwireless edge computing. Thus, we advocate one-shot Neural Architecture Search technique as a basis to propose a solution which can deal with non-i.i.d. problem and is robust to the intermittent connection issue. We adopt a large networkas the global model which includes all the candidate network architectures. The non-i.i.d. problem is alleviated by two steps: (1) identify and train the candidate networks which are potentially high performance and trained with less bias using a heuristic sampling scheme; (2) search for the final model with the highest accuracy rate from the candidate networks. Experimental results show that the model trained by our proposed method is robust to non-i.i.d. problem and can achieve 84% reduced communication overhead compared with the baselines.
\subsection{Delay Tolerant Networks, Underwater and Multimedia Networks}
\subsubsection{Graph-Based Edge-User Collaborative Caching with Social Attributes}
abstract:Collaborative caching in edge-user architecture hasbeen regarded as one of the most promising technologies torelease the pressure of core networks and reduce the contentdownload delay. However, the caching resources at edge serversand devices are limited, so how to utilize their cache spaceefficiently has become a significant issue. This paper introducesa three-tier caching framework consisting of a macro-cell basestation (MBS), multiple small-cell base stations (SBSs), and userequipments (UEs). In this framework, by combining physicaland social attributes, we propose a directed graph-based edgeusercollaborative caching (DG-EUCC) strategy to minimize thecontent download delay. Specifically, the wireless communicationnetworks between the different types of nodes at the SBSs tierand the UEs tier are simplified to a one-tier directed graph (DG)with social attributes, to realize the simplification of the systemmodel. Further, we design a DG-based collaborative cachingstrategy to minimize the content download delay, where eachnode caches the most popular contents according to the weightedcontent popularity set. Simulation results show that, comparedwith the benchmark strategies, the proposed DG-EUCC strategycan effectively reduce average download delay. For example,compared with the random caching strategy, the average delayof the proposed strategy decreased by 21%.
\subsubsection{Achieving Ultra High Freshness in Real-Time Monitoring and Decision Making with Incremental Decoding}
abstract:Real-time monitoring and remote control of stochastic systems have attracted considerable attention due to their potential in task-oriented communications and industrial Internet of Things (IIoT). How to achieve ultra high-freshness in real-time monitoring and remote control becomes a challenging problem. In this paper, we are interested in the freshness oriented source coding with incremental decoding. This is contrast to conventional source encoding/decoding, in which a random sample is estimated after its entire codeword is received. Incremental decoding, however, allows the real-time estimation of a random sample once a new bit or channel coding block is decoded in the physical layer. Its source codebook is then optimized, based on which we further conceive a real-time decision policy. Our policies minimize the average mean square error (MSE) or decision cost by judiciously designed codebook for source encoding. Numerical results show that the incremental decoding substantially reduces the MSE and decision cost in real-time monitoring.
\subsubsection{Placing Information Boxes to Reduce Power Consumption in Disaster Communications Using DTN}
abstract:In large-scale disasters such as earthquakes, a quick and safe evacuation is necessary. For this purpose, it is important to smoothly share information about damaged roads among evacuees. However, in large-scale disasters, the communication infrastructure might become damaged to the point of being unusable or subject to a loss of power. As alternatives means of communications, various methods using the delay tolerant network (DTN) concept, in which mobile terminals directly communicate without using a communication infrastructure, have been proposed for disaster communication. However, when using the DTN, nearby mobile terminals need to exchange road information many times, so their power consumption will increase. Moreover, when a blackout occurs in a disaster, it may become impossible to charge the mobile battery, so it is important to reduce its power consumption. In this paper, we propose to place information boxes at multiple locations that store the information on road locations that have sustained damage. By exchanging such road information through the information boxes, i.e., uploading and downloading the information at the information boxes, one can expect to dramatically reduce the power consumption of mobile terminals. We also propose a method of determining the placement of the information boxes that considers the estimated number of evacuees passing through and their separation. A numerical evaluation using a multi-agent simulation shows the effectiveness of the proposed method.
\subsubsection{Adaptive F-FFT Demodulation for ICI Mitigation in Differential Underwater Acoustic OFDM Systems}
abstract:This paper addresses the problem of frequency-domain inter-carrier interference (ICI) mitigation for differential orthogonal frequency-division multiplexing (OFDM) systems. The classical fractional fast Fourier transform (F-FFT), adopting the fixed sampling interval, would suffer from the limited accuracy of ICI mitigation and low adaptability in dynamic Doppler spread. To target the above challenges, we propose an adaptive fractional Fourier transform (A-FFT) demodulation method, in which an estimation algorithm based on the coordinate descent approach is designed to compute the fiducial frequency offset without increasing pilots. By means of compensating ICI at fractions of the fiducial frequency offset adapted to the time-varying Doppler shift, the A-FFT has the capability of tracking Doppler fluctuations over the underwater acoustic channels, thus extending the application range of frequency-domain ICI mitigation. Simulation results show that the A-FFT is significantly superior to the existing classical methods, the partial fast Fourier transform (P-FFT) and the F-FFT, for both medium and high Doppler factors and large carrier numbers in terms of the mean squared error (MSE). Numerically, the MSE of the A-FFT is reduced by 39.88%-72.14% compared to that of the F-FFT with the input signal-to-noise ratio ranging from 10 dB to 30 dB at a Doppler factor of \bf{2.5\times 10^{-4}} and a carrier number of 1024, while the P-FFT even cannot work well.
\subsubsection{Study on Characteristics of Metric-aware Multipath Algorithms in Real Heterogeneous Networks}
abstract:Multipath transmission is considered one of the promising solutions to improve wireless resource utilization where there are many kinds of heterogeneous networks around. Most scheduling algorithms rely on real-time network metrics, including delay, packet loss, and arrival rates, and achieve satisfying results in simulation or wired environments. However, the implicit premise of a scheduling algorithm may conflict with the characteristics of real heterogeneous wireless networks, which has been ignored before. This paper analyzes the real network metrics of three Chinese heterogeneous wireless networks under different transmission rates. To make the results more convincing, we conduct experiments in various scenarios, including different locations, different times of the day, different numbers of users, and different motion speeds. Further, we verify the suitability of a typical delay-aware multipath scheduling algorithm, Lowest Round Trip Time, in heterogeneous networks based on the actual data measured above. Finally, we conclude the characteristics of heterogeneous wireless networks, which need to be considered in a well-designed multipath scheduling algorithm.
\subsection{Edge Computing and Fog Computing}
\subsubsection{A Shapley Value-Based Incentive Mechanism in Collaborative Edge Computing}
abstract:In recent years, with the rapid proliferation of smart devices, Mobile Edge Computing (MEC) has been regarded as a promising technique that provides computing services in proximity to end-users. To improve the performance of MEC systems, Collaborative Edge Computing (CEC) is proposed to balance the load among cooperative edge servers. In practice, however, edge servers belong to different MEC service providers (SPs) and they have no incentive to help others. To encourage the cooperation between self-interested SPs, in this paper, we propose a profit-sharing incentive mechanism based on the Shapley value. In addition to the desirable properties such as efficiency and fairness, we also proved that our mechanism induces optimal offloading strategies and provides every SP an incentive to join the coalition. To protect the private information of SPs, we defined an aggregate profit function for each SP and showed that revealing this function is sufficient to calculate the profit allocation. Simulation results demonstrate that the system performance and SPs' revenue are substantially improved under cooperation.
\subsubsection{Task Offloading in UAV Swarm-Based Edge Computing: Grouping and Role Division}
abstract:Due to the outstanding characteristics of unmanned aerial vehicles (UAV), i.e., maneuverability and flexibility, UAV enabled mobile edge computing (MEC) has become a widely attractive research direction. However, single-UAV cannot be qualified for numerous tasks and application scenarios in view of its limited computing capacity, while multi-UAV enabled MEC is still in the initial stage, and most existing work transformed the problem of multi-UAV enabled MEC into multiplied single-UAV. The UAV swarm can make UAVs cooperate intelligently, and accomplish diversified tasks in complex environments at low cost, which is regarded as a promising development direction of UAV technology. Nevertheless, it is inefficient since each UAV node is responsible for both communication and computation, and multi-hop transmission among UAVs may lead to a very high delay. Toward this end, the paper takes the lead in studying the problem of grouping and role division in UAV swarm-based edge computing, and puts forward a grouping and role division algorithm to solve it. Final experimental results corroborate that the complexity of our algorithm is less than that of the traditional algorithm, and role division can maximize the use of communication and computing resources.
\subsubsection{Cooperative Task Allocation in Edge Computing Assisted Vehicular Crowdsensing}
abstract:As a popular scenario of mobile crowdsensing, edge computing assisted vehicular crowdsensing (EVCS) encourages vehicles to participate in sensing data with the equipped devices. Due to the vehicular mobility, vehicles may dynamically enter and leave the coverage area of an edge node, leading to recurrent task allocations that consume excessive communication and computational resources. How to avoid recurring recruitment in task allocation is challenging. In this paper, we propose an optimization framework to facilitate task allocation by utilizing the cooperation between edge nodes. The proposed framework avoids complicated recruitment procedures while maximizing the connection time between the recruited vehicles and the edge node. Due to the NP-hardness of the formulated optimization problem, we design a reinforcement learning based algorithm to solve the problem with high accuracy and efficiency. Simulation results show the effectiveness of our proposed framework.
\subsubsection{Online Auction Based Resource Allocation for Soft-Deadline Tasks in Edge Computing}
abstract:With the development of edge computing (EC), more and more tasks are offloaded to edge servers (ESs). However, faced with a huge number of users offloading tasks to ESs, how to allocate resources reasonably and reduce the response time of the system are problems worth studying. In this paper, we design an online auction algorithm to deal with those two issues at the same time. We first introduce four task value functions to model the sensitivity to the delay of different tasks. Then, we construct a three-layer EC model. Based on it, we define the resources allocation problem as a social welfare (SW) maximization problem, which is NP-hard. To solve this problem, we utilize the master-dual technique to transform it into an online auction problem. Finally, an algorithm considering task classification is proposed, which realizes both resource allocation and latency reduction in a polynomial time. Experiment results show that our approach reduces the scheduling latency by an average of 38% while maintains SW at the same time.
\subsubsection{Cybertwin Assisted Wireless Asynchronous Federated Learning Mechanism for Edge Computing}
abstract:The significant advances in wireless communication together with edge intelligent (EI) technology have facilitated the decentralized edge computing paradigm for data-intensive and delay-sensitive solution on massive Internet of Things (IoT) devices. In this paper, a Cybertwin assisted asynchronous federated learning (AFL) mechanism is proposed for realizing efficient edge computing by taking full advantage of local computation capability under heterogeneous wireless environment. First, Cybertwin is introduced as intermediary communication assistant to coordinate individual model aggregation between the users and the cloud server under AFL training process. Second, for the sake of flexible and effective utilization of communication-computation resources for edge computing, Cybertwin plays the role of intelligent agent to jointly take the local computing and up-link transmission into consideration. A resource optimization problem considering the diversified computing power, varied data size, and available communication bandwidth is formulated and we leverage the block coordinate descent (BCD) method to obtain optimal resource management solution. Extensive simulations are conducted to demonstrate the effectiveness of our proposed Cybertwin assisted AFL mechanism, which can shed further light on the application of data-intensive edge computing paradigm over wireless communication network.
\subsection{Efficient Wireless Systems}
\subsubsection{Communication-Efficient Split Learning Based on Analog Communication and Over the Air Aggregation}
abstract:Split-learning (SL) has recently gained popularity due to its inherent privacy-preserving capabilities and ability to enable collaborative inference for devices with limited computational power. Standard SL algorithms assume an ideal underlying digital communication system and ignore the problem of scarce communication bandwidth. However, for a large number of agents, limited bandwidth resources, and time-varying communication channels, the communication bandwidth can become the bottleneck. To address this challenge, in this work, we propose a novel SL framework to solve the remote inference problem that introduces an additional layer at the agent side and constrains the choices of the weights and the biases to ensure over the air aggregation. Hence, the proposed approach maintains constant communication cost with respect to the number of agents enabling remote inference under limited bandwidth. Numerical results show that our proposed algorithm significantly outperforms the digital implementation in terms of communication-efficiency, especially as the number of agents grows large.
\subsubsection{One for All: Traffic Prediction at Heterogeneous 5G Edge with Data-Efficient Transfer Learning}
abstract:By placing the computing, storage and networking resources close to the end users, distributed edge computing greatly benefits the performance of 5G communication systems. However, as a tradeoff, resources on the edge are usually limited and imbalanced among the heterogeneous edge nodes. To overcome this drawback, this paper proposes a Transfer Learning based Prediction (TLP) framework that allows the edge nodes to share their resources and data in an efficient manner. In particular, the TLP framework focuses on the prediction of the future traffic load, which is a key reference for many automated network functions. To enhance the efficiency of data and bandwidth, TLP first learns a base model on a data-abundant edge node (the source), and then transfers this model (instead of data) to other data-limited nodes (the targets). To achieve a delicate balance between maintaining common features and learning target-specific features, we develop a new transfer learning technique named Similarity-based Elastic Weight Consolidation (SEWC), and integrate it into TLP. Experiments on real-world data illustrate that, compared to the state-of-the-art methods, TLP-SEWC reduces the Mean Absolute Error (MAE) of traffic prediction by up to 57.9%.
\subsubsection{An Efficient Outlier Detection and Classification Clustering-Based Approach for WSN}
abstract:Wireless Sensor Network (WSN) is one of the main components of the Internet of things (IoT) for gathering information and monitoring the environment in a variety of applications (medical, agricultural, manufacturing, militarily, etc.). However, data collected and transferred from sensors to the main station are susceptible to outliers occurring due to sensor nodes itself or to the harsh environment where the sensor nodes are deployed. Thus, it is necessary for the WSN to be able to detect the outliers and take actions in order to ensure network quality of service and to avoid further degradation of the application efficiency. In this paper, we propose a distributed outlier detection and classification algorithm for WSN. Our approach is capable of distinguish between an error due to a faulty sensor and an error due to an interesting event. We take into consideration the spatial-temporal correlation between sensors' data values and between neighbouring sensor nodes. Simulations with both synthetic and real datasets showed that our proposed approach outperforms other techniques by obtaining high detection rate (DR) and low false alarm rate (FAR).
\subsubsection{Autonomous Braking Algorithm for Rear-End Collision via Communication-Efficient Federated Learning}
abstract:Realizing driving safety is the fundamental goal pursued by artificial intelligence (AI)-enabled autonomous driving. However, due to the limited capacity of the vehicle and the limited scenarios involved, the reliability and environmental adaptability of the current single-vehicle intelligence still need to be improved. In addition, driving knowledge only exists locally in each connected autonomous vehicle (CAV) and cannot be effectively reused or shared with other CAVs. To solve the above problems, this paper proposes to use federated learning (FL) to realize the collaboration of CAVs without revealing local data, thereby improving the accuracy of CAVs decision-making and driving safety. First, for the typical rear-end collision scenario, we propose a local decision-making model that comprehensively considers multiple influencing factors to fit the actual traffic environment. Then, we design a federated learning process for knowledge sharing. In particular, we propose a model similarity method to select high-quality local models for upload, thereby reducing communication overhead while improving the accuracy of the global model. Extensive simulations validate the performance of our proposal in reducing communication overhead and improving decision accuracy.
\subsubsection{Load-Balanced Routing for Hybrid Fiber/Wireless Backhaul Networks}
abstract:Dense deployment of small-cell base stations (BSs) requires a backhaul network to efficiently connect the BSs to the core network. In this paper, we focus on a hybrid backhaul architecture where some BSs connect with fiber to the core network and provide mmWave backhaul connections for the rest of the BSs. This architecture brings new challenges, e.g., how to prevent a large amount of traffic from becoming concentrated at certain egress BSs, thereby hurting overall backhaul performance. In this paper, we propose a load-balanced routing algorithm to address this challenge. We first define the concept of load balance factor (LBF) and address the challenge through a hill climbing procedure that attempts to minimize LBF. Results show that the proposed algorithm can distribute the dynamic traffic loads from different BSs nearly optimally among fiber-connected BSs for the simulated settings. We also present a variation of the algorithm that permits trade-offs between routing path length and load balance factor.
\subsection{Federated Learning in Wireless Networks (I)}
\subsubsection{Joint Client Selection and Task Assignment for Multi-Task Federated Learning in MEC Networks}
abstract:In this paper, we investigate the multi-task federated learning in mobile edge computing (MEC) networks where a central server assigns different federated learning tasks to different MEC servers and select feasible clients to participate in federated learning training process. The problem is formulated as a joint client selection and task assignment problem to maximize the total utility of all tasks, subject to the trained model quality and total training latency. Since the above-mentioned problem is NP-Hard, it poses challenges to obtain the optimal solution within polynomial time, the problem is transformed into a many-to-one-to-one 3D matching problem. To further reduce the computation while ensuring the matching stability, we first adopt the spectral clustering algorithm to cluster the clients into multiple client clusters. Then we reformulate the problem as a 3-Partite weighted hypergraph total weight maximization problem. Finally, we propose a greedy and local search (GLS) based algorithm to resolve the problem. Simulation results demonstrate the effectiveness of the proposed algorithm as compared with baseline schemes.
\subsubsection{To Talk or to Work: Delay Efficient Federated Learning over Mobile Edge Devices}
abstract:Federated learning (FL), an emerging distributed machine learning paradigm, in conflux with edge computing is a promising area with novel applications over mobile edge devices. In FL, since mobile devices collaborate to train a model based on their own data under the coordination of a central server by sharing just the model updates, training data is maintained private. However, without the central availability of data, computing nodes need to communicate the model updates often to attain convergence. Hence, the local computation time to create local model updates along with the time taken for transmitting them to and from the server result in a delay in the overall time. Furthermore, unreliable network connections may obstruct an efficient communication of these updates. To address these, in this paper, we propose a delay-efficient FL mechanism that reduces the overall time (consisting of both the computation and communication latencies) and communication rounds required for the model to converge. Exploring the impact of various parameters contributing to delay, we seek to balance the trade-off between wireless communication (to talk) and local computation (to work). We formulate a relation with overall time as an optimization problem and demonstrate the efficacy of our approach through extensive simulations.
\subsubsection{A Joint Communication and Federated Learning Framework for Internet of Things Networks}
abstract:In this paper, a communication efficient Federated learning (FL) framework is proposed for internet of things networks. To reduce the FL transmission delay, a joint learning and resource allocation problem is formulated via optimizing the transmit power of each device, time allocation, and user selection. To solve this problem, its objective function is first converted into a tractable form. Next, a successive convex optimization method is used to solve the simplified optimization problem. Two simulations are conducted using the electromyographic signals for finger movement detection, in which the results demonstrate that the proposed FL framework with the personalized training process is able to provide high performance in detecting single and combined finger movements for distributed users. Specifically, over 98% overall test accuracy is achieved using the proposed FL framework on two benchmark datasets, which surpasses the conventional learning framework by 1.6% and 0.5% on average.
\subsubsection{Incentive Mechanism for AI-Based Mobile Applications with Coded Federated Learning}
abstract:Federated learning (FL) has emerged as a highly-effective distributed learning framework for various AI-based mobile applications. However, in conventional FL, participating mobile users (MUs) may have limited computing resources to train their local data, which leads to degradation of learning quality for the whole FL process. To address this problem, coded FL (codFL) has been recently introduced, allowing MUs to upload part of their coded data to a mobile application provider (MAP) before the learning process. As a result, codFL can not only deal with the MUs' limited computing resources, but also provide more benefits for the MUs to participate in the learning process. Nonetheless, in practice, the MAP and MUs often belong to different parties who unilaterally aim to maximize their individual utility functions. Thus, in this paper, we propose an effective mechanism for the codFL process to incentivize all the participating MUs while improving the learning quality of the MAP. Specifically, we first design a codFL contract optimization problem leveraging a multi-principal one-agent (MPOA) approach in contract theory, under limited computing resources at the MAP and MUs as well as information asymmetry between them. To find the optimal contracts for MUs, we develop an iterative contract algorithm which can produce maximum utilities for all MUs while satisfying all the constraints of the MAP. Numerical results show that our framework can enhance the utilities of MUs up to 113% and system performance in terms of social welfare up to 42% compared with the baseline method.
\subsubsection{Enabling Large-Scale Federated Learning over Wireless Edge Networks}
abstract:Major bottlenecks of large-scale Federated Learning (FL) networks are the high costs for communication and computation. This is due to the fact that most of current FL frameworks only consider a star network topology where all local trained models are aggregated at a single server (e.g., a cloud server). This causes significant overhead at the server when the number of users are huge and local models' sizes are large. This paper proposes a novel edge network architecture which decentralizes the model aggregation process at the server, thereby significantly reducing the aggregation latency of the whole network. In this architecture, we propose a highly-effective in-network computation protocol consisting of two components. First, an in-network aggregation process is designed so that the majority of aggregation computations can be offloaded from cloud server to edge nodes. Second, a joint routing and resource allocation optimization problem is formulated to minimize the aggregation latency for the whole system at every learning round. The problem turns out to be NP-hard, and thus we propose a polynomial time routing algorithm which can achieve near optimal performance with a theoretical bound. Numerical results show that our proposed framework can dramatically reduce the network latency, up to 4.6 times. Furthermore, this framework can significantly decrease cloud's traffic and computing overhead by a factor of K/M, where K is the number of users and M is the number of edge nodes, in comparison with conventional baselines.
\subsection{Federated Learning in Wireless Networks (II)}
\subsubsection{FedVANET: Efficient Federated Learning with Non-IID Data for Vehicular Ad Hoc Networks}
abstract:The vehicular ad hoc networks (VANETs) play a significant role in intelligent transportation systems (ITS). In recent years, federated learning (FL) has been widely used in VANETs to preserve the privacy-sensitive data, such as vehicle locations, drivers' driving patterns, on-board camera data, etc. However, conventional FL faces the challenges of non-independent and identically distributed (Non-IID) data and high communication overheads in VANETs. To address these challenges, we propose a novel FL framework for VANETs, named FedVANET, where a hierarchical inner-cluster FL model and a weighted inter-cluster cycling update algorithm are, respectively, developed. Extensive experiments demonstrate the high efficiency of the FedVANET in inner-cluster communications, effectiveness in handling Non-IID data, and robustness in dynamic VANET topologies.
\subsubsection{Beam Management in Ultra-dense Millimeter Wave Network via Federated Learning}
abstract:Millimeter wave (mmWave) communication is one of the key technologies in 5G and beyond systems to address the tremendous growth in mobile data traffic owing to the abundant spectrum resources. Ultra-dense network deployment is a promising solution to combat the limited coverage, high propagation loss and attenuation of mmWave signals. This study investigates the beam management, with focus on beam configuration of mmWave base stations, in the ultra-dense mmWave network. To fulfill adaptive and intelligent beam management while protecting user privacy, we employ a double deep Q-network under a federated learning to tackle the beam management problem which is formulated to maximize the long-term system throughput. Simulation results demonstrate the performance gain of our proposed scheme.
\subsubsection{A Privacy-preserved D2D Caching Scheme Underpinned by Blockchain-enabled Federated Learning}
abstract:Cache-enabled device-to-device (D2D) communication has been widely deemed as a promising approach to tackle the unprecedented growth of wireless traffic demands. Recently, tremendous efforts have been put into designing an efficient caching policy to provide users better quality of service. However, public concerns of data privacy still remain in D2D cache sharing networks, which thus arises an urgent need for a privacy-preserved caching scheme. In this study, we propose a double-layer blockchain-based federated learning (DBFL) scheme with the aim of minimizing the download latency for all users in a privacy-preserving manner. Specifically, in the sublayer, the devices within the same coverage area run a federated learning (FL) to train the caching scheme model for each area separately without exchange of local data. The model parameters for each area are recorded in sublayer chains with Raft consensus mechanism. Meanwhile, in the main layer, a mainchain based on practical Byzantine fault tolerance (PBFT) mechanism is used to resist faults and attacks, thus securing the reliability of FL updates. Only the reliable area models authorized by the mainchain are utilized to update the global model in the main layer. Numerical results show the convergence, as well as the gain of download latency of the proposed DBFL caching scheme when compared with several traditional schemes.
\subsubsection{On-the-fly Resource-Aware Model Aggregation for Federated Learning in Heterogeneous Edge}
abstract:Edge computing has revolutionized the world of mobile and wireless networks world thanks to its flexible, secure, and performing characteristics. Lately, we have witnessed the increasing use of it to make more performing the deployment of machine learning (ML) techniques such as federated learning (FL). FL was debuted to improve communication efficiency com- pared to conventional distributed machine learning (ML). The original FL assumes a central aggregation server to aggregate locally optimized parameters and might bring reliability and latency issues. In this paper, we conduct an in-depth study of strategies to replace this central server by a flying master that is dynamically selected based on the current participants and/or available resources at every FL round of optimization. Specifically, we compare different metrics to select this flying master and assess consensus algorithms to perform the selection. Our results demonstrate a significant reduction of runtime using our flying master FL framework compared to the original FL from measurements results conducted in our EdgeAI testbed and over real 5G networks using an operational edge testbed.
\subsubsection{Client Selection Approach in Support of Clustered Federated Learning over Wireless Edge Networks}
abstract:Clustered Federated Multitask Learning (CFL) was introduced as an efficient scheme to obtain reliable specialized models when data is imbalanced and distributed in a non-i.i.d. (non-independent and identically distributed) fashion amongst clients. While a similarity measure metric, like the cosine similarity, can be used to endow groups of the client with a specialized model, this process can be arduous as the server should involve all clients in each of the federated learning rounds. Therefore, it is imperative that a subset of clients is selected periodically due to the limited bandwidth and latency constraints at the network edge. To this end, this paper proposes a new client selection algorithm that aims to accelerate the convergence rate for obtaining specialized machine learning models that achieve high test accuracies for all client groups. Specifically, we introduce a client selection approach that leverages the devices' heterogeneity to schedule the clients based on their round latency and exploits the bandwidth reuse for clients that consume more time to update the model. Then, the server performs model averaging and clusters the clients based on predefined thresholds. When a specific cluster reaches a stationary point, the proposed algorithm uses a greedy scheduling algorithm for that group by selecting the clients with less latency to update the model. Extensive experiments show that the proposed approach lowers the training time and accelerates the convergence rate by up to 50% while imbuing each client with a specialized model that is fit for its local data distribution.
\subsection{Intelligent Reflecting Surfaces}
\subsubsection{Hybrid Precoding for Multiple IRS-Assisted mmWave MIMO Communication Exploiting Mixed Timescale CSI}
abstract:Intelligent reflecting surface (IRS) has recently emerged as a potential low-cost solution to reshape the wireless propagation environment for improving the spectral efficiency. In this paper, we consider the hybrid precoding for multiple IRSaided millimeter-wave (mmWave) multiple-input multiple output (MIMO) communication exploiting mixed timescale channel state information (CSI). In particular, the digital precoding matrix at the base station (BS) is optimized by using the instantaneous CSI, while the analog precoding matrix at the BS and the reflection coefficient matrices at the multiples IRSs are optimized by using the statistical CSI. We first formulate the problem to jointly optimize the digital precoding, analog precoding, and the reflection coefficient matrices by using the mixed timescale CSI. Then, an efficient algorithm and a low-complexity algorithm are proposed to solve this problem. Simulation demonstrates that the proposed scheme can significantly improve the spectral efficiency.
\subsubsection{Static Reflecting Surface Based on Population-level Optimization}
abstract:We consider a Static Reflecting Surface (SRS) assisted communication system. Part of a cell served by a Base Station (BS) is blocked from Line-of-Sight (LoS), and an SRS is deployed to assist communication in that target area. The SRS has a high number of reflecting elements, with a static phase shift matrix, optimized offline at installation phase for a user population. To find the beamformer at the BS and phase shift matrix at the SRS, we formulate an optimization problem aimingto maximize the average data rate in the target area assuming LoS communication between SRS and BS, as well as between SRS and the users. A local optimum of the population-level problem is obtained using the interior point method. We furthermore consider a low complexity approach, where we divide the SRS & BS antennas into sub-blocks and the target area into subareas; each sub-block is designed to serve a user at the center of the corresponding subarea. Simulation results show that as compared to a fully dynamic Reconfigurable Intelligent Surface (RIS) of the same size, where there is real-time electronic control of the phase shifter, an SRS loses 30% in performance. Comparing to other SRS approaches, and broadcast approach from the literature, the population based approach provides higher average Spectrum Efficiency (SE), 5% SE and fairness index.
\subsubsection{Placement Optimization and Power Control in Intelligent Reflecting Surface Aided Multiuser System}
abstract:Intelligent reflecting surface (IRS) is a new and revolutionary technology capable of reconfiguring the wireless propagation environment by controlling its massive low-cost passive reflecting elements. Different from prior works that focus on optimizing IRS reflection coefficients or single-IRS placement, we aim to maximize the minimum throughput of a single-cell multiuser system aided by multiple IRSs, by joint multi-IRS placement and power control at the access point (AP), which is a mixed-integer non-convex problem with drastically increased complexity with the number of IRSs/users. To tackle this challenge, a ring-based IRS placement scheme is proposed along with a power control policy that equalizes the users' non-outage probability. An efficient searching algorithm is further proposed to obtain a close-to-optimal solution for arbitrary number of IRSs/rings. Numerical results validate our analysis and show that our proposed scheme significantly outperforms the benchmark schemes without IRS and/or with other power control policies. Moreover, it is shown that the IRSs are preferably deployed near AP for coverage range extension, while with more IRSs, they tend to spread out over the cell to cover more and get closer to target users.
\subsubsection{Multi-Tier Task Offloading with Intelligent Reflecting Surface and Massive MIMO Relay}
abstract:This paper investigates the task offloading problem in a hybrid intelligent reflecting surface (IRS) and massive multiple-input multiple-output (MIMO) relay assisted fog computing system, where multiple task nodes (TNs) offload theircomputational tasks to computing nodes (CNs) nearby massive MIMO relay node (MRN) and fog access node (FAN) via the IRS for execution. By considering the practical imperfect channel state information (CSI) model, we formulate a joint task offloading, IRS phase shift optimization, and power allocation problem to minimize the total energy consumption. We solve the resultant non-convex optimization problem in three steps. First, we solve the IRS phase shift optimization problem with the semidefinite relaxation (SDR) algorithm. Then, we exploit a differential convex (DC) optimization framework to determine the power allocation decision. Given the IRS phase shifts, the computational resources, and the power allocation, we propose an alternating optimization algorithm for finding the jointly optimized results. The simulation results demonstrate the effectiveness of the proposed scheme as compared with other benchmark schemes.
\subsubsection{Real-Time Beam steering in mmWave with Reconfigurable Intelligent Meta-surfaces}
abstract:The control logic of a reconfigurable meta-surface in mmWave is investigated in order to perform real-time beam steering in mobility contexts. When it is necessary to track an object or a person in movement, it is required to change the direction of the signal transmitted/reflected by the meta-surface. To do so, the meta-surface has to be reconfigured to modify the generated radiation pattern. Here a specific meta-surface, working around 78.5 GHz, is adopted, which consists of many unit-cells containing a diode to make its electromagnetic behaviour reconfigurable. In order to enable the system to adaptively compute the new coding schemes, namely to change the states of the meta-surface unit-cells, a machine-learning algorithm is adopted. In particular, we propose a Genetic Algorithm, integrate it in the architecture of our meta-surface and evaluate the performance. Results show that a good accuracy and a good convergence time are achieved for real-time beam steering functionality and the capacity of the system equipped with RIM is increased.
\subsection{Inter-networking and Software Defined Wireless Networks}
\subsubsection{Combinatorial User Association in Heterogeneous Wireless Networks via a Statistical Representation}
abstract:Future heterogeneous wireless networks (HetNets) should provide massive connectivity for a large number of devices. The problem of user association and multiple access management in such networks is of paramount importance. This paper formulates a statistical equivalent of the user association problem using users' probability density. Hence, this paper is a bridge between the dynamical user association used in relaxation, game theory, reinforcement learning approaches, and the static user association considered in analytical stochastic geometry methods. To this aim, a novel representation independent of the number of users is derived from the combinatorial user association formulation. Interestingly, we show that for fair user association, the statistical representation is a multi-objective optimization. The first objective is to maximize the network throughput with fairness consideration, and the second objective is to optimize the load balancing in terms of Shannon entropy. Based on this representation, we propose an algorithm for optimizing user association using the first-order derivative formula. Consequently, we propose a method that can optimize individual base stations' bias factors inside each tier of a HetNet. Numerical results show that the statistical representation closely tracks the stochastic behavior of the dynamical problem and the proposed optimization method improves the outage probability and load balancing in the network.
\subsubsection{Dynamic Routing for Software-Defined LEO Satellite Networks based on ISL Attributes}
abstract:The software-defined Low Earth Orbit (LEO) satellite network can effectively improve the flexibility of the inter-satellite networking. However, the impact of Inter-Satellite Link (ISL) attributes on the network topology has not been well investigated in existed works, which leads to the loss of topology information and the unreliability of routing path. In this paper, we propose a dynamic routing algorithm based on ISL attributes to improve the adaptability and reliability of routing in LEO satellite network. Firstly, we construct the utility function of ISL attributes to quantify the link utility. Then, the maximum deviation algorithm is introduced to adaptively calculate the weight of attribute parameters in link utility function, which can quantify the impact of each attribute on link quality. Finally, the mathematical model of path utility is established, the routing path with highest path utility is selected as the optimal routing path. The simulation results show that the proposed scheme can optimize the routing path and improve the network performance of packet drop rate, end-to-end delay and throughput.
\subsubsection{Load Balancing and Handover Optimization in Multi-band Networks using Deep Reinforcement Learning}
abstract:Cellular networks continue to trend rapidly towards more bands and carrier frequencies, along with higher base station density, requiring complex decisions to be made when associating a mobile user with a band and cell. This paper develops a novel approach to optimizing frequency band and cell selection while taking into account user mobility and handovers. This is a complex problem because of the uncertain link failure events, handover related overheads, and the significant difference in the propagation characteristics between different frequency bands. The network dynamics due to user mobility are modeled as a Markov decision process, and we develop a recurrent Q-learning framework to exploit the relationship between user trajectories and the history of SINR measurements. The effective cell boundaries are therefore based on user trajectories and velocities rather than just position and signal strength. Detailed system-level simulations show that the proposed learning-based approach improves the throughput of the edge users by 54% and the median throughput by 34% compared to traditional SINR-based association and achieves a superior rate/coverage tradeoff (quantified as sum-log-rate) compared to SINR or signal-strength-based associations.
\subsubsection{On Velocity-based Association Policies for Multi-tier 5G Wireless Networks}
abstract:Mobility is a key challenge for beam management in 5G cellular networks due to the overhead incurred at beam switching and base station (BS) handover events. This paper focuses on a network that has a multi-tier structure with two types of BSs operating in the same frequency bands, namely macro BSs that are sparser but with higher transmit power, and micro BSs that are denser and with lower transmit power. We propose a downlink user association policy which is a function of the user mobility. Typically, high mobility users should associate with macro BSs so as to incur less beam switching overhead, whereas low mobility ones should be associated with micro BSs. The main contribution of the paper is a formalization of the optimal threshold association policy, when the optimality is understood with respect to the effective Shannon rate. The analysis is based on stochastic geometry and on an exact representation of the effective Shannon rate of the typical user in this beamforming multi-tier context. Two models are discussed. The simplest one focuses on a single-user optimization problem. We also discuss a more realistic model with bandwidth sharing between all users in the cell. Finally, we identify the mobility and user-density patterns where the velocity-based threshold association policy outperforms the classical best mean power association policy.
\subsubsection{Maximizing the Connectivity of Wireless Network Slicing Enabled Industrial Internet-of-Things}
abstract:The emergence of 5G brings unprecedented possibilities for deploying the anticipated Industrial Internet of Things (IIoT). To achieve high density connectivity with multiple services in 5G empowered IIoT, we consider the non-orthogonal network slicing in this work. In particular, we jointly utilize network slicing to incorporate two different types of services and exploit non-orthogonal multiple access (NOMA) to maximize the number of total devices that can be accessed to the system. We formulate the connectivity maximization problem as a mixed-integer nonlinear programming (MINLP) by jointly optimizing the transmit power and device-subcarrier association. To tackle the intractable MINLP, we first transform it into a mixed-integer linear programming (MILP) and then reduce the MILP by devising a simple but effective transmit power allocation scheme. Thereafter, we propose a low-complexity best-effort pairing (BEP) algorithm to solve the reduced MILP. By comprehensive simulations, we find that our proposed BEP significantly outperforms the benchmark schemes.
\subsection{Machine Learning and Emerging Technologies}
\subsubsection{Strategies in Covert Communication with Imperfect Channel State Information}
abstract:This paper considers the problem of covert communication over a multiple-input single-output channel (MISO) in the case of the imperfect channel state information. The transmitter wants to communication with the receiver while theeavesdropper detects the existence of the transmission. We first study the scenario that the accurate channel state information (CSI) is known and show that the condition achieving the positive rate by the beamforming vector. However, because of the non-cooperation between the transmitter and the eavesdropper, we consider the CSI is imperfect. There are estimation errors for the transmitter. When the positive rate can not be achieved, the objective is to maximize the communication rate. We discuss the deterministic bounded CSI errors for the covert communication. Robust beamforming and the transmitter power is investigated, which can be transformed as a second order cone programming (SCOP) problem. We consider the line-of-sight (LoS) channel and the linear array antenna model. Comparing with the perfect CSI, the results show that the strategy of the transmitter is more conservative under the imperfect CSI. The capacity declines by the estimation errors.
\subsubsection{ECHO: Enhanced Conditional Handover boosted by Trajectory Prediction}
abstract:Conditional handover (CHO) has been introduced in 5G to improve mobility robustness, namely, to reduce the number of handover failures by preparing target Base Stations (BSs) in advance and allowing the user to decide when to make a handover. This algorithm constantly prepares and releases BSs, thereby adapting to the fast changing radio condition. A user might make a handover to a distant BS that has a favorable channel only for a short time due to signal fluctuations. This increases the handover rate and might result in a Radio Link Failure (RLF) afterwards. Moreover, the constant preparation and release of BSs leads to an increased exchange of control messages between the user, the serving BS and all target BSs. Hence, there is a need to carefully select the target BSs. Therefore, we propose the Enhanced CHO (ECHO) scheme that uses trajectory prediction to prepare the BSs along the user's path. To achieve this, we also propose a Sequence to Sequence (Seq2Seq) mobility prediction model. ECHO with only one prepared BS (ECHO-1) outperforms CHO with three prepared BSs. ECHO-1 reduces the handover rate by 23 percent and the RLF rate by 77 percent, while also reducing the number of control messages in the network by 69 percent.
\subsubsection{R-Learning-Based Admission Control for Service Federation in Multi-domain 5G Networks}
abstract:Network service federation in 5G/B5G networks enables service providers to extend service offering by collaborating with peering providers. Realizing this vision requires interoperability among providers towards end-to-end service orchestration across multiple administrative domains. Smart admission control is fundamental to make such extended offering profitable. Without prior knowledge of service requests, the admission controller (AC) either determines the domain to deploy the demand or rejects it to maximize the long-term average profit. In this paper, we first obtain the optimal AC policy by formulating the problem as a Markov decision process, which is solved through the policy iteration method. This provides the theoretical performance bound under the assumption of known arrival and departure rates of demands. Then, for practical solutions to be deployed in real systems, where the rates are not known, we apply the Q-Learning and R-Learning algorithms to approximate the optimal policy. The extensive simulation results show that learning approaches outperform the greedy policy and are capable of getting close to optimal performance. More specifically, R-learning always outperformed the rest of practical solutions and achieved an optimality gap of 3-5% independent of the system configuration, while Q-Learning showed lower performance and depended on discount factor tuning.
\subsubsection{Learning-based Cache Placement and Content Delivery for Satellite-Terrestrial Integrated Networks}
abstract:To support the explosive content demands from multifarious services and applications, cache-enabled satellite-terrestrial integrated networks (STINs) are envisioned as a key enabler to reduce the content delivery delay and alleviate the backhaul pressure. In this paper, we investigate the joint optimization of cache placement and content delivery in the STIN to minimize the long-term overall content delivery delay. Considering that cache placement and content delivery are interrelated and affected by network dynamics in terms of satellite movement and random content requests, the joint optimization problem is formulated as a sequential decision making problem by leveraging a Markov decision process. We propose a hierarchical deep Q learning (HDQL) algorithm by leveraging two independent deep neural networks to learn the cache placement and content delivery policies with small action space and low time complexity. Simulation results demonstrate that the proposed HDQL algorithm outperforms the benchmark algorithms in terms of content delivery delay in the STINs.
\subsubsection{Power Control Based on DRL Algorithm for D2D-Enabled Networks}
abstract:The problem of power control in the uplink network of cellular users communicating with Device-to-Device (D2D) is mainly studied. Since cellular users and D2D users share spectrum resources, several serious interference will be caused by them. Reasonable measures are taken to control the interference caused by the sharing spectrum resources, otherwise that influences the quality of service (QoS) of cellular users. The reduction of entire system interference can be achieved by power control, so a method of power control based deep reinforcement learning (DRL), namely Asynchronous Advantage Actor Critic (A3C) Algorithm, is proposed. At the same time, the spectrum resources utilization of the system is improved and QoS of cellular users is guaranteed. The simulation results prove rationality of the proposed algorithm, and have better convergence performance than the traditional DRL algorithm.
\subsection{Performance Evaluation and LTE Networks}
\subsubsection{On the Challenges and Performance of Cooperative Communication in 5G and B5G Systems}
abstract:URLLC is the key features of 5G enabling extensive V2X applications and control of complex industrial process via wireless communication systems. In order to ensure the highest reliability requirements with low latency communication, the fluctuating characteristics of a wireless link must not become a limiting factor for the use case. Cooperative communication approaches eliminate this problem by exploiting additional links via relay nodes to transmit data redundantly on different routes in the network to the destination node. We present a suitable multi-path scheme for integration in 5G and B5G systems. The theoretical analysis shows nearly one order of magnitude improvement of reliability for each additional route with the proposed architecture. Furthermore, we demonstrate a significant gain in terms of reliability for the overall link even for highly disrupted relay paths. Using experimental vehicle platooning measurements in real-world traffic based on LTE and IEEE802.11p, we confirm the conceptual advantage of our multi-path approach.
\subsubsection{A Comprehensive Analysis on Multicast and Unicast Performance and Selection}
abstract:With the need to serve multiple users intendedfor the same content, especially in mission-critical applications,multicast has long been studied with evolving standards. Comparedwith its counterpart unicast, multicast has an apparentadvantage of sending one copy instead of multiple copies.However, in Long Term Evolution (LTE) Multicast-BroadcastSingle-Frequency Network (MBSFN), multicast does not supportMultiple Input Multiple Output (MIMO) technology, which isone major technology that improves unicast performance significantly.Multicast also differs from unicast in other aspects thathave major performance impacts, such as constructive signalsand significant interference reduction, no retransmissions, lessavailable subframes, extended cyclic prefix, and denser referencesignals, to name a few. While almost all existing work focuseson a single factor and few addresses MIMO, in this paperwe study multicast and unicast in detail with all these factorsincluded, together with their integrated impact on performance.Profound analysis reveals that, contrary to what is commonlyassumed in existing studies, multicast and unicast would notshare the same modulation and coding scheme, but rather differsignificantly in how efficient they use resources. In addition, thebalance among various factors mentioned above leads to a switchpoint where multicast or unicast outperforms, and the switchpoint changes upon system configurations and the performancemetric of interest. Given that multicast configuration is semistaticin LTE, the results provide insightful guidelines in unicastor multicast deployment in serving user traffic. The work canalso be easily extended to other Single-Frequency Network (SFN)based multicast technologies.
\subsubsection{C-V2X (LTE-V2X) Performance Enhancement Through SAE J3161/1 Probabilistic One-Shot Transmissions}
abstract:This paper presents an in-depth performance comparison between several C-V2X (LTE-V2X) configurations in congested highway scenarios in 10 MHz and 20 MHz channels. Specifically, we focus on the inter-packet gap (IPG) performance, which is a key metric used in several V2X related studies. IPG measures the delay between two successfully received transmissions from a particular device. The results show that significant performance gains in terms of IPG can be achieved using the probabilistic one-shot transmission mechanism specified in SAE J3161/1 standard, which dictates that vehicles randomly skip their reserved Semi-Persistent Scheduling (SPS) transmissions and send one-shot transmissions instead. Furthermore, we conclude that this IPG gain with J3161/1 probabilistic one-shot transmissions can be achieved without any significant impact in terms of packet reception rate (PRR) performance.
\subsubsection{Comparison of performance- and cost-optimal functional splits in 5G and beyond}
abstract:Centralization in 5G radio access networks brings two main benefits: reducing cost and improving performance. Although an ideal, fully-centralized architecture would provide minimum cost and maximum performance, actual deployments cannot simultaneously optimize both. Previous research focuses on how to select the functional split of a 5G network to either maximize performance or minimize cost on partially-centralized architectures, without exploring which approach is the most appropriate. In this work, we investigate the trade-off between cost and performance of both approaches, in order to figure out which one is more adequate for real network operators. We provide a comprehensive study under a wide range of network conditions and show that, in general, a performance-maximizing approach is more likely to produce a higher net revenue.
\subsubsection{A Hierarchical BLE Mesh Network for IoT and Performance Analysis}
abstract:Bluetooth Low Energy (BLE) mesh which is the latest and the most innovative network technology has deeply changed the connection mode among massive BLE devices in Internet of Things (IoT). The study on the architecture and networking is quite necessary for the development of BLE Mesh. In IoT, the BLE devices are often used for voice, audio and data services. To satisfy the demands of diverse services and the mobility of the nodes, a hierarchical BLE mesh network (HBMN) architecture is proposed consisting of Hub Layer, Mesh Layer and User Layer. The HBMN architecture combines the star-network and mesh topology. Besides, the networking process of HBMN from the unprovisioned devices is also investigated in this paper. Moreover, two important performance measures have been analyzed and derived including networking time and single-hop delay. Finally, the access delay of one node is measured on the nRF52832 hardware platform. In addition, the networking time and single-hop delay of HBMN are evaluated and compared with the traditional BLE scatternet. The results show that the average single-hop delay of the BLE scatternet is 1.74 times as long as HBMN averagely and HBMN has a superior performance on the networking time when the network has a large number of devices.
\subsection{Pervasive Systems and Cellular Networks}
\subsubsection{Collaborative D2D Pairing in Cache-Enabled Underlay Cellular Networks}
abstract:In this paper, we propose a collaborative smart solution for online traffic offloading among device-to-device (D2D) users underlying a cellular network. Specifically, we investigate the distributed pairing problem between requesting users and caching devices in their vicinity. Given that this problem is NP-hard, we propose a novel multi-agent reinforcement learning approach based on QMIX algorithm, where each requesting user is an agent capable of deciding to which cache device to pair, while respecting the quality-of-service of cellular users. Through simulations, we show the efficiency of the proposed algorithm in achieving D2D pairing. Finally, the impact of several parameters, such as the size of the network, size of files library, and communication requirements, is investigated.
\subsubsection{DCCP: Deep Convolutional Neural Networks for Cellular Network Positioning}
abstract:Although location awareness is prevalent outdoors due to the GPS, we get confused and disoriented in many blocked environments such as in urban canyons and under multi-level flyovers. A straightforward solution is to employ cellular signals for positioning, but the cellular signatures are always sparse and uneven in vast region, and vary among different devices and postures. In this paper, we propose DCCP, a novel cellular network positioning approach that transforms the localization problem into a corresponding object recognition task in geographic space. Specially, we elicit the receptive region of each cellular station via crowdsourced user queries, and exploit neighbour base stations to derive a multi-dimensional feature map. We also devise a CNN model to learn local correlations among nearby map grids, and employ it for cellular positioning. Extensive experiments on two real-world traffic datasets from the DiDi platform have demonstrated our effectiveness compared with the state-of-the-art. This is the first approach to use only user queries instead of RF signatures for cellular network positioning, and our system meets requirements of the E911.
\subsubsection{Joint Pushing, Pricing, and Recommendation for Cache-enabled Radio Access Networks}
abstract:Proactive pushing can exploit the spectrum underutilized during the off-peak time to push popular content files, thereby significantly improving the spectrum efficiency. Moreover, in a communication system that the virtual network operator (VNO) has to buy spectrum from the base station to conduct file transmission, proactive pushing has been recognized as a promising technology to improve the income of the VNO. However, the appropriate pushing schemes and the achievable income of the VNO are unclear yet. In this paper, joint pushing, pricing, and recommendation (JPR) schemes are presented for cache-enabled radio access networks. We aim to investigate recommendation-based pushing policy to maximize the average income of the VNO. We establish a Markov chain model, which derives the average income of the VNO. Based on this, we formulate an optimization problem to achieve the maximum average income. We further convert the optimization problem into an equivalent linear programming problem. Moreover, a greedy algorithm is applied to solve the problem with lower computational complexity. Finally, simulation results show the significant income gains that can be achieved by JPR schemes compared with the system without the JPR schemes.
\subsubsection{On using Deep Reinforcement Learning to dynamically derive 5G New Radio TDD pattern}
abstract:The deployment of 5G and 6G is highly motivated by the emerging network services that demand more bandwidth and very low latency. Besides, these services are shifting from dominant Downlink (DL) Traffic to a more equilibrate DL/UpLink (UL) and dominant UL traffic for specific emerging services. One option to accommodate this new behavior is to use Time Duplex Division (TDD), where the radio frame is shared between UL and DL time slots, namely UL/DL pattern. While 4G TDD has a fixed number of configurations that cannot be updated on runtime, 5GNR allows complete flexibility to define the UL/DL pattern. Therefore, 5G base stations can dynamically change the pattern to adapt to the type of traffic (i.e., UL or DL). However, the 5G standard does not specify algorithms or solutions to derive the UL/DL pattern. To fill this gap, we propose a Deep Reinforcement Learning (DRL) that adds intelligence to the base station to self-adapt to the traffic pattern of the cell type. The proposed DRL algorithm monitors UL and DL buffers at the5G base station to derive the UL/DL pattern that is optimal to the current traffic configuration. The proposed solution delivers the optimal configuration in a timely and efficient manner. Simulation results demonstrated the efficiency of the proposed algorithm to avoid buffer overflow and its ability to ensure thegenerality by reacting to traffic pattern changes.
\subsubsection{A Neural Network based Power Allocation Algorithm for D2D Communication in Cellular Networks}
abstract:This paper studies the power allocation problem in device-to-device (D2D) communications underlaying cellular networks. A Q-learning based distributed power allocation (Q-PA) algorithm is first proposed, which attempts to obtain optimal power allocation through iterative updating of Q-value tables. Based on the Q-PA algorithm, a neural network (NN) based distributed power allocation (N-PA) algorithm is further proposed, in which training data obtained using the Q-PA algorithm are used to train an NN model, and the trained NN model is used to perform power allocation for D2D users. Simulation results show that both the Q-PA algorithm and the N-PA algorithm can improve the system throughput. The N-PA algorithm has similar system throughput performance but is superior to the Q-PA algorithm in terms of the time cost for power allocation.
\subsection{Resource Allocation and Channel Management}
\subsubsection{Distributed DRL-based Resource Allocation for Multicast D2D Communications}
abstract:Device-to-device (D2D) communication is one of the promising solutions to improve spectrum efficiency and alleviate the mobile traffic explosion. However, interference mitigation and resource allocation in the underlying cellular network is a challenging task. In this paper, we propose a distributed deep reinforcement learning (DRL) based scheme to solve the interference mitigation and resource allocation problem. According to the channel status, each cellular user (CU) and D2D transmitter (D2D TX) will determine the appropriate reused channel andtransmit power to maximize the system throughput. We propose a distributed DRL scheme and integrate two hotbooting algorithms into the scheme to improve the system throughput at the early stage of training. Simulation results show that the proposed distributed DRL with hotbooting outperforms the baselines regarding running time, message overhead, and throughput.
\subsubsection{Volatility-Aware Channel Sensing with Commodity 802.11 Hardware}
abstract:Information about channel quality is essential for channel selection and channel hopping in wireless ad-hoc networks. In this paper, we present a sophisticated channel quality metric, its implementation on commodity 802.11 hardware, and results of real experiments in our testbed. The metric has the following properties. First, collection of channel busy times as raw data exploits customary hardware mechanisms, yielding accurate values. Second, the accuracy of raw data is further improved by determining and subtracting the effect of internal traffic of overlapping channels. Third, corrected raw data are aggregated, making the metric more stable. Fourth, an exponential weighted moving average is applied, providing for adaptivity. Fifth and finally, several candidates to measure the volatility of raw data in a moving window regarding downward fluctuations are defined and applied, yielding channel quality metrics that are more conservative without being overly pessimistic. We have implemented each of these steps and present experimental results.
\subsubsection{Dynamic switch between load based and frame based channel access mechanisms in unlicensed spectrum}
abstract:In unlicensed spectrum, a 5G device is required to access to a channel by using load based equipment (LBE) where it does channel sensing whenever it has data to transmit or frame based equipment (FBE) where it only does channel sensing per fixed period. The devices using LBE and FBE can coexist in the 5G network. Therefore, this paper provides a Markov chain model to analyze the system where the LBE devices and the FBE devices coexist. Subsequently, based on channel access time and transmission probability from the Markov chain model, we propose that the devices are able to switch dynamically from FBE to LBE to serve data with high priority such as Ultra-reliable low-latency communication or data with high arrival rate and from LBE to FBE to serve data with low priority such as Enhanced mobile broadband or data with low arrival rate. The numerical results show the benefits of the dynamic switch between LBE and FBE in reducing channel access time for high priority data and energy consumption for low priority data.
\subsubsection{How Often Do We Need to Estimate Wireless Channels in Massive MIMO with Channel Aging?}
abstract:In massive multiple-input-multiple-output (MIMO) systems, wireless channels are estimated at a fixed and short time interval for all users, which causes redundancy since most of users in practice might have a considerably larger coherence time than the prescribed time interval of estimation, and consequently wastes considerable signaling resources on channel acquisition. In this paper, we propose a novel channel estimation scheme for time-division-duplex massive MIMO systems, which can fully exploit the redundancy by exploring the temporal channel correlation underlying the channel aging effect. We also derive a rigorous lower bound on the achievable spectral efficiency, and maximize the lower bound to determine the optimal time interval of channel estimation. Numerical results show that the proposed estimation scheme can offer great spectral efficiency gains over the conventional one, and provide insights on how to put the proposed scheme into practice.
\subsubsection{OMP-Based Channel Estimation without Prior Information for Underwater Acoustic OFDM Systems}
abstract:A crucial prerequisite for orthogonal matching pursuit (OMP), a widely-used channel estimation method in underwater acoustic (UWA) orthogonal frequency division multiplexing (OFDM) communication systems, is the determination of a termination condition. However, the appropriate condition, which is commonly considered equal to the physical sparsity of the UWA channel, actually dramatically varies with the suffered noise, thus possibly leading to extremely unstable estimation performance. Existing OMP-based algorithms attempt to solve this problem by elaborately adjusting iteration numbers to balance the proportion of genuine channel taps and noise in the reconstructed signal based on noise levels, which inevitably increases the dependency on the prior information, i.e., signal-to-noise ratio (SNR). In order to overcome this challenge, an intuitive idea is eliminating the influence of noise to restore the originally sparse signal before implementing the standard OMP, naturally avoiding the variation of termination conditions. Considering the powerful ability of deep learning, we imitate and elegantly modify the feed-forward denoising convolution neural network (DnCNN), one of the most typical neural networks for image denoising, to develop our prior-information-free denoising OMP (DnOMP) algorithm with a constant iteration number. Simulation results validate that, compared to the standard OMP with the dynamic termination condition, the DnOMP can reduce the normalized mean square error (NMSE) by 39.47%.
\subsection{Resource Management in 5G networks}
\subsubsection{Virtual User Emulation and Resource Allocation Designs for 5G Mobile Wireless Networks}
abstract:The mobile wireless communication system with high-speed massive transmissions attracts immense attention in the fifth-generation (5G) new radio (NR) networks. However, it provokes difficulties to provide high-performance services for massive connections due to inter-carrier interference (ICI) caused by mobility factors.Furthermore, under the limited number of hardware antennas, it becomes essential to develop an effective testbed that is capable of emulating and evaluating an excessive number of users. In this paper, we propose a virtual user equipment (VUE) emulation system with a VUE generator emulating massive VUE transmissions under limited number of antennas. The precoding scheme is designed to transform the virtual channels of mobile VUEs to the realistic channels of antennas of the VUE generator. With the consideration of channel transformation estimation error and ICI, we propose the antenna selection and power/sub-carrier allocation (APSA) scheme aiming to maximize the achievable system sum rate. Simulation results demonstrate that the performance is influenced by VUE positions and channel errors in the designed VUE emulation system. Moreover, the proposed APSA scheme outperforms other methods in terms of quality-of-service (QoS) outage and sum rate.
\subsubsection{Resource allocation for Public Safety Users in the 5G Cellular Network}
abstract:Ensuring communication for Public Safety Users(PSUs) in all events, and specifically in disaster situations, isone of the main challenges of Cellular Networks (CN). Varioustypes of communications, such as in-band and out-bandcommunications, can be used to integrate the Public SafetyNetwork (PSN) into the CN. In this paper, we focus on in-bandoverlay Device-to-Device (D2D) communication, which effectivelyreduces the interference caused by Cellular Users (CUs), andensures the availability of Resource Blocks (RBs) for PSUs atany time and for different events. Furthermore, by using aNon-Orthogonal Multiple Access (NOMA) based system and theParticle Swarm Optimization (PSO) algorithm, the user sum-throughputand the resource wastage problem are improved. Ourgoal is to provide the necessary resources to PSUs and, at thesame time, to maximize the use of these resources. Compared tothe traditional Orthogonal Frequency-Division Multiple Access(OFDMA) system, the simulation results show the efficiency ofour proposed PSO-based NOMA system in terms of user sum-throughput,and also show that the relation between throughputand fairness among users is a requirement-dependent tradeoff,where we can achieve optimal fairness by decreasing the totalthroughput.
\subsubsection{Dynamic Resource Allocation for SDN-based Virtual Fog-RAN 5G-and-Beyond Networks}
abstract:Software-defined networking (SDN)-based virtual Fog computing radio access network (Fog-RAN) architecture is known as a potential solution to cope with massive traffic loads in 5G and beyond networks. Dynamic radio and computational resource allocation is needed to handle the fluctuating traffic loads, aiming to reduce power consumption and enhancing user satisfaction. In this paper, we propose a dynamic resource allocation for a SDN-based virtual Fog-RAN. We formulate a mixed-integer non-linear problem to minimize the network power consumption by optimizing the user equipment (UE) - radio units (RUs) association, the physical resource block (PRB) allocation, and the RU power allocation, according to the real-time traffic loads. Then, exploiting the obtained results in the previous problem, we formulate a multiple knapsack problem to optimize the assignment between the active RUs and the virtual baseband units (vBBUs). Finally, we perform a simulation study to analyse the impact of the proposed resource allocation on network power consumption, vBBUs resources savings, and user satisfaction so to validate the performance gains achieved.
\subsubsection{Decentralized Cooperative Resource Allocation with Reliability at Four Nines}
abstract:Decentralized cooperative resource allocation schemes for robotic swarms represents an alternative to infrastructure-based communications across different commercial, industrial and environmental protection use cases. The cooperative communication schemes, device sequential and group scheduling in [1], have shown superior performance in comparison to 5G NR sidelink mode 2, but have also shown performance issues due to signaling overhead and signalinginduced failures. In this paper we introduce different techniques that reduce the failure probability of data packet transmissions and the packet inter-reception (PIR) time. We evaluate two techniques, respectively, of incremental redundancy using hybrid automatic repeat request and link adaptation by aggregation, as well as their combination for our decentralized cooperative resource allocation schemes and sidelink mode 2. Our results show that the introduced enhancements, allow to double the amount of supported swarm members while achieving fournines reliability when compared to the case where the same enhancements are applied to the sidelink mode 2.
\subsubsection{A framework for joint admission control, resource allocation and pricing for network slicing in 5G}
abstract:The analysis of the techno-economic interactions among the mobile operators and tenants (slices) in the context of 5G network slicing has lately received growing interest by the research community. In particular the problems of admission control, resource allocation/scheduling and pricing for network slicing are not trivial as they affect the profits of the involved stakeholders such as the operator (slice provider) and the network slice owners (tenants or service providers) and therefore the viability of the slice market. Since such problems are entwined, we propose here a novel optimization framework that jointly addresses admission control, resource allocation and pricing. In the proposed framework, the operator owns a fixed amount of resources whereas each slice is characterized by a stochastic demand and utility function reflecting its Quality of Service (QoS) requirements. We have considered two types of slices, one with low traffic profile and deterministic QoS, representing Ultra Reliable Low Latency Communications (URLLC), and one with higher traffic profile and statistical QoS, representing enhanced Mobile BroadBand (eMBB). We devise several models based on whether we allow statistical multiplexing and whether we make use of different prices for different types of slices. We also evaluate the impact of the description of traffic in an aggregate way compared to a detailed one. The outcomes of all variants are analyzed through some numerical experiments which enable us to assess the main flavors of each model.
\subsection{Scheduling, NOMA and reliable networks}
\subsubsection{Improved Whale Optimization Algorithm based Resource Scheduling in NOMA THz Networks}
abstract:Terahertz (THz) technology and non-orthogonal multiple access (NOMA) technology have huge potential to enhance the spectrum efficiency of wireless communications. This paper aims to study the resource scheduling problem by maximizing energy efficiency (EE) of NOMA two-tier heterogeneous network in THz frequency band, taking a full account of the influence of subchannel and power allocation on the downlink of THZ-NOMA network. In order to achieve the research goal better, it's the first time that a subchannel allocation method and power allocation scheme based on improved Whale Optimization Algorithm (WOA) are proposed in this system. The simulation results indicate that, compared with the existing methods, the proposed scheme has faster convergence speed and has certain advantages in performance.
\subsubsection{Distributed Joint Power and Rate Control for NOMA/OFDMA in 5G and Beyond}
abstract:In this paper, we study the problem of minimizing the uplink aggregate transmit power subject to the users' minimum data rate and peak power constraint on each sub-channel for multi-cell wireless networks. To address this problem, a distributed joint power and rate control algorithm called JPRC is proposed, which is applicable to both non-orthogonal frequency-division multiple access (NOMA) and orthogonal frequency-division multiple access (OFDMA) schemes. Employing JPRC, each user updates its transmit power using only local information. Simulation results illustrate that the JPRC algorithm can reach a performance close to that obtained by the optimal solution via exhaustive search, with the NOMA scheme achieving a 42% improvement on the aggregate transmit power over the OFDMA counterpart. It is also shown that the JPRC algorithm can outperform existing distributed power control algorithms.
\subsubsection{Recursive Periodicity Shifting for Semi-Persistent Scheduling of Time-Sensitive Communication in 5G}
abstract:Various legacy and emerging industrial control applications create the requirement of periodic and time-sensitive communication (TSC) for 5G/6G networks. State-of-the-art semi-persistent scheduling (SPS) techniques fall short of meeting the requirements of this type of critical traffic due to periodicity misalignment between assignments and arriving packets that leads to significant waiting delays. To tackle this challenge, we develop a novel recursive periodicity shifting (RPS)-SPS scheme that provides optimal scheduling policy by recursively aligning the period of assignments until the timing mismatch is minimized. RPS can be realized in 5G wireless networks with minimal modifications to the scheduling framework. Performance evaluation shows the effectiveness of the proposed scheme in terms of minimizing misalignment delay with arbitrary traffic periodicity.
\subsubsection{Frame-Level Video Caching and Transmission Scheduling via Stochastic Learning}
abstract:To meet the ever-increasing demand for mobile video services, one of the effective solutions is caching some popular videos in edge nodes. In this paper, we propose an online stochastic learning algorithm with two time scales for joint caching and transmission optimization in the video frame level. To overcome the drift distortion caused by the dependency among video frames, the transmission process is formulated as an infinite horizon Markov decision process (MDP). We derive the equivalent Bellman equation and design the online value iteration algorithm via stochastic approximation for transmission. Due to the lack of the expression between the system performance and the caching policy, we design a gradient-free stochastic optimization algorithm to update the caching policy. Furthermore, we prove that the combined online stochastic learning algorithm almost surely converges. Finally, simulation results show that our proposed algorithm achieves better performance than conventional caching algorithms.
\subsubsection{Asymptotic Analysis of the Reliability-Latency Tradeoff for URLLC in the High SNR Regime}
abstract:Ultra-Reliable and Low-Latency Communications (URLLC) has attracted considerable attention because of its potential applications in factory automation, automated driving, and telesurgery anticipated for the era of the sixth-Generation (6G) networks. In URLLC with random channel gains and a hard delay constraint, the scheduling of backlogged queues and finite blocklength coding in the physical layer make it rather challenging to specify its performance limit. In this paper, we focus our attention on the asymptotic cross-layer analysis of URLLC when the Signal-to-Noise Ratio (SNR) is sufficiently high. More specifically, we find that a fundamental tradeoff exists between the latency and error probability in the high SNR regime, which is characterized by a gain conservation equation. The main result of this work reveals that the sum of our defined real-time gain and reliability gain is equal to one under the optimal scheduling policy. Numerical simulations are also exploited to validate that the derived gain conservation equation holds even with bounded random arrival.
\subsection{UAV, UAV-Assisted Networks}
\subsubsection{Joint Path Planning of Truck and Drones for Mobile Crowdsensing: Model and Algorithm}
abstract:The utilization of drones (also known as unmanned aerial vehicles) can largely improve the performance of mobile crowdsensing (MCS) such that the drones can serve as mobile participants for task execution and therefore increase the service range. In this paper, we introduce a truck-drones MCS model by jointly considering the use of trucks and drones for task execution. In this model, drones are allowed to depart from the truck, perform one or multiple tasks, and rendezvous with the truck for data retrieval and battery swap while the truck can simultaneously serve as a mobile hub and also a mobile task executor. The objective is to minimize the total cost for truck movement, drone flying, and driver payment in the MCSprocess. We formulate this problem as a mixed-integer linear programming problem. We further propose an efficient algorithm based on variable neighborhood search for efficient joint task assignment and path planning. Numerical results show the effectiveness of the proposed algorithm and the advantage of joint use of trucks and drones for mobile crowdsensing.
\subsubsection{A Joint Strategy Design for CUAV-based Traffic Offloading via Deep Reinforcement Learning}
abstract:The dramatic proliferation on emerging Internet-of-Things (IoT) makes our telecommunications networks more and more congested. Due to the flexible deployment and spectrum supplement capabilities, cognitive radio based unmanned aerial vehicles (CUAVs) have been regarded as a promising solution to help the network offload the overwhelming traffic. For the CUAV-assisted network, how to offload as much traffic as possible is significant. It is necessary to jointly consider both sides on data collection and data transmission, which, however, is a very challenging problem due to the heterogeneous and uncertain environment on both traffic demand and spectrum availability. In this paper, aiming at maximizing the offloaded traffic, we propose a joint strategy on trajectory, time division, and spectrum access. Considering the unobtainable environmental information on both traffic demand and spectrum availability, we further develop a model-free deep reinforcement learning (DRL) based solution for the joint strategy, so that the CUAV could make the best decisions autonomously under the uncertain environment. Simulation results have shown the effectiveness of the designed DRL solution and also the offloading efficiency of the proposed strategy.
\subsubsection{Energy Efficiency Optimization for D2D communications in UAV-assisted Networks with SWIPT}
abstract:This paper investigates the energy efficiency (EE) optimization problem for device-to-device (D2D) communications underlaying non-orthogonal multiple access (NOMA) in unmanned aerial vehicles (UAVs)-assisted networks with simultaneous wireless information and power transfer (SWIPT). Our aim is to maximize the energy efficiency of the system while satisfying the constraints of transmission rate and transmission power budget. However, the considered EE optimization problem is non-convex involving joint optimization of the UAV's location, beam pattern, power control and time scheduling, which is difficult to solve directly. To tackle this problem, we develop an efficient resource allocation algorithm to decompose the original problem into several subproblems and solve them sequentially. Specifically, we first apply the Dinkelbach method to transform the fraction problem to a subtractive-form one, and propose a mulitiobjective evolutionary algorithm based on decomposition (MOEA/D) based algorithm to optimize the beam pattern. We then optimize UAV's location and power control by applying the successive convex optimization techniques. Finally, after solving the above variables, the original problem is transformed into a single-variable problem with respect to the charging time, which is a linear problem and can be tackled directly. Numerical results verify that the significant EE gain can be obtained by our proposed method as compared to the benchmark schemes.
\subsubsection{Capacity Optimization for Downlink Multiuser MISO UAV Mobile Communication}
abstract:The airborne mobile wireless networks (AMWNs) composed of unmanned aerial vehicles (UAVs) communication networks is one of the emerging application scenarios of the Space-Ground-Sea Integrated Networks in 6G. Due to its high mobility, UAV exhibits great advantages in assisted cellular network communication and emergency communication. The multi-antenna technology and quality of service (QoS) can help cellular networks reduce latency and improve network capacity. In this paper, we jointly optimize the transmit power and flight trajectory of the multiuser MISO UAV communication system to improve the total effective capacity of the system. Specifically, zero-forcing beamforming and block diagonalization are adopted to eliminate multiuser interference. In addition, the optimization problem with regard to maximizing the total effective capacity of the system under the condition of delay constraint is proposed. Furthermore, an iterative optimization algorithm jointly considering UAV trajectory and subchannel transmit power is considered, in which we use successive convex optimization technique and block coordinate descent method to solve the thorny non-convex problem. Simulation results verify the effectiveness and convergence of the proposed algorithm.
\subsubsection{Drone-Aided Network Coding for Secure Wireless Communications: A Reinforcement Learning Approach}
abstract:This study investigates how base stations (BSs) apply network coding to protect the downlink data and how drones relay the coded packets to resist active eavesdropping that performs jamming to induce the BS to raise the transmit power and thus steal more data. We present a drone-aided network coding framework for secure downlink transmission, which incorporates a random linear network coding algorithm to encode the BS messages against active eavesdropping. This framework designs a model-based reinforcement learning to choose the BS network coding and transmission policy based on the jamming power sent by the active eavesdropper, the previous transmission performance, and the BS channel states without the prior knowledge of the drone-eavesdropper channel states. The learning parameters such as the Q-values are updated by the real experiences in the downlink transmission process besides the simulated experiences that are generated from the virtual model in the designed Dyna architecture. Simulation results show that our proposed scheme outperforms the benchmarks in terms of the intercept probability, the transmission performance, and the BS energy consumption.
\subsection{Vehicular Communication Networks}
\subsubsection{Scaling A Blockchain System For 5G-based Vehicular Networks Using Heuristic Sharding}
abstract:5G communications are expected to expand both capacity and flexibility in future vehicular networks. However, due to the wide coverage range of 5G-based networks, massive device access in the 5G era will pose great challenges in access control and terminal management. In order to address the scalability issue in large-scale 5G-based vehicular networks, we propose in this paper the use of two heuristic sharding schemes which are based on the Determinantal Point Process (DPP) with different complexities. Specifically, in the proposed algorithms, both location and wireless channel condition of a base station (BS) are jointly considered respectively as diversity and quality parameters in the DPP. Both of them can effectively control the size of each shard, ensure the shards are evenly distributed and allow in-shard cooperation among the BSs. The communication robustness is then greatly improved due to the efficient in-shard cooperation and the system guarantees stable throughput even in scenarios where transactions volume changes dynamically. While compared to benchmark schemes, the simulation results of the proposed protocol and algorithms show significant performance gains in terms of coverage and load balancing.
\subsubsection{Communication Delay-Aware Network Topology Adaptation for Cooperative Control of Vehicular Platoons}
abstract:Communication-enabled Cooperative Adaptive Cruise Control (CACC) is an important technology to improve the safety and traffic capacity of vehicle platoons. However, Most existing CACC research considers a conventional communication delay and fixed network topology. When the network is attacked, the stability of the system will be affected. In this paper, a CACC system considering dynamic network topologies and communication delay jitters is proposed. A multi-vehicle look ahead network topology of CACC is designed to investigate the relationship between communication delay and the optimal network topology. Control parameters are obtained by solving Algebraic Riccati equations. Moreover, the relationship between the minimum inter-vehicle spacing and the communication delay is analyzed and derived under different network topologies. Finally, the performance of the proposed CACC system with adaptive network topologies is evaluated by simulations. Simulation results demonstrate that the proposed CACC system outperforms the H infinity synthesis-based controller considering the fixed network topologies. The robustness against communication failure of the proposed system, such as the DeGrading of Service (DGoS) attack, can be significantly improved, which can reduce the minimum safety headway buffer and further support better mobility of vehicles.
\subsubsection{Optimal Stochastic Coded Computation Offloading in Unmanned Aerial Vehicles Network}
abstract:Today, modern unmanned aerial vehicles (UAVs) are equipped with increasingly advanced capabilities that can run applications enabled by machine learning techniques, which require computationally intensive operations such as matrixmultiplications. Due to computation constraints, the UAVs can offload their computation tasks to edge servers. To mitigate stragglers, coded distributed computing (CDC) based offloading can be adopted. In this paper, we propose an Optimal Task Allocation Scheme (OTAS) based on Stochastic Integer Programming with the objective to minimize energy consumption during computation offloading. The simulation results show that amid uncertainty of task completion, the energy consumption in the UAV network is minimized.
\subsubsection{Time or Reward: Digital-twin Enabled Personalized Vehicle Path Planning}
abstract:Efficient path planning is the key enabling technology for the realization of intelligent transportation systems (ITS). However, due to poor real-time performance and lack of effective incentive methods, it is difficult for traditional path planning schemes to significantly improve the efficiency of traffic management. In addition, existing solutions that use driving distance and driving time as indicators cannot meet the personalized requirements of vehicle users. To this end, by considering the personalized requirements of vehicle users, we propose a digital-twin (DT) enabled path planning scheme to facilitate traffic management. To be specific, based on the collection of traffic data, we first establish a DT architecture for traffic scheduling to reduce the delay of path planning. Then, according to the traffic density of different road sections, we regard road sections as resources and set different rewards for different road sections to encourage vehicles to obey the scheduling instructions. In addition, by jointly considering the driving time and rewards, we further design personalized utility models to map the requirements of different vehicle users. After that, based on the personalized requirement of the vehicle user, we use Q-learning algorithm to obtain the optimal path with the target of maximizing the user's utility. The simulation results show that the proposed scheme can bring higher utility to the vehicle users than the conventional schemes.
\subsubsection{V2V-Assisted V2I MmWave Communication for Cooperative Perception with Information Value-Based Relay}
abstract:Millimeter-wave (mmWave) vehicular communication is a key technology that enables autonomous vehicles to collaborate in environment perception to improve traffic efficiency and safety to a new level.Many recent works have focused on vehicular relay-based transmission solutions to overcome the inherent defects of mmWave such as severe path loss and sensitivity to blockages. However, the selfish nature of the vehicles is often ignored. Considering the application-oriented nature, we investigate an information value-based relay strategy for mmWave vehicle-to-infrastructure (V2I) transmission. In particular, vehicles are allowed to make relay decisions by evaluating the value of the messages from their own perspectives. This paper introduces a simple relay probability model based on the required awareness range of vehicles and analyzes the outage performance by modeling the vehicular network using stochastic geometry. The analytical results are validated via simulations. Impacts of both network-related and application-related parameters on outage performance are studied. These preliminary results provide a basis for us to extend the information value-based relay strategies to a wider range of network settings in the future.
\section{Next-Generation Networking and Internet}
\subsection{AI for networking}
\subsubsection{CCAS: A Collective Communication Accelerating Switch Architecture for Distributed ML Training}
abstract:To update weights in distributed machine learning(DML), two approaches are used for the aggregation of local dataon each server node: by centralized parameter servers (PS), oramong worker servers based on Allreduce operations. AlthoughAllReduce-based approach solves the well-known scalabilityproblem of PS approach, it requires more network hops andincurs extra traffic. To alleviate the problem of AllReduce-basedapproach, we propose a collective communication acceleratingswitch (CCAS) architecture for DML. Using built-in hardwareaccelerators in a switch chip, CCAS offloads aggregations fromservers to switches for collective operations, especially Allreduceoperations. Furthermore, we propose a novel dual-crossbarstructure to improve switching efficiency in CCAS. Evaluationsbased on a cycle-accurate simulator we developed show that,CCAS offers a 2.69 speedup in terms of training efficiency andsignificant reduction network traffic during training process.
\subsubsection{Packet Routing with Graph Attention Multi-agent Reinforcement Learning}
abstract:Packet routing is a fundamental problem in communication networks that decides how the packets are directed from their source nodes to their destination nodes through some intermediate nodes. With the increasing complexity of network topology and highly dynamic traffic demand, conventional model-based and rule-based routing schemes show significant limitations, due to the simplified and unrealistic model assumptions, and lack of flexibility and adaption. Adding intelligence to the network control is becoming a trend and the key to achieving high-efficiency network operation. In this paper, we develop a model-free and data-driven routing strategy by leveraging reinforcement learning (RL), where routers interact with the network and learn from the experience to make some good routing configurations for the future. Considering the graph nature of the network topology, we design a multi-agent RL framework in combination with graph attention network (GAT), tailored to the routing problem. Three deployment paradigms, centralized, federated, and cooperated learning, are explored respectively. Simulation results demonstrate that our algorithm outperforms some existing benchmark algorithms in terms of packet transmission delay and affordable load.
\subsubsection{NerualMon: Graph Neural Network for Flow Measurement Allocation}
abstract:Fine-grained and accurate network flow measurements are essential for various network management tasks. In recent years, the evolution of programmable networks enables flow measurement on the switch. However, limited hardware resources on programmable switches drive the shift of measurement from a single switch to network-wide coordinations. This paper aims to optimize the allocation strategy of flow measurement among switches under the objective of measurement coverage and accuracy in network-wide measurement scenarios. We design a Graph Neural Network model, NeualMon, that can model and solve the above problem precisely. NeualMon converts network topologies and network flows into a hypergraph and transforms the flow measurement task allocation problem into a node classification problem. NeuralMon is effective in learning the task allocation solution from the network topologies and flows directly. Even on untrained real-world network topologies, NeualMon still provides excellent performance.
\subsubsection{Proactive 3C Resource Allocation for Wireless Virtual Reality Using Deep Reinforcement Learning}
abstract:Virtual reality (VR) over wireless has emerged as an important application in future mobile networks. However, it is difficult for the existing mobile networks to meet the requirements of massive data transmissions and ultra-low latency for wireless VR. Multi-access edge computing (MEC) network, providingcaching and computing capacities at network edge, emerges as a promising method to support wireless VR. However, mobile VR users' quality of experience (QoE) may be degraded by frequent handoffs. In this paper, we propose a proactive caching, computing and communication (3C) resource allocation method to provide smooth VR videos to handoff users. Specifically, the expected 3-dimensional (3D) video or 2D video for rendering is cached at a target base station (BS) ahead of time, and the size and quality of the video file are decided according to the 3C resources at the BS. Then, we model the the proactive 3C resource allocation as a Markov decision process and an effective allocation policy is obtained by a model-free algorithm based on deep reinforcement learning. Numerical results show that theproposed method can provide VR users with high QoE when they are moving between BSs.
\subsubsection{Deep Recurrent Learning versus Q-Learning for Energy Management Systems in Next Generation Network}
abstract:An AI based energy management system (EMS) for microgrids is proposed. It is composed of three modules: a strategy based module, a deep learning (DL) and a reinforcement learning module (RL). This framework determines heuristically the optimal actions for the microgrid system under different time-dependent environmental conditions. In essence, a main innovation is applied to the EMS. Our deep learning algorithm uses recurrent neural networks (RNNs) instead of the habitual State Action Reward (SAR) approach (whether classical or deep). Learning is hence guided by successful actions rather than by blind exploration. A large improvement in learning rates is hence observed when compared to classical Q-learning on real datasets that present a large diversity in energy consumption profiles, acquired in French premises over a long period. It leads to question about the best appropriate reinforcement policies to adopt when solving large state environments.
\subsection{Edge/Fog Computing}
\subsubsection{Routing and Packet Scheduling For Virtualized Disaggregate Functions in 5G O-RAN Fronthaul}
abstract:Open Radio Access Network (O-RAN) is an innovative RAN architecture designed to revolutionize 5G and beyond mobile networks. O-RAN virtualizes the fronthaul network functions into Open Centralized Unit (O-CU), Open Distributed Unit (O-DU) and Open Radio Unit (O-RU). Unfortunately, no standard data communication mechanism has been defined for the communication between these elements. Therefore, O-DUs may not work efficiently in O-DU pool, limiting the RAN performance. This paper investigates an optimized solution for routing and packet scheduling, allowing multiple O-DU pools to communicate with their O-RUs meeting the requirements of different 5G classes of service.We propose an O-DU pool architecture and formulate the problem of optimal routing and packet scheduling to forward Orthogonal Frequency-Division Multiplexing (OFDM) symbols over the optimal routes and map UDP packet sizes to fragment OFDM symbols. Numerical results show our solution can select the optimal routes and packet sizes to carry requested traffic. Moreover, in the multiple O-DU pools coexisting, we use the Dynamic Programming (DP) algorithm to find out the optimal global solution and a greedy algorithm to approximate the solution in near real-time.
\subsubsection{Content Popularity Prediction in Fog-RANs: A Bayesian Learning Approach}
abstract:In this paper, the content popularity prediction problem in cache-enabled fog radio access networks (F-RANs) is investigated. In order to predict the content popularity with high accuracy and low complexity, we propose a Gaussian process based Poisson regressor to model the content request pattern. Firstly, the relationship between content features and popularity is captured by our developed model. Then, we utilize Bayesian learning to learn the model parameters, which are robust to over-fitting. However, Bayesian methods are usually unable to find a closed-form expression of the posterior distribution. To tackle this issue, we apply a Stochastic Variance Reduced Gradient Hamiltonian Monte Carlo (SVRG-HMC) to approximate the posterior distribution. Two types of predictive content popularity are formulated for the requests of existing contents and newly-added contents. Simulation results show that the performance of our proposed policy outperforms the policy based on other Monte Carlo based method.
\subsubsection{Auction Pricing-Based Task Offloading Strategy for Cooperative Edge Computing}
abstract:Mobile edge computing (MEC) enables resource-constrained mobile devices (MDs) to offload their tasks onto nearby edge servers. However, there exists a profit allocation problem between users and edge nodes (ENs) due to the limitations of ENs computing capacity and spectrum resources. In this paper, we propose an auction pricing-based MEC offloading strategy to maximize the profit of ENs. Firstly, we design an overall auction process using the binary offloading model by considering MDs battery capacity, basic profit, and tasks tolerable delay. Secondly, the bidding willingness of MDs in each round of auction are given on the premise of effectively ensuring users rationality. Finally, an auction pricing-based task offloading strategy is proposed, in which the winner of a single-round auction can offload its computation task to the ES. Simulation results verify the performance of the proposed strategy. Compared with the VA algorithm, the profit obtained by ENs has increased by 23.8%.
\subsubsection{AFC: A Mechanism for Distributed Data Processing in Edge/Fog Computing}
abstract:A service in cloud computing is executed in a data center which may result in large communication delay.To host applications requiring low latency, edge/fog computing has attracted great attention.This paper proposes a mechanism called AFC (Application Function Chaining) for realizing distributed edge/fog computing.In AFC, an application is defined as a Chained-AF, which is composed of one or more AFs (Application Functions) with control structures such as conditional branches using the execution status of AFs.An AF basically fulfills a single task similar to a UNIX command.Thus, a Chained-AF looks like a distributed shell script and the networks look like a single computer from users' viewpoint.A MANO (Management and Network Orchestration) mechanism in AFC places AFs on optimal edge/fog servers which satisfy Chained-AF's requirements when it is launched.Evaluation results of a prototype implementation show that it takes 0.5 seconds and 3 seconds to launch a Chained-AF composed of two and six AFs, respectively, and that the throughput of a Chained-AF is more than 90 % of the line speed, which is independent of the number of AFs.The throughput of AFC depends on processing performance of AFs, not on the AFC mechanism.
\subsubsection{A Trust-aware Fog Offloading Game with Long-term Trustworthiness of Users}
abstract:The novel fog computing can save substantial resources for resource-constrained mobile users with computation offloading. However, in the existing on-demand schemes, the fog node cannot satisfy the users' demands during peak time due to its limited resources. Therefore, an efficient allocation scheme is desirable, in which the users' priority is evaluated and sorted based on their features, e.g., trustworthiness. In terms of the trust, allocating resources and motivating users to behave cooperatively are important for network efficiency and fairness. In this paper, a long-term trust-based offloading scheme (LTOS) is proposed with a resource allocation scheme and a non-cooperative game. Firstly, a long-term trust evaluation scheme is designed considering the users' behaviors in the task assignment process. In the offloading process, the computation and transmission resources are allocated to users based on their trust values. In addition, a non-cooperative offloading game is formulated to maximize users' utilities, which considers the joint optimization of energy cost and delay. The simulation results demonstrate that our proposed long-term trust-based offloading scheme is more efficient than existing on-demand methods in terms of energy cost and delay. Moreover, the task acceptance and trust level of users can be improved with the proposed LTOS.
\subsection{Mobile Edge Computing}
\subsubsection{Reinforcement based Communication Topology Construction for Decentralized Learning with Non-IID Data}
abstract:Federated Learning (FL) allows Internet-of-Things (IoT) devices to train a global model collaboratively and circumvent the security issue. However, the current FL framework has three main drawbacks, the huge network overhead, single point of failure, and accuracy degradation in non-independent-and-identically-distributed (non-IID) data distribution. We propose a novel Deep Reinforcement Learning (DRL) based Decentralized Learning (DL) framework, DeepSelect, to 1) reduce the network overhead of conventional FL, 2) construct a good communication topology adaptively to mitigate the effect of non-IID data, and 3) accelerate the DL training by balancing the effects of hitting time (HT) and data bias. Moreover, DeepSelect with a subtly-designed DRL agent is reusable with different levels of non-IID data distributions. To the best of our knowledge, this paper is the first one to indicate that a proper neighbor selection for exchanging parameters (not raw data) can counterbalance the data bias's effect and improve the DL convergence with non-IID data. The experiment results indicate that DeepSelect can always get the best performance and reduce 18%-51% training rounds than the other heuristics on FashionMNIST and CIFAR-10 with non-IID data distributions.
\subsubsection{Cost-Efficient Shuffling and Regrouping Based Defense for Federated Learning}
abstract:Federate learning (FL) enables multiple user devices to collaboratively train a global machine learning (ML) model by uploading their local models to the central server for aggregation. However, attackers may upload tampered local models (e.g., label-flipping attack) to corrupt the global model. Existing defense methods focus on outlier detection, but they are computationally intensive and can be circumvented by advanced model tampering. We employ a shuffling-based defense model to isolate the attackers from ordinary users. To explore the intrinsic properties, we simplify the defense model problem and formulate it as a Markov Decision Problem (MDP) to find the optimal policy. Then, we introduce a novel notion, (re)grouping, into the defense model to propose a new cost-efficient defense framework termed SAGE. Experiment results manifest that SAGE can effectively mitigate the impact of attacks in FL by efficiently decreasing the ratio of attacker devices to ordinary user devices. SAGE increases the testing accuracy of the targeted class by at most 40%.
\subsubsection{User-Oriented Task Offloading for Mobile Edge Computing in Ultra-Dense Networks}
abstract:The rapid development of 5G and Internet-of-Things catalyzes ever-increasing computation-intensive and delay-sensitive applications demanding ubiquitous computation services. Integrating mobile edge computing (MEC) in ultra-dense network (UDN) is a key enabler to meet the service demand by allowing smart devices to perform uninterrupted task offloading via densely deployed MEC servers. In this paper, we take a user-oriented approach to minimize a long-term delay for a given task duration under a price budget constraint. To address this problem, we develop a novel contextual sleeping bandit learning (CSBL) algorithm, which integrates context information and sleeping bandit theory to handle the fast changing environment and leverages Lyapunov optimization to deal with the price budget. We derive the upper bound of learning regret and provide a rigorous proof that CSBL asymptotically approaches the Oracle algorithm within bounded deviations for a finite task duration. Simulation results illustrate that CSBL significantly outperforms existing algorithms.
\subsubsection{Dynamic Service Migration with Partially Observable Information in Mobile Edge Computing}
abstract:Service migration, determining when, where and how to migrate the ongoing service, is of paramount importance in mobile edge computing (MEC) for provisioning high quality of service to mobile users. With respect to high network dynamics and stringent delay requirements, service migration is a rather challenging issue in MEC. In this paper, we formulate service migration as a partially observable Markov decision process (POMDP) based on the fact that an edge server can only obtain partial users' information, or the information of its own serving users. A learning-based intelligent service migration algorithm, named iSMA, is proposed to minimize the long-term service delay of all users. iSMA consists of two function modules, a latent space model and a cross-entropy planning algorithm, where the latent space model is used to infer the full state of the environment based on the partial information observed, and the cross-entropy planning algorithm is used to search the best service migration strategy. Numerical results show that our proposed iSMA reduces the service delay by about 58% when compared with a well-known deep learning-based solution.
\subsubsection{ML-driven scaling of 5G Cloud-Native RANs}
abstract:The evolution of the different network functions to a cloud-native configuration creates fertile ground for the efficient management and reconfiguration of the network. Through the wide application of softwarization and virtualization, cloud-native approaches can extend even to the RAN, that has been dominated by monolithic non-configurable hardware equipment in the past generations of mobile network access. As such, a cloud-native deployment can cover the end-to-end 5G network architecture, from the Core Network to the base stations, with the respective services benefiting from several advanced features, such as automatic scaling of the deployed functions based on monitored metrics. Through the application of Machine Learning, the evolution of the metrics can be predicted and thus the respective functions can be pro-actively scaled. In this work, we use an end-to-end real-world cloud-native deployment of a 5G network, and deal with two different types of scaling, applied at three different parts of the network: vertical scaling for the base station, and horizontal scaling for control and user plane functions of the core network. We use a real-world dataset for replicating traffic over our setup and closely monitor the evolution of metrics from different parts of the network. By applying Machine Learning methods, we accurately predict the future network load and use it to decide on the pro-active allocation of resources for the RAN and the Core Network.
\subsection{Named Data Networking}
\subsubsection{Towards Problem of First Miss under Mobile Edge Caching}
abstract:Mobile Edge Caching (MEC) can cache content at the edge of the network to reduce the delay and overhead of content transmission, which has become an effective method to solve the explosive growth of network traffic. To make good use of the limited resources in edge devices, many contents caching strategies use various methods to predict the popularity of contents. However, caches get close to the edge of the network can lead to the rapid increase of caches' number and the user's requests are dispersed into a large number of caches, which leads to the popularity distribution of contents in edge caches is quite different and the number of first miss requests (the corresponding content is requested for the first time and is not in the cache) in edge caches becoming an important factor affecting the cache hit rate. In this paper, we first demonstrate the huge impact of the first miss requests through dataset analysis and establish a mathematical model for the first miss problem in the edge cache. Then we analyze the similarity of requests received by caches, and propose a proactive push algorithm based on similarity to improve the hit rate of edge caches. Through the trace-driven simulation experiment, we verify that the methods proposed in this paper can greatly improve the caches hit rate.
\subsubsection{When QUIC's Connection Migration Meets Middleboxes A case study on mobile Wi-Fi hotspot}
abstract:QUIC's connection migration provides an opportunity for network middleboxes to control paths of network traffic in a fine time granularity without worrying about breaking down the transport layer connections. This paper takes the first step to investigate its feasibility and effectiveness. We apply connection migration to a multi-carrier Wi-Fi relay to counter the latency caused by cellular network suspension. We identify a deadlock in such a situation, which eliminates the gain of passive migration. We analyze the root cause and propose a simple but effective approach to resolve the deadlock. Our findings may also motivate similar applications in other middlebox scenarios.
\subsubsection{A GNN-based Approach to Optimize Cache Hit Ratio in NDN Networks}
abstract:Named data networking (NDN) is an emerging network architecture that has the in-network caching functionality. Optimizing caching in NDN can reduce the traffic workload and improve the network efficiency. In this paper, we represent a Graph Neural Network (GNN) based caching strategy to improve caching in NDN. Firstly, we utilize convolutional neural network to extract time-series features for each node. Secondly, we apply GNN to make node-wise content caching probability prediction. Finally, we make cache replacement decisions according to the content caching probability ranking of each node. Experimentation shows that our caching strategy achieves around 30% higher cache hit ratio and 5 milliseconds lower latency than the state-of-the-art caching strategy.
\subsubsection{QKDN meets ICN: Efficient Secure In-Network Data Acquisition}
abstract:With the advent of massive Internet of Things (IoT) applications, the demand for efficient and secure data communication has been increased. Quantum key distribution (QKD) and information-centric networking (ICN) can be considered promising approaches. However, because these approaches have been developed independently thus far, they impose problems in effectively meeting demand. In this paper, we propose a system architecture that combines the two networking technologies such that each problem can be compensated for in a sophisticated manner. Through comprehensive simulations, we demonstrate that the streamlined system brings about a significant synergistic effect especially in terms of the utilization efficiency of secure keys and the effectiveness of content delivery.
\subsubsection{Blue Data Computation Maximization in 6G Space-Air-Sea Non-Terrestrial Networks}
abstract:Non-terrestrial networks (NTN), encompassing space and air platforms, are a key component of the upcoming sixth-generation (6G) cellular network. Meanwhile, maritime network traffic has grown significantly in recent years due to sea transportation used for domestic and international trade, national defense, research, and recreational activities. In this paper, the seamless and reliable demand for communication and computation in maritime wireless networks is investigated. Two types of marine user equipment (UEs), i.e., low-antenna gain and high-antenna gain UEs, are considered. A joint task computation and time allocation problem for weighted sum rate maximization is formulated as mixed-integer linear programming (MILP). The goal is to design a new algorithm that enables the network to efficiently offload HUEs tasks and allocate UAV backhaul resources for blue data (i.e., marine user's data) computation maximization in a complex environment. To solve this MILP, a solution based on the Bender and primal decomposition is proposed. The Bender decomposes MILP into the master problem for binary task decision and subproblem for continuous-time resource allocation. Moreover, primal decomposition deals with a coupling constraint in the subproblem. Finally, numerical results demonstrate that the proposed algorithm provides the maritime UEs coverage demand in polynomial time computational complexity and achieves a near-optimal solution.
\subsection{Network Functions Virtualization (NFV) - 1}
\subsubsection{Towards Fine-Grained Resource Allocation in NFV Infrastructures}
abstract:Resource optimization arguably comprises a crucial aspect for Network Function Virtualization (NFV) infrastructures. In this respect, the problem of virtualized network function (VNF) placement commonly entails the selection of the most appropriate server within a single or among multiple Points-of-Presence (PoPs). Nevertheless, CPU cache hierarchy and memory locality in NUMA multi-core servers along with the diversity in NFV resource profiles introduce significant challenges in terms of intra-server resource allocation; a problem that is often overlooked. As such, we stress on the need for fine-grained resource allocation in NFV infrastructures, and, to this end, we study various aspects of CPU allocation for VNF chains. We deem this intra-server resource allocation problem as complementary to the large body of literature that seeks to optimize VNF placement onto a server.More particularly, we shed light on the intra-server VNF placement problem, treating CPU cores as the main resource allocation unit. To this end, we assess the performance of multiple CPU allocation combinations under varying server utilization levels and processing workloads with diverse requirements in terms of CPU and memory. Our experimentation approach let us progressively gain useful insights, which ultimately form ground rules that can be leveraged for optimized server resource allocation.
\subsubsection{Benchmarking Kubernetes Container-Networking for Telco Usecases}
abstract:Kubernetes container orchestration technology is beginning to address telco Network Function Virtualization (NFV) deployment needs of: i) multiple data-plane interfaces and one management-plane interface, and ii) support for various protocols/software stacks targeting I/O performance, including software acceleration technologies and software switch architectures. The purpose of this work is to accurately benchmark different network-virtualization architectures (OvS-DPDK, SR-IOV and VPP) in a Kubernetes deployment for telco data plane configurations. By leveraging industry-standard testing methodologies and an open source testing framework (VSPERF) that can control different hardware-based commercial and open source traffic generators, we benchmark the data plane of the telco NFV deployments. We also study the effects of different resource allocation strategies (CPU-Pinning, compute and storage resource variations, NUMA) on the data plane performance.
\subsubsection{Shortening the Deployment Time of SFC by Adaptively Querying Resource Providers}
abstract:We consider the SFC embedding (SFCE) problem in the Slice as a Service (SlaaS) model. In this model, a slice provider leases resources from multiple cloud and network providers in order to instantiate the Service Function Chain (SFC) requested by a slice tenant. As the slice provider has no visibility on the infrastructures of the resource providers, in which resources may be purchased and released quite rapidly, it has to query them to determine what are the possible allocations and their costs. We show that when there are many resource providers and many VNFs composing the SFC, the number of queries to be made for discovering a minimum cost SFC embedding grows quickly, leading to excessively long deployment times. In order to reduce the latter quantity, we propose to query resource providers strategically, rather than collecting the information on all possible allocations at once. We provide bounds on the number of queries to be made in this approach, and propose to exploit a Shortest Path Discovery algorithm in order to reduce this number of queries and thus the SFC deployment time. Our numerical results suggest that this algorithm is fairly efficient, and that the deployment times can be significantly shortened, in particular when initial estimates of allocation costs can be provided by the slice provider.
\subsubsection{On using Deep Reinforcement Learning for Multi-Domain SFC placement}
abstract:Service Function Chaining (SFC) has emerged as a promising technology for 5G and beyond. It leverages on Network Function Virtualization (NFV) and Software Defined Networking (SDN) and allows the decomposition of a given service into a set of blocks that successively process data.The SFC placement issue has been extensively studied in the literature, and different solutions have been proposed using mathematical models and heuristics. More recently, Reinforcement Learning (RL), has emerged as a tool for decision making that allows agents to elaborate policies based on the environment's feedback. In this paper, we study the benefits of using Deep Reinforcement Learning methods for the multi-domain SFC placement problem. We propose a Deep Deterministic Policy Gradient (DDPG) approach, where Linear Physical Programming is employed to generate rewards that reflect the solution's quality in terms of cost and latency.Through our experiments, we are able to demonstrate the efficiency of our approach with results that satisfy the SLA requirements.
\subsubsection{Traffic Prediction based VNF Migration with Temporal Convolutional Network}
abstract:In network function virtualization enabled networks with dynamic traffic, virtual network function (VNF) migration has been considered as an effective way to improve quality of service as well as resource utilization. However, due to time-varying network traffic, designing a fast and accurate VNF migration algorithm is still a great challenge. To address this issue, in this paper, we exploit the temporal convolutional network (TCN) to predict traffic flow for VNF migration decision in a fast and accurate manner. Based on the predicted results, we define a metric, i.e., migration index, to represent the load trend of each node in the network. A fast and efficient heuristic VNF migration algorithm is then proposed based on the migration index, with the goal to minimize the total migration cost in a time period. Extensive simulations are carried out to validate the effectiveness of TCN for traffic prediction. The results demonstrate that the proposed VNF migration algorithm can reduce the total migration cost up to 20% compared with existing algorithms.
\subsection{Network Functions Virtualization (NFV) - 2}
\subsubsection{Enforcing Resource Allocation and VNF Embedding in RAN Slicing}
abstract:Going beyond the one-type-fits-all design philosophy, the future 5G radio access network (RAN) with network slicing methodology is employed to support widely diverse applications over the same physical network. RAN slicing aims to logically split an infrastructure into a set of self-contained programmable RAN slices, with each slice built on top of the underlying physical RAN (substrate) is a separate logical mobile network, which delivers a set of services with similar characteristics. Each RAN slice is constituted by various virtual network functions (VNFs) distributed geographically in numerous substrate nodes. A RAN configuration scheme for the network is imperative to embed VNFs in substrate nodes. In this paper, we propose to design new algorithms to enhance the stability of RAN slicing by addressing the resources allocation and VNF embedding problem, referred to as RS-configuration. Specifically, we establish the theoretical foundation for using RS-configuration to construct a VNF mapping plan for all VNFs with two efficient algorithms, including Group-based Algorithm (GBA) and Group-Connectivity-based Algorithm (GCBA). Through rigorous analysis and experimentation, we demonstrate that the proposed algorithms perform well within reasonable bounds of computational complexity.
\subsubsection{A multidimensional coloured packing approach for Network Slicing with dedicated protection}
abstract:Network Function Virtualization (NFV) enables the virtualization of core-business network functions on top of a NFV infrastructure. It has gained an increasing attention in the telecommunication field. Virtual network functions (VNFs) can be represented by a set of virtual network function components (VNFCs). Typically, these VNFCs are usually designed with a redundancy scheme and need to be deployed against failures of the compute servers. However, such deployment must respect a particular resiliency mechanism for protection purposes. Therefore, choosing an efficient mapping of VNFCs to the compute servers is a challenging problem in the optimization of the software-defined, virtualization-based next generation of networks. In this paper, we model the problem of reliable VNFC placement under anti-affinity constraints using several optimization techniques. A novel approach based on an extension of bin packing is proposed. We perform a comprehensive evaluation in terms of performance under real-world ISP network along with synthetic traces. We show that our methods can calculate efficient solutions for large instances very fast.
\subsubsection{Proposal and Investigation of an ETSI NFV Architecture supporting AI-based Resource Prediction}
abstract:The high reconfiguration time of cloud resources in Network Function Virtualization architecture has led to make ineffective the reactive cloud resource allocation procedures whose application lead to over-allocate resources or to degrade Quality of Service in decreasing/increasing traffic scenario. Recently many Artificial Intelligence (AI)-based allocation procedures have been proposed to pre-allocate cloud resource according to required processing capacity predictions. In this paper we illustrate how an ETSI NFV architecture can support these predictions procedures. Furthermore they aim to exactly predict the processing capacity to be allocated and they are all based on the minimization of a symmetric loss function of the neural network. For this reason we propose a resource allocation procedure with a asymmetric loss function whose parameters are dependent on an overall cost expressed in terms of allocation and QoS degradation costs. We prove that the proposed solution allows for a cost reduction in the order of 30% in a typical NFV traffic and network scenario.
\subsubsection{Non-parametric Decision-Making by Bayesian Attractor Model for Dynamic Slice Selection}
abstract:In 5G, the network is divided into slices to provide communications with different characteristics, such as low latency and reliable communications (URRLC), multiple connections (MTC), and high speed and high capacity communications (eMBB), for different applications.Although the selection of network slices is often static, in practice, dynamic slice selection is required depending on the application situation.However, there are issues such as the slice change itself changing the application situation and the delay associated with the slice change.In this paper, we realize dynamic slice selection by recognizing the rough situation and the mapping between the recognized situation and the slice.The Bayesian Attractor Model (BAM) is used for recognition to achieve consistent recognition and is extended to the Dirichlet Process Mixture Model (DPMM) to achieve automatic attractor construction.The mapping between situations and slices is also automatically learned by using feedback.As an application of dynamic slice selection, we also show slice selection based on the video streaming situation.Through numerical examples, we show that our method can keep the quality of video streaming high while reducing slice changes.
\subsubsection{KRS: Kubernetes Resource Scheduler for resilient NFV networks}
abstract:To address the diversity of use cases envisioned by the 5G technology, it is critical that the design of the future networks allows maximum flexibility and cost effectiveness. This requires that network functions should be designed in a modular fashion to enable fast deployment and scalability. This expected efficiency can be achieved with the cloud-native paradigm where network functions can be deployed as containers.Virtualization tools such as Kubernetes [1] offer multiple functionalities for the automatic management of the deployed containers hosting the network functions. These tools such as resource scheduling and replicas must be applied efficiently to improve the network functions availability and resilience. This paper focuses on resource allocation in a Kubernetes infrastructure hosting different network services. The objective of the proposed solution is to avoid resource shortage in the cluster nodes while protecting the most critical functions.A statistical approach is followed for the modeling of the problem as well as for its resolution, given the random nature of the treated information.
\subsection{Network management and control - 1}
\subsubsection{Low Priority Congestion Control for Multipath TCP}
abstract:Many applications are bandwidth consuming but may tolerate longer flow completion times. Multipath protocols, such as multipath TCP (MPTCP), can offer bandwidth aggregation and resilience to link failures for such applications, and low priority congestion control (LPCC) mechanisms can make these applications yield to other time-sensitive ones. Properly combining the above two can improve the overall user experience. However, the existing LPCC mechanisms are not adequate for MPTCP. They do not take into account the characteristics of multiple network paths, and cannot ensure fairness among the same priority flows. Therefore, we propose a multipath LPCC mechanism, i.e., Dynamic Coupled Low Extra Delay Background Transport, named DC-LEDBAT. Our scheme is designed based on a standardized LPCC mechanism LEDBAT. To avoid unfairness among the same priority flows, DC-LEDBAT trades little throughput for precisely measuring the minimum delay. Moreover, to be friendly to single-path LEDBAT, our scheme leverages the correlation of the queuing delay to detect whether multiple paths go through a shared bottleneck. Then, DC-LEDBAT couples the congestion window at shared bottlenecks to control the sending rate. We implement DC-LEDBAT in a Linux kernel and experimental results show that DC-LEDBAT can not only utilize the excess bandwidth of MPTCP but also ensure fairness among the same priority flows.
\subsubsection{Traffic Statistical Upper Limit Prediction from Flow Features in Network Provisioning}
abstract:Machine learning-based network traffic prediction has been a hot research topic in recent works.The majority of recently developed prediction models have employed Long Short-Term Memory (LSTM) to predict the traffic at a few steps in the future from the most recent past traffic.However, for network provisioning to plan the capacity of a facility in a few days or months, it is necessary to derive its statistical upper limit from the traffic prediction results of a larger number of time steps.This paper investigates the network provisioning to estimate the conditional probability density function (PDF) of traffic given the future conditions instead of the traffic prediction for each step.Specifically, the network provisioning estimates the PDF of traffic given the future flow feature data and predicts the upper limit of the confidence interval of the estimated PDF as the facility capacity.This paper proposes two PDF estimation models are based on Supervised-Variational AutoEncoder (SVAE) and Student-t-SVAE (t-SVAE).Especially to deal with the biased traffic caused by the mixed flows with different features, such as elephant and mice flows, the t-SVAE uses a heavy-tailed Student-t distribution as the predictive traffic distribution.Our experimental result confirms that t-SVAE-based prediction can perform the high accuracy of the statistical upper limit prediction for provisioning networks where flows with different features are mixed.
\subsubsection{Adaptive Multi-slot-ahead Prediction of Network Traffic with Gaussian Process}
abstract:Multi-slot-ahead forecasting on network traffic provides an extra degree of freedom to proactively manipulate the network resources when immediate reconfiguration of networks is expensive or infeasible. In return, it challenges the existing data-driven learning-based approaches on accuracy, especially when considering the evolving property of the traffic process. To this end, we establish an adaptive learning framework for multi-slot-ahead network traffic prediction based on Gaussian Process (GP). GP facilitates learning and comprehending the traffic process from a Bayesian perspective, where the main characteristics can be encoded into the kernel function for performance enhancement. The contributions of this paper are two-fold: 1). To track the evolving traffic characteristics, we approximate the optimal kernel adapting to the current traffic. 2). To predict in a large time horizon without significantly hurt the performance, Linear Model of Co-regionalization (LMC) is utilized to better make use of the correlation among subsequent multiple time-slots. Finally, we demonstrate the high tracking capability as well as the superiority of the proposed framework in terms of prediction accuracy through simulation.
\subsubsection{A Novel Mobile Core Network Architecture for Satellite-terrestrial Integrated Network}
abstract:Loading the functions of base station on satellite can improve the flexibility of user access and expand the effective coverage of Satellite-Terrestrial Integrated Network (STIN).However, the functions of management and control that supports the satellite networking have not been well investigated. A large amount of signaling needs to be forwarded to the ground control center for processing, which increases the delay of network control and management.In this paper, we propose a novel mobile core network architecture that loads the mobility management function of core network on the satellite, which can improve the flexibility of management and control of STIN.Firstly, considering the on-board computing capacity and the dynamic characteristic of satellites, we design the lightweight Satellite Mobile Core Network (SMCN) to improve the flexibility of the Satellite next-generation NodeB (Sat-gNB) networking.Then, the interactive protocol for user access control, mobility control of Sat-gNBs and the coordinated control between terrestrial network and satellite network are designed to support the mobility management.Finally, in order to optimize the delay of Sat-gNBs networking, we construct the mathematical model for the multi-SMCN deployment.The simulation results show that compared to the architecture of non-terrestrial networks (NTN), the proposed mobile core network architecture can improve the flexibility of management and control, and then optimize the network delay.
\subsubsection{Flow-level Adaptive Routing Scheme for RDMA enabled Dragonfly Network}
abstract:To minimize the number of expensive global links, Dragonfly topology is developed greatly in today's data centers. However, deploying Remote Direct Memory Access (RDMA) applications inside Dragonfly requires the network to provide a routing scheme running at the flow-level to avoid packet disorder. The existing Dragonfly routing scheme uses queue length to estimate link load, which is not a reasonable criterion for flow-level routing. In this paper, we use the amount of remaining data of flows to estimate the flow completion time and propose our routing scheme, named Remaining Data-based Adaptive Load-balance (RDAL). We compare the performance of RDAL with another routing scheme at the flow-level. Our simulation shows that for flow-level routing, RDAL provides improvements in both average flow completion time and saturation throughput, especially in the adversarial traffic pattern. At most, RDAL can increase saturation throughput by 12% and reduce average flow completion time by 34% than UGAL, the state-of-the-art routing scheme for Dragonfly.
\subsection{Network management and control - 2}
\subsubsection{Concept Drift Detection in Federated Networked Systems}
abstract:As next-generation networks materialize, increasing levels of intelligence are required. Federated Learning has been identified as a key enabling technology of intelligent and distributed networks; however, it is prone to concept drift as with any machine learning application. Concept drift directly affects the model's performance and can result in severe consequences considering the critical and emergency services provided by modern networks. To mitigate the adverse effects of drift, this paper proposes a concept drift detection system leveraging the federated learning updates provided at each iteration of the federated training process. Using dimensionality reduction and clustering techniques, a framework that isolates the system's drifted nodes is presented through experiments using an Intelligent Transportation System as a use case. The presented work demonstrates that the proposed framework is able to detect drifted nodes in a variety of non-iid scenarios at different stages of drift and different levels of system exposure.
\subsubsection{Monte Carlo Tree Search for Network Planning for Next Generation Mobile Communication Networks}
abstract:In this paper, we investigate the network planning problem in mmWave mobile communication systems, where the narrow-beam antennas can adjust azimuths and downtilts of antennas so as to maximize the power coverage of the network, as well as the system throughput. However, searching for the optimal configurations of antennas yields a combinatorial optimization problem, which cannot be addressed even for a medium scale antenna set case. We formulate this optimization task as a finite Markov decision process, and develop a multi-layer Monte Carlo tree search method to produce a promisingsolution with reasonable complexity, which evaluates the outcome of given azimuth and downtilt settings without acquiring all configurations of antennas. Experiments in a real urban environment show that our proposed scheme outperforms the state-of-the-art algorithms over 10\% in terms of system throughput while guaranteeing high power coverage.
\subsubsection{MPTCP under Virtual Machine Scheduling Impact}
abstract:Multipath TCP (MPTCP) has captured the networking community's attention in recent years since it simultaneously transfers data over multiple network interfaces, thus increases the performance and stability. Existing works on MPTCP study its performance only in traditional wired and wireless networks. Meanwhile, cloud computing has been growing rapidly with lots of applications deployed in private and public clouds, where virtual machine (VM) scheduling techniques are often adopted to share physical CPUs among VMs. This motivates us to study MPTCP's performance under VM scheduling impact. For the first time, we show that VM scheduling negatively impacts all MPTCP subflows' throughput. Specifically, VM scheduling causes the inaccuracy in computing the overall aggressiveness parameter of MPTCP congestion control, which leads to the slow increment of the congestion windows of all MPTCP subflows instead of just a single subflow. This finally results in a poor overall performance of MPTCP in cloud networks. We propose a modified version for MPTCP, which considers VM scheduling noises when MPTCP computes its overall aggressiveness parameter and its congestion windows. Experimental results show that our modified MPTCP performs considerably better (with up to 80% throughput improvement) than the original MPTCP in cloud networks.
\subsubsection{Balancing Traffic Flow Efficiency with IXP Revenue in Internet Peering}
abstract:We consider a traffic peering game between Internet Service Providers (ISPs) at an Internet Exchange Point (IXP), where each ISP pair has a choice of exchanging traffic through a public IXP switch or sending the traffic through transit providers. We analyze the traffic flow efficiency (measured as social welfare) and the IXP revenue at the equilibrium of this game, as a function of the per-unit price charged by the IXP. We show that there exists a price point at which both social welfare and revenue are high, and the corresponding price-of-anarchy values can be expressed in terms of certain sublinearity measures of the inverse demand curves of the ISPs. Simulations carried out using models based on actual IXP data obtained from PeeringDB demonstrate that the theoretical bounds correctly capture the performance trends against the variation of price, and for a carefully chosen pricing point both social welfare and IXP revenue are within a factor of two of the corresponding optimal values.
\subsubsection{Flow-Level Rerouting in RDMA-Enabled Dragonfly Networks}
abstract:Due to the characteristic of large-radix routers, the Dragonfly topology can achieve low diameter, high performance/cost ratio. However, in the Dragonfly networks deployed with Remote Direct Memory Access (RDMA), existing packet-level routing algorithms which are mostly based on queue length information are neither good enough to achieve load balancing nor meet the requirement of in order. To tackle the above issues, we first analyze the drawbacks of flow-level source routing in RDMA-enabled Dragonfly networks. Then, a flow-level rerouting scheme that can estimate traffic distribution and link load based on the routers' history information is proposed. Finally, the simulation results show that our scheme can obtain significant performance gains over existing algorithms in both average flow completion time (AFCT) and saturation throughput. In particular, under the adversarial traffic pattern, our scheme can greatly reduce the AFCT of flow-level UGAL by 25% and improve the saturation throughput by 13% while avoiding disorder.
\subsection{QoS/QoE}
\subsubsection{Bandwidth Allocation with Slice Quality Fairness in Network Slicing under Variable Link Capacity}
abstract:Network slicing has been proposed to meet the diverse requirements of mobile applications. In a wireless environment with fluctuating link capacity, we need to adjust each slice's bandwidth dynamically according to the capacity while maintaining the user experience. In addition, network operators need to consider the out-of-slices users when the link capacity is insufficient for slicing. The traditional best-effort control does not allocate the bandwidth to them because it prioritizes the slice users. We set a utility function, called slice quality, for each slice based on its requirement and adjust each slice's bandwidth depending on the variable link capacity to keep these qualities fair. We define linear utility functions for the normal slice, which has a fixed bandwidth requirement called SLO, and exponential utility functions for the residual slice, which manages the out-of-slices bandwidth. The experiment results based on the real cellular trace show that we can improve the slice quality's fairness by 56% compared to a simple bandwidth allocation policy while keeping the quality degradation of normal slices below 11%.
\subsubsection{A Machine Learning Approach for Rate Prediction in Multicast File-stream Distribution Networks}
abstract:Large-volume scientific data is one of the prominent driving forces behind next generation networking. In particular, Software Defined Network (SDN) makes leveraging path-based network multicast services practically feasible. In our prior work, we have developed a cross-layer architecture for supporting reliable file-streams multicasting over SDN-enabled Layer-2 network, and implemented the architecture for a meteorology data distribution application in atmospheric science. However, it is challenging to determine an optimal rate for this application with the varying type, volume, and quality of meteorological data. In this paper, we propose a Quality of Service (QoS)-driven rate management pipeline to determine the optimal rate based on the input traffic characteristics and performance constraints. Specifically, the pipeline employs a feedtype classifier using Multi-Layer Perception (MLP) to recognize the type of meteorological data and a delay prediction regressor using stacked Long Short-Term Memory (LSTM) to predict per-file delay for the file-streams. Finally, we determine the optimal rate for the given file-streams using the trained regressor. We implement this pipeline to test the real-world file-stream data collected from a trial deployment, and the results show that our regressor outperforms all baselines by selecting the optimal rate in the presence of varying file set sizes.
\subsubsection{In-Network Processing for Low-Latency Industrial Anomaly Detection in Softwarized Networks}
abstract:Modern manufacturers are currently undertaking the integration of novel digital technologies - such as 5G-based wireless networks, the Internet of Things (IoT), and cloud computing - to elevate their production process to a brand new level, the level of smart factories. In the setting of a modern smart factory, time-critical applications are increasingly important to facilitate efficient and safe production. However, these applications suffer from delays in data transmission and processing due to the high density of wireless sensors and the large volumes of data that they generate. As the advent of next-generation networks has made network nodes intelligent and capable of handling multiple network functions, the increased computational power of the nodes makes it possible to offload some of the computational overhead. In this paper, we show for the first time our IA-Net-Lite industrial anomaly detection system with the novel capability of in-network data processing. IA-Net-Lite utilizes intelligent network devices to combine data transmission and processing, as well as to progressively filter redundant data in order to optimize service latency. By testing in a practical network emulator, we showed that the proposed approach can reduce the service latency by up to 40%.Moreover, the benefits of our approach could potentially be exploited in other large-volume and artificial intelligence applications.
\subsubsection{Towards SDN-based Deterministic Networking: Deterministic E2E Delay Case}
abstract:The explosion of the number of things connected to the Internet gave birth to a new set of services. Customers now are expecting next-generation networks to satisfy vertical applications with different requirements. For instance, 5G networks can carry these various demands by separating the physical network into multiple slices, each slice features specific characteristics based on the type of service. Software-Defined Network (SDN) and Network Function Virtualization (NFV) introduced the term "Network Virtualization" which is the main enabler of network slicing and 5G networks. Specifically, SDN separates the control plane from the infrastructure plane, this concept brings many benefits such as dynamicity flexibility, and innovation. However, when it comes to the Quality of Service (QoS), SDN is still behind. Not only SDN was not optimized for real-time communications, but also SDN networks cannot offer a deterministic End-to-End (E2E) delay. In this paper, we study OpenFlow-based communications with a focus on the delay. The modelization of queueing delay shows a stable linear development of the mean waiting time under a probability of 0.2 that 10 switches generate packet_in message. After that, the increase becomes exponential and thus hard to predict.
\subsubsection{Deterministic Service Function Chaining over Beyond 5G Edge Fabric}
abstract:With the increasing demand of latency-sensitive network services, Deterministic Network (DetNet) concept has been proposed recently to provide deterministic latency assurance for services featured with bounded latency requirements in 5G edge networks. Network Function Virtualization (NFV) enables Internet Service Providers (ISPs) to place Virtual Network Functions (VNFs) flexibly achieving performance and cost benefits. Then a service function chain (SFC) is formed by steering traffic through a series of VNF instances in a predefined order. Moreover, the required network resources and placement of VNF instances along SFC should be optimized to meet the deterministic latency requirements. Therefore, it is significant for ISPs to determine an optimal SFC deployment strategy to ensure network performance while improving the network revenue. In this paper, we will investigate the joint resource allocation and SFC placement in 5G edge networks. We formulate this problem as a mathematic programming model with the objective of maximizing the overall network profit for ISP. Furthermore, a novel Deterministic SFC deployment (Det-SFCD) algorithm has been proposed to efficiently embed SFC requests while providing deterministic latency assurance. Performance evaluation results show that the proposed algorithm can provide higher performance in terms of SFC request acceptance rate, network cost reduction, network resource efficiency compared with benchmark solutions.
\subsection{Software Defined Networking (SDN)}
\subsubsection{CSH: Towards More Accurate Flow Measurement with Counter-Sketch-Hybrid Algorithm}
abstract:Flow measurement is a fundamental problem in various network domains. As network bandwidth develops, the available memory space becomes too scarce to enable full-accurate flow measurement. Thus, counter and sketch algorithms are used to leverage approximation techniques so as to create a trade-off between memory usage and measurement accuracy. However, a single algorithm may not be able to measure Top-ks and small flows simultaneously. Accordingly, this paper proposes the Counter-Sketch-Hybrid (CSH) algorithm, which combines the counter and sketch algorithms to facilitate more accurate flow measurement under memory space constraints. We tune the relative proportion of memory allocated to the counter and sketch part to obtain the global optimal accuracy in CSH. Results show that CSH outperforms a single counter algorithm in terms of both frequency estimation and the Top-k problem, under all values of identical memory space. CSH reduces ARE by 27% and MSE by 35% on average for frequency estimation. CSH also improves F1 score by 1.9% in the Top-k problem.
\subsubsection{QCell: Self-optimization of Softwarized 5G Networks through Deep Q-learning}
abstract:With the unprecedented rise in traffic demands and mobile subscribers, real-time fine-grained optimization frameworks are crucial for the future of cellular networks. Indeed, rigid and inflexible infrastructures are incapable of adapting to the massive amounts of data forecast for 5G networks. Network softwarization, i.e., the approach of controlling "everything" via software, promises to endow the network with unprecedented flexibility, allowing it to run optimization and machine learning-based frameworks that are capable to flexibly adapt to current network conditions and traffic demands. This work presents QCell, a Deep Q-Network-based optimization framework for softwarized cellular networks. QCell dynamically allocates slicing and scheduling resources to the network base stations adapting to varying interference conditions and traffic patterns. QCell is prototyped on Colosseum, the world's largest network emulator, and tested in a variety of network conditions and scenarios. Our experimental results show that using QCell significantly improves user's throughput (up to 37.6%) and the size of the transmission queues (up to 11.9%), decreasing service latency.
\subsubsection{Towards Scalable and Expressive Stream Packet Processing}
abstract:Modern multi-core servers are powerful enough to process multi-gigabit live packet streams on the network data plane. However, in most cases network programmers must build their applications from scratch, by implementing both the interfaces towards the lower hardware level and the proper mechanisms for parallel programming. Data Stream Processing (DaSP) frameworks have recently emerged as promising approaches to overcome the above issues and to let programmers simply focus on the logic of the application to develop. However, DaSP platforms are generally not designed for the networking domain, in terms of both performance and functions. In this paper, we selected the WindFlow DaSP framework and built suitable extensions to attach multiple (accelerated) packet sources of data to it. We then implemented a simple monitoring application on top of WindFlow and carried out stress tests with synthetic and real traffic. The results prove that performance scale linearly with the processing cores so that the application was able to process the whole amount of live data up to nearly 20 Gbps rate.
\subsubsection{Dynamic Router's Buffer Sizing using Passive Measurements and P4 Programmable Switches}
abstract:The router's buffer size imposes significant implicationson the performance of the network. Network operatorsnowadays configure the router's buffer size manually. Theytypically configure large buffers that fill up and never go empty, increasing the Round-trip Time (RTT) of packets significantly, and decreasing the application performance. Previous work suggested setting the buffer size to the Bandwidth-delay Product (BDP) divided by the square root of the number of long flows. Such formula is adequate when the RTT and the number of long flows are known in advance. This paper proposes a system that leverages programmable switches as passive instruments to measure the RTT and count the number of flows traversing a legacy router. Based on the measurements, the programmable switch dynamically adjusts the buffer size of the legacy router in order to mitigate the unnecessary large queuing delays. Results show that when the buffer is adjusted dynamically, the RTT, the loss rate, and the fairness among long flows are enhanced. Additionally, the Flow Completion Time (FCT) of short flows sharing the queue is greatly improved. The system can be adopted in campus, enterprise, and service provider networks, without the need to replace legacy routers.
\subsubsection{Enabling In-band Network Telemetry in Software-based Virtual Switches}
abstract:Software-based virtual switches are indispensable in multi-tenant cloud networks. They either work as bridges between virtual machines and the underlay networks, or act as the virtual network function (VNF) carriers for flexible service chaining and orchestration. Generally, high-accuracy congestion/failure monitoring of the virtual switches is essential for ease of data center network management. The recently proposed In-band Network Telemetry (INT), which relies on the protocol-independent switch architecture (PISA), can achieve high monitoring precision. However, not all the virtual switches with production quality are P4-based or built under the PISA. In this work, we provide the design and implementation of label-based INT and probe-based INT on top of OVS and VPP, the two mainstream software-based virtual switches with non-PISA architecture. We strive to develop different INT extension strategies dedicated to the flow table-based architecture of OVS as well as the graph-based architecture of VPP. Extensive evaluation shows that our implementation has low performance overhead in terms of forwarding latency, packet loss ratio and CPU utilization. Under 100Mbps traffic pressure, the CPU overhead of INT on OVS and VPP are less than 0.1% and 0.5%, and the switch latency are added by less than 4s and 3s, respectively.
\subsection{Unmanned Aerial Vehicle (UAV) networking}
\subsubsection{Federated Learning for Air Quality Index Prediction Using UAV Swarm Networks}
abstract:People need to breathe, and so do other living beings, including plants and animals. It is impossible to overlook the impact of air pollution on nature, human well-being, and concerned countries' economies. Monitoring of air pollution and future predictions of air quality have lately displayed a vital concern. There is a need to predict the air quality index with high accuracy; on a real-time basis to prevent people from health issues caused by air pollution. With the help of Unmanned Aerial Vehicle's onboard sensors, we can collect air quality data easily. The paper proposes a distributed and decentralized Federated Learning (FL) approach within a UAV swarm. The accumulated data by the sensors are used as an input to the Long Short Term Memory (LSTM) model. Each UAV used its locally gathered data to train a model before transmitting the local model to the central base station. The central base station creates a master model by combining all the UAV's local model weights of the participating UAVs in the FL process and transmits it to all UAVs in the subsequent cycles. The effectiveness of the proposed model is evaluated with other machine learning models using various evaluation metrics using test data from the capital city of India, i.e., Delhi.
\subsubsection{Performance Analysis of Aircraft-to-Ground Communication Networks in Urban Air Mobility (UAM)}
abstract:To meet the growing mobility needs in intra-city transportation, urban air mobility (UAM) has been proposed in which vertical takeoff and landing (VTOL) aircraft are used to provide on-demand service. In UAM, an aircraft can operatein the corridors, i.e., the designated airspace, that link the aerodromes, thus avoiding the use of complex routing strategies such as those of modern-day helicopters. For safety, a UAM aircraft will use air-to-ground communications to report flight plan, off-nominal events, and real-time movements to ground basestations (GBSs). A reliable communication network between GBSs and aircraft enables UAM to adequately utilize the airspace and create a fast, efficient, and safe transportation system. In this paper, to characterize the wireless connectivity performance in UAM, a stochastic geometry-based spatial model is developed. In particular, the distribution of GBSs is modeled as a Poisson pointprocess (PPP), and the aircraft are distributed according to a combination of PPP, Poisson cluster process (PCP), and Poisson line process (PLP). For this setup, assuming that any given aircraft communicates with the closest GBS, the distribution of distance between an arbitrarily selected GBS and its associated aircraft and the Laplace transform of the interference experienced by the GBS are derived. Using these results, the signal-to-interference ratio (SIR)-based connectivity probability is determined to capture the connectivity performance of the aircraft-to-ground communication network in UAM. Simulation results validate the theoretical derivations for the UAM wireless connectivity and provide useful UAM design guidelines by showing the connectivity performanceunder different parameter settings.
\subsubsection{Towards using Deep Reinforcement Learning for Connection Steering in Cellular UAVs}
abstract:This paper investigates the fundamental connection steering issue in cellular-enabled Unmanned Aerial Vehicles (UAVs), which allows each UAV to steer the connection with one of several Mobile Network Operators (MNOs) for ensuring enhanced Quality-of-Service (QoS). We first formulate it as an optimization problem for minimizing the maximum outage probability. This is a nonlinear and nonconvex problem that is generally difficult to solve. To this end, we propose a new approach for solving the optimization problem based on Deep Reinforcement Learning (DRL), where two important reinforcement learning algorithms (i.e., Deep Q-Learning (DQN) and Advantage Actor Critic (A2C)) are also carefully considered in the approach. Extensive simulation results show that the UAVs can make optimal decisions to select the connection with MNOs for achieving the minimization of the maximum outage probability under the new approach. Furthermore, the results also show that in our new approach, the A2C based algorithm is better than the DQN based one, especially when the number of MNOs increases, while the DQN based algorithm can be executed in a shorter time.
\subsubsection{Toward Proactive Service Relocation for UAVs in MEC}
abstract:Multi-Access Edge Computing (MEC) is considered as one of the key enablers of Unmanned Aerial Vehicles (UAVs) use cases. However, the envisioned MEC deployments introduce new challenges related to the management of the mobility of services across the distributed MEC hosts, following the UAVs movements and possible handovers to ensure sustainable Quality-of-Service (QoS). A major challenge for MEC service mobility is the decision-making on where and when to relocate services. In this paper, we motivate the use of the predefined flight plans of the UAVs for devising proactive relocation strategies that can deal efficiently with realistic asynchronous relocation processes. Moreover, we formulate the Proactive Service Relocation for UAV (PSRU) problem using linear programming and we validate the gains introduced by the proactive relocation strategy and the use of the predefined flight plans of UAVs.
\subsubsection{Defending against Flooding Attacks in the Internet of Drones Environment}
abstract:Even though drones are still in the infancy period in terms of widespread adoption and use, they have already pierced through solid conventional barriers in various domains of industry. As the usage of drones is becoming commonplace,a next generation aerial communication paradigm, Internet of Drones (IoD), has been proposed to further explore drone technology in a broad scope. IoD relies on the mobility of drones and intermittent drone-to-drone (D2D) and drone-to-ground station (D2I) communications for information sharing and exchange. Because of high mobility and resource constraints, IoD is defenseless to flooding attacks where an adversary sends an excessive amount of packets (original or replica) to legitimate drones with the intention of draining the limited IoD resources (i.e., communication bandwidth and drones' storage space). In this paper, we propose a lightweight distributed detection scheme, hereafter referred to as Lids, to defend against flooding attacks in the IoD environment. The basic idea of Lids is that each drone counts the number of packets that it has sent within a predefined time interval and shares the self-counting report with other drones during contacts. The receiving drones store the self-counting reports while flying and send them to nearby ground station which will check the consistency of self-counting reports to detect flooding attacks. For performance evaluation, we implement Lids and its counterparts in OMNeT++ and conduct extensive experiments. Our experimental results indicate the superior performance of Lids to defend against flooding attacks in the IoD environment.
\subsection{Vehicle Networking}
\subsubsection{Optimal UAV Hitching on Ground Vehicles}
abstract:Due to its mobility and agility, unmanned aerial vehicle (UAV) has emerged as a promising technology for various tasks, such as sensing, inspection and delivery. However, a typical UAV has limited energy storage and cannot fly a long distance without being recharged. This motivates several existing proposals to use trucks and other ground vehicles to offer riding to help UAVs save energy and expand the operation radius. We present the first theoretical study regarding how UAVs should optimally hitch on ground vehicles, considering vehicles' different travelling patterns and supporting capabilities. For a single UAV, we derive closed-form optimal vehicle selection and hitching strategy. When vehicles only support hitching, a UAV would prefer the vehicle that can carry it closest to its final destination. When vehicles can offer hitching plus charging, the UAV may hitch on a vehicle that carries it farther away from its destination and hitch a longer distance. The UAV may also prefer to hitch on a slower vehicle for the benefit of battery recharging. For multiple UAVs in need of hitching, we develop the max-saving algorithm (MSA) to optimally match UAV-vehicle collaboration. We prove that the MSA globally optimizes the total hitching benefits for the UAVs.
\subsubsection{Selective Federated Learning for On-Road Services in Internet-of-Vehicles}
abstract:The Internet-of-Vehicles (IoV) can make driving safer and bring more services to smart vehicle (SV) users. Specifically, with IoV, the road service provider (RSP) can collaborate with SVs to provide high-accurate on-road information-based services by implementing federated learning (FL). Nonetheless, SVs' activities are very diverse in IoV networks, e.g., some SVs move frequently while other SVs are occasionally disconnected from the network. Consequently, obtaining information from all SVs for the learning process is costly and impractical. Furthermore, the quality-of-information (QoI) obtained by SVs also dramatically varies. That makes the learning process from all SVs simultaneously even worse when some SVs have low QoI. In this paper, we propose a novel selective FL approach for an IoV network to address these issues. Particularly, we first develop an SV selection method to determine a set of active SVs based on their location significance. In this case, we adopt a K-means algorithm to classify significant and insignificant areas where the SVs are located according to the areas' average annual daily flow of vehicles. From the set of SVs in the significant areas, we select the best SVs for the FL execution based on the SVs' QoI at each learning round. Through simulation results using a real-world on-road dataset, we observe that our proposed approach can converge to the FL results even with only 10% of active SVs in the network. Moreover, our results reveal that the RSP can optimize on-road services with faster convergence up to 63% compared with other baseline FL methods.
\subsubsection{TruClu: Trust Based Clustering Mechanism in Software Defined Vehicular Networks}
abstract:Vehicular ad hoc Networks (VANETs) have emerged as a powerful technology for enabling user applications on moving vehicles. However, maintaining acceptable levels of Quality of Service and message latency is a challenging task. A number of solutions have been proposed for improving performance of these networks. Clustering is also a popular mechanism that is used for improving the performance of vehicular networks. Recently other alternatives based on integration of different computing paradigms such as Software Defined Networks with traditional VANETs are also being explored. In liue of this, TruClu: a trust based clustering mechanism which creates vehicular clusters for a Software Defined Vehicular Network is proposed. Cluster formation and cluster head selection in TruClu is done on the basis of vehicular mobility along with trust value of vehicles as the main parameters which helps in accomplishing seamless data connectivity between vehicles. The performance of TruClu is evaluated through extensive simulations and obtained results indicate the comparable performance of the proposed scheme in terms of standard performance parameters.
\subsubsection{Resource Allocation for Low-Latency NOMA-Enabled Vehicle Platoon-Based V2X System}
abstract:As an essential role in the intelligent transportation system, the vehicle platoon-based system meets tremendous challenges in guaranteeing low-latency communication for vehicles with dynamic channel information and high mobility. To handle the challenges, non-orthogonal multiple access (NOMA) has been considered as a promising candidate to improve the system capacity and spectrum efficiency. However, it is still an open issue on how to organize multiple transmission links with suitable resource allocation. In this paper, we investigate the problem of the resource allocation for the NOMA-enabled vehicle platoon-based vehicle-to-everything (V2X) system. First, an optimization problem is formulated to jointly consider the resource allocation on vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) links, while satisfying the quality-of-service (QoS) requirements for each vehicle, including the delay requirements, rate demands, and power constraints. To cope with the nonconvex problem, a fractional programming-based transformation is used, and an iterative resource allocation algorithm is proposed to find the solution. The numerical results indicate that our proposed algorithm can significantly reduce the system delay compared with other methods while satisfying the QoS requirements, so as to tackle the latency issues for vehicle platoon-based V2X communications.
\subsubsection{High Altitude Platform Station (HAPS) Assisted Computing for Intelligent Transportation Systems}
abstract:High altitude platform station (HAPS) computing can be considered as a promising extension of edge computing to improve intelligent transportation systems (ITS). HAPS is deployed in the stratosphere to provide wide coverage and strong computational capabilities, which is suitable to coordinate terrestrial resources and store the fundamental data associated with ITS-based applications. In this work, three computing layers, i.e., vehicles, terrestrial network edges, and HAPS, are integrated to build a computation framework for ITS, where the HAPS data library stores the fundamental data needed for the applications. In addition, the caching technique is introduced for network edges to store some of the fundamental data from the HAPS so that large propagation delays can be reduced. We aim to minimize the delay of the system by optimizing computation offloading and caching decisions as well as bandwidth and computing resource allocations. The simulation results highlight the benefits of HAPS computing for mitigating delays and the significance of caching at network edges.
\section{None}
\section{Optical Networks and Systems}
\subsection{Optical Network Optimisation I}
\subsubsection{Strategies for Dedicated Path Protection in Filterless Optical Networks}
abstract:Enabling Dedicated Path Protection (DPP) in Filterless Optical Networks (FONs) poses specific design challenges, as FONs require dividing the network topology in non-overlapping fiber trees, and lightpaths cannot cross from one tree to another unless additional devices are installed. In this study, we consider the possibility to deploy three type of devices, namely Inter-Tree Transceivers (ITTs), Wavelength Blockers (WBs) and Colored Passive Filters (CPFs) to achieve DPP in FON, and we compare the three resulting DPP strategies, called P-ITT, P-WB and P-WBC. More specifically, we formulate three Integer Linear Programming (ILP) models for DPP in FON with the objective to minimize additional device cost and minimize total wavelength consumption. Numerical results over two realistic topologies show that P-WBC achieves cost savings up to 33% in comparison to P-WB and up to 97% in comparison to P-ITT. However, even if it is the costliest approach, P-ITT ensures up to 7% savings in wavelength consumption and up to 23% savings in resource overbuild compared to P-WB and P-WBC, making it a possible candidate in spectrum-scarce deployments.
\subsubsection{Edge-cloud Collaborative Heterogeneous Task Scheduling in Multilayer Elastic Optical Networks}
abstract:With the explosive growth of edge applications in the 5G/B5G era, edge-cloud collaboration (ECC) is playing a prominent role in edge service provisioning. For highly diversified edge-cloud collaborative services (ECSs), the joint allocation of heterogeneous computing resources in heterogeneous servers and multi-dimensional underlying optical network resources should be conducted. In this paper, we investigate the heterogeneous task scheduling for ECSs over multilayer elastic optical network (ML-EON), which involves the joint allocation of heterogeneous computing resources in edge and cloud servers and high-dimensional network resources. We propose a Task-Node Matching Score (TNMS) based method, which evaluates the fitness for each mapping tuple between each task in ECS and each substrate node in ML-EON, and adaptively generates a specific matching score for each task-node pair. Furthermore, TNMS is extended with a pre-allocation mechanism (TNMS-Pre) to estimate the costs of multi-dimensional resources in ML-EON for virtual link (VL) mapping. The estimated VL mapping costs are integrated into the matching scores to guide the task placement to be cost-efficient. To guarantee the feasibility, a maximal weight matching (MWM) based method is presented to determine the task placement schemes. Simulation results demonstrate the effectiveness of the adaptive scoring for heterogeneous task placement and the pre-allocation mechanism for reducing the ML-EON costs.
\subsubsection{Dynamic Bandwidth Allocation for PON Slicing with Performance-Guaranteed Online Convex Optimization}
abstract:The emergence of diverse network applications demands more flexible and responsive resource allocation for networks. Network slicing is a key enabling technology that provides each network service with a tailored set of network resources to satisfy specific service requirements. The focus of this paper is the network slicing of access networks realized by Passive Optical Networks (PONs). This paper proposes a learning-based Dynamic Bandwidth Allocation (DBA) algorithm for PON access networks, considering slice-awareness, demand-responsiveness, and allocation fairness. Our online convex optimization-based algorithm learns the implicit traffic trend over time and determines the most robust window allocation that reduces the average latency. Our simulation results indicate that the proposed algorithm reduces the average latency by prioritizing delay-sensitive and heavily-loaded ONUs while guaranteeing a minimal window allocation to all ONUs.
\subsubsection{A Reinforcement Learning-Based Admission Control Strategy for Elastic Network Slices}
abstract:This paper addresses the problem of admission control for elastic network slices that may dynamically adjust provisioned bandwidth levels over time. When admitting new slice requests, sufficient spare capacity must be reserved to allow existing elastic slices to dynamically increase their bandwidth allocation when needed. We demonstrate a lightweight deep Reinforcement Learning (RL) model to intelligently make admission control decisions for elastic slice requests and inelastic slice requests. This model achieves higher revenue and higher acceptance rates compared to traditional heuristic methods. Due to the lightness of this model, it can be deployed without GPUs. We can also use a relatively small amount of data to train the model and to achieve stable performance. Also, we introduce a Recurrent Neural Network to encode the variable-size environment and train the encoder with the RL model together.
\subsubsection{Design of Survivable Metro-Aggregation Networks based on Digital Subcarrier Routing}
abstract:The ever-increasing traffic requirements need cost-efficient and reliable solutions. Metropolitan networks are responsible for a large fraction of the telecom infrastructure CAPEX. Albeit the traffic pattern in these networks tends to be hub-and-spoke, the current transceiver deployments do not leverage this feature. This paper describes and models various survivable metro-aggregation network scenarios using point-to-multipoint capable coherent transceivers. The results obtained over a reference mesh network show that these transceivers can reduce total transceiver costs by ~40% on average.
\subsection{Optical Network Optimisation II}
\subsubsection{Delay-aware Wireless Resource Allocation and User Association in LiFi-WiFi Heterogeneous Networks}
abstract:The ever-growing wireless networks demand high capacity, have strict latency requirements, and must support diverse communication services. A LiFi-WiFi heterogeneous network has proven to be a useful tool to satisfy the growing capacity demand. However, to leverage these co-existing, non-interfering technologies, intelligent resource management schemes have to be developed. To support diverse applications with varying delay and data rate requirements, the resource management scheme should consider the Quality of Service (QoS) while allocating wireless resources. In this work, the downlink wireless resources are allocated to users such that the average network packet delay is minimized. Users that are both capable and not capable of multi-homing are considered and a separate optimization problem is formulated for each case. These problems are then solved using a global Branch and Bound-based solver and a genetic algorithm-based Metaheuristic is also proposed. The algorithms are then evaluated with simulations and the results show that the average network packet delay is significantly lowered and each user's strict QoS requirements are satisfied even in a network with heavy traffic flow.
\subsubsection{Optimal QoS-Aware Allocation of Virtual Network Resources to Mixed Mobile-Optical Network Slices}
abstract:Slicing allows 5G networks to accommodate services with different needs over a unified physical network infrastructure. Particularly, in radio access networks (RANs) the baseband processing functions of different slices are treated as virtual applications running on shared compute nodes. Managing this cloud-native network in a cost-effective way requires tightly coupled control of connectivity and processing resources. This paper proposes an optimization problem to minimize the overall cost while guaranteeing distinct latency and reliability requirements of different network slices deployed over the infrastructure. This is achieved by choosing for each service, the functional split and transmission parameters that best adapt to the connectivity and processing resources available in the infrastructure. Results demonstrate that the proposed flexible resource allocation scheme provides considerable cost saving (up to 2.4 times) and reduced request blocking (up to 2 times) compared to conventional fixed deployment techniques. The reliability requirements can be guaranteed by packet duplication and/or virtualized forward error correction at the expense of consuming connectivity and/or processing resources, respectively. The flexible assurance of the reliability requirements in the proposed scheme contributes considerably to the achieved improvements on cost saving and request blocking.
\subsubsection{Parameterized Exhaustive Routing with First Fit for RSA Problem Variants}
abstract:We present a new single-step solution approach for the routing andspectrum allocation (RSA) problem that integrates the first-fit (FF) heuristic with a new routing strategy that we refer to as "parameterized exhaustive routing." Our approach is to explore the whole routing space for a subset of the traffic requests, e.g., those with the largest demands or those of higher priority or importance. For each of the remaining requests we employ a greedy heuristic to select one of the candidate paths jointly with spectrum allocation. Our solution represents a two-parameter family of algorithms that bridges the gap between an exhaustive search of the routing space and current two-step methodologies for the RSA problem that select paths for each traffic request in isolation. The parameter values may be used to trade off the quality of the final solution and the computational requirements. Our results indicate that exploring the joint routing space of even a few large requests leads to better solutions than purely greedy approaches.
\subsubsection{Latency-aware VNF Protection for Network Function Virtualization in Elastic Optical Networks}
abstract:In network function virtualization (NFV), the customer may request a set of virtual network functions (VNFs) that the customer traffic will go through. To accommodate such requests, the service providers have to embed the requested VNFs onto the substrate network (SN) to form an actual traffic forwarding path called service function path (SFP). In the elastic optical network (EON), how to protect the running network services against VNF failures becomes an attractive research focus.Most existing work concentrates on the protection or restoration of the physical node or fiber link failures in the SN.Few research attention has been paid to the failure of the virtual machines running the VNF. In this paper, we study how to protect VNFs when a VNF failure occurs in an EON. We define a new VNF failure protection cover (VFPC) problem and mathematically formulate VFPC.We propose the protection cover list based VNF protection (PCL-VP) algorithm against any single VNF failure while satisfying the latency requirement. Extensive simulations and analysis show that the proposed algorithm outperforms the benchmarks.
\subsection{Optical Transmission Systems and Processing}
\subsubsection{Machine Learning Driven Model for Software Management of Photonics Switching Systems}
abstract:Modern elastic optical networking requires additional flexibility at each layer compared to the traditional approach. The application of the Software-defined Networking (SDN) paradigm can provide the required degrees of freedom. The implementation of optical SDN down to the physical layer requires the complete abstraction of network elements to support full control by the centralized controller. In this work, we propose a topological and technological agnostic model based on Machine Learning (ML) to abstract the behavior of optical switches for the computation of Quality-of-transmission (QoT) penalties and the definition of control states. Training and testing datasets are obtained synthetically by software simulation of the photonic switching structure. Results show the capability of the proposed method to predict QoT impairments with high accuracy, and we envision its application in a real-time control plane.
\subsubsection{A Real-Time OSNR Penalty Estimator Engine in the Presence of Cascaded WSS Filters}
abstract:Virtual Optical Networks (VONs) offer scientists the opportunity to test and validate Software Defined Networking (SDN) controllers without requiring access to expensive optical equipment.A key component in VON is the real-time optical layer emulation (OLE) engine that must estimate the OSNR at the virtual receiver of every virtual lightpath that is established in the VON.This study investigates two algorithms for training a two-hidden layer Neural Network (NN)whose aim is to estimate the OSNR penalties caused by a series ofvirtual Wavelength Selective Switches (WSS) that are traversed by a virtual lightpath before reaching its virtual receiver.The NN solution yields good accuracy and only requires a few microseconds of computation time - a two key requirements for achieving a meaningful OLE engine.
\subsubsection{DeepDRAMA: Deep Reinforcement Learning-based Disaster Recovery with Mitigation Awareness in EONs}
abstract:Elastic Optical Networks (EONs) have become a promising solution to satisfy the dramatic growth of demand in 5G and cloud applications. Due to the flexibility of resource allocation, EONs provide high spectrum utilization efficiency, and because of this, developing efficient policies to ensure the survivability of EONs is a challenging problem. A well-designed disaster management plan is needed to prevent data loss during network failures and large-scale disasters. The bottleneck problem caused by disabled parts of the network causes difficulties for disaster recovery. Depending on the disaster, even traffic that may be far away from the disaster may be impacted by the disaster. In this paper, we propose a new approach to disaster management using machine learning to facilitate efficient recovery. In addition to traffic immediately affected by the disaster, all traffic which are ''close to'' the disaster is re-routed and re-assigned with possibly degraded service, while requests ''far from'' the disaster are left unaffected. A deep reinforcement learning disaster recovery algorithm with mitigation awareness (DeepDRAMA) is proposed for recovery. A novel deep reinforcement learning agent is designed and trained for the agent to select the appropriate level of service degradation for re-assigned traffic. Simulation results show the performance improvement with DeepDRAMA.
\subsubsection{Dual-Hop Full-Duplex DF Relaying with Parallel Hybrid RF/FSO Links}
abstract:In this paper, we carry out a performance analysis of a full-duplex (FD) relaying system consisting of parallel hybrid radio frequency (RF)/free-space optical (FSO) communication links. The RF links are hampered by the residual self-interference (RSI), due to the FD relaying operation, along with the in-phase and quadrature-phase imbalance (IQI) effect, due to imperfections at the RF nodes' front-ends. The parallel FSO links, of the dual-hop configuration, are influenced by the joint effects of atmospheric turbulence and pointing errors. The performance of the dual-hop FD system with parallel hybrid RF/FSO links, operating under a hard-switching scheme, is evaluated in terms of the outage probability. Analytical closed-form expressions are derived for both RF and FSO subsystems as well as for the overalldual-hop hybrid system. The presented numerical results show the significant performance gains obtained by the exploitation of parallel RF/FSO links in an FD relaying channel under various operating conditions. Finally, the derived analytical results are verified by Monte Carlo simulations.
\subsection{Optical Wireless Communications}
\subsubsection{Modelling of Multi-Tier Handover in LiFi Networks}
abstract:This paper presents a two-tier LiFi network and analyses the cross-tier handover rate between the primary and secondary cells. Based on the semiangle at half illuminance of the primary and secondary cells, we propose coverage model for the secondary cells. Using stochastic geometry, closed-form expressions are derived for the cross-tier handover rate and sojourn time in terms of the received optical signal intensity, time-to-trigger and user mobility. The analytical models are validated with simulation results.
\subsubsection{Reliable Optical Receiver for Highly Dynamic Wireless Channels: An Experimental Demonstration}
abstract:Optical wireless communication (OWC) has attracted more and more attention in recent decades. One promising method of improving the sensitivity of the receivers in OWC system is the utilization of the single-photon avalanche diode (SPAD). However, due to the non-linear effects induced by the dead time, the performance of SPAD-based receivers is significantly degraded in high incident power regime. Recently, we proposed a novel hybrid SPAD/PD receiver in which the receiver can adaptively switch between the SPAD- and PD-mode based on the instantaneous received optical power. In addition, a variable optical attenuator (VOA) is applied to optimize the performance of the SPAD unit. In this work, the superiority of the proposed hybrid receiver is experimentally demonstrated. For a data rate transmission of 450 Mbps, it is demonstrated that the proposed receiver significantly outperforms the traditional PD and SPAD array receiver in terms of the BER performance and can greatly extend the receiver dynamic range to reliably operate at both extremes of the incident light levels.
\subsubsection{Avoiding Inter-Light Sources Interference in Optical Camera Communication}
abstract:Optical Camera Communication (OCC) is a promising solution for future wireless communication thanks to the advantages including security, license, and cost-efficiency. Widely available smart devices with embedded cameras such as smartphones, tablets, and digital cameras can be employed as receivers in OCC without modifying hardware. A complementary metal-oxide-semiconductor (CMOS) sensor in a camera can receive optical signals from multiple light sources at the same time. Since the light source occupies a certain area in the image plane, the received signal powers differ among the corresponding pixels. When the number of light sources increase, the signals from a light source can be blocked by another light source or affected by the blooming effect of other optical signals. However, there has never been a general model for avoiding such interference in OCC. Therefore, in this paper we propose a general model for avoiding inter-light sources interference. The proposed model formulates the constraints with perspective transformation based on the parameters of an image sensor and a camera. We also provide preliminary experimental results to validate the proposed model.
\subsubsection{Resource Allocation in Laser-based Optical Wireless Cellular Networks}
abstract:Optical wireless communication provides data transmissionat high speeds which can satisfy the increasing demandsfor connecting a massive number of devices to the Internet. In thispaper, vertical-cavity surface-emitting(VCSEL) lasers are usedas transmitters due to their high modulation speed and energyefficiency. However, a high number of VCSEL lasers is requiredto ensure coverage where each laser source illuminates a confinedarea. Therefore, multiple users are classified into different setsaccording to their connectivity. Given this point, a transmissionscheme that uses blind interference alignment (BIA) is implementedto manage the interference in the laser-based network.In addition, an optimization problem is formulated to maximizethe utility sum rate taking into consideration the classificationof the users. To solve this problem, a decentralized algorithm isproposed where the main problem is divided into sub-problems,each can be solved independently avoiding complexity. The resultsdemonstrate the optimality of the decentralized algorithm wherea sub-optimal solution is provided. Finally, it is shown that BIAcan provide high performance in laser-based networks comparedwith zero forcing (ZF) transmit precoding scheme.
\subsection{Visible Light Communications}
\subsubsection{Visible Light Communications: A Novel Indoor Network Planning Approach}
abstract:We propose a Partition-based Visibility (PV) graph modeling to find the minimum number of Visible Light Communications (VLC) nodes and their precise locations for a reliable indoor coverage. VLC network offers a low-cost technology on a license-free spectrum to complement the contemporary mobile network services on Radio Frequency (RF) bands. However, VLC suffers from propagation limits; Firstly, in presence of opaque obstacles such as walls, doors, and even curtains, strong link blockage is experienced as the power of reflections is much weaker than the power of the Line-of-Sight (LoS) link. Secondly, the received optical power at users drops as the angle of irradiance between a LED lamp and the user increases, imposing a range constraint on VLC nodes. So, inspired by the Art Gallery Problem, we optimize the number and locations of VLC nodes by characterizing the PV graph as a dual presentation of the floor plan and a Maximal Clique Clustering algorithm able to solve both unconstrained and range constrained art gallery problem.
\subsubsection{Single LED Gbps Visible Light Communication with Probabilistic Shaping}
abstract:Using a single low power LED, we present a probabilistic shaped (PS) VLC system with near Shannon capacity transmission rates. For the channel conditions under consideration and a single 21 MHz -3 dB modulation bandwidth LED, probabilistic shaped symbols resulted in Gbps transmission rates. Comparatively, the PS resulted in above 27 % higher transmission speed than the widely used adaptive bit-power loading algorithm under the same channel conditions.
\subsubsection{Indoor monitoring system based on ARQ signaling generated by a Visible Light Communication link}
abstract:Visible Light Communications (VLC) is a candidate technology that complements the benefits of radio communication networks, particularly in those situations where a low-cost solution using license-free spectrum is required to enable ultra-dense deployments of indoor small cells. However, the main drawback that VLC has when compared to wireless communications on RF bands is that, in presence of obstacles between the transmitter and the receiver, full-blockage events are likely to happen as the power received on reflections is much weaker than the power of the blocked line-of-sight link. In this paper, we take advantage of this phenomenon and study the effect that different activities performed by people in the service area to-be-monitored have on the Automatic Repeat reQuest (ARQ) signaling of the VLC link. Based on the presented experimental studies, it is possible to conclude that different relevant events are able to be detected correctly according to the statistics of the ARQ signaling that is collected from the ongoing VLC transmission.
\subsubsection{Quad-LED OTFS Modulation in Indoor Visible Light Communication Systems}
abstract:In this paper, we investigate orthogonal time frequency space (OTFS) modulation in multi-LED indoor visible light wireless communications. Specifically, we propose two quad-LED OTFS schemes, namely, 1) quad-LED complex modulation OTFS (QCM-OTFS), and 2) spatial modulation dual-LED complex modulation OTFS (SM-DCM-OTFS). The proposed QCM-OTFS scheme sends the magnitudes of real and imaginary parts of complex signals through intensity modulation (IM) and their sign information through spatial indexing of LEDs. The proposed SM-DCM-OTFS scheme sends the magnitude and phase of complex signals (polar representation) through a pair of LEDs and frame indexing across two pairs of LEDs. The proposed schemes do not require Hermitian symmetry and DC bias operations to obtain real positive valued signals suited for IM of LEDs. We obtain upper bounds on the bit error rate (BER) performance of the proposed schemes, which are tight at high signal-to-noise ratios (SNR). Compared to the QCM-OFDM and SM-DCM-OFDM schemes known in the literature, the proposed schemes achieve significantly better BER performance. We also analyze the spatial distribution of the SNR gain in the proposed QCM-OTFS/SM-DCM-OTFS schemes compared to QCM-OFDM/SM-DCM-OFDM schemes using the ratio of minimum distance of normalized received signal sets as a metric.
\subsubsection{Physical Layer Security Optimization for MIMO Enabled Visible Light Communication Networks}
abstract:This paper investigates the optimization of physical layer security in multiple-input multiple-output (MIMO) enabled visible light communication (VLC) networks. In the considered model, one transmitter equipped with light-emitting diodes (LEDs) intends to send confidential messages to legitimate users while one eavesdropper attempts to eavesdrop on the communication between the transmitter and legitimate users. This security problem is formulated as an optimization problem whose goal is to minimize the sum mean-square-error (MSE) of all legitimate users while meeting the MSE requirement of the eavesdropper thus ensuring the security. To solve this problem, the original optimization problem is first transformed to a convex problem using successive convex approximation. An iterative algorithm with low complexity is proposed to solve this optimization problem. Simulation results show that the proposed algorithm can reduce the sum MSE of legitimate users by up to 40% compared to conventional zero forcing scheme.
\section{Selected Areas in Communications: Machine Learning for Communications}
\subsection{AI-based channel estimation I}
\subsubsection{An Attention-Aided Deep Neural Network Design for Channel Estimation in Massive MIMO Systems}
abstract:Channel estimation is one of the key issues in practical massive multiple-input multiple-output (MIMO) systems. Compared with conventional estimation algorithms, deep learning-based designs have exhibited great potential in terms of both performance and complexity. In this paper, an attention-aided deep neural network is proposed for channel estimation in hybrid analog-digital massive MIMO systems. Specifically, the integrated attention mechanism automatically realizes the "divide-and-conquer" policy to exploit the distribution characteristics of highly separable channels with narrow angular spread. Simulation results show that the channel estimation performance is significantly improved with the aid of attention at the cost of small complexity overhead, and the strong robustness further strengthens the practical value of the proposed approach. Moreover, the distributions of learned attention maps are also investigated to reveal the role of attention and endow the proposed approach with a certain degree of interpretability.
\subsubsection{Online DNN-based Channel Estimator for Massive MIMO Systems with Nonlinear Distortion}
abstract:In this paper, we propose a two-stage deep neural network (DNN)-based channel estimator for massive multiple-input multiple-output (MIMO) systems with nonlinear amplifier distortions. The proposed two-stage structure is able to jointly learn a DNN-based channel estimator and the nonlinear transfer functions online based on real-time received pilot measurements, while generating channel estimates (CE) simultaneously. This is realized by a careful design of online loss function that does not depend on ground truth channels while taking into account the unknown nonlinear distortions. Simulation shows that the proposed scheme outperforms the traditional compressive-sensing (CS) algorithms in terms of CE accuracy, and enjoys a much faster computational time during channel inferencing. The proposed scheme is also robust to various model mismatches and can adapt to the change of underlying channel model.
\subsubsection{MTCNet: Multi-Task Complex Network for Concurrent Channel Estimation and Equalization}
abstract:Convolutional neural networks (CNNs) have been widely adopted in various fields and the wireless communication domain is no exception; researchers leveraged CNNs for communication tasks such as channel estimation and equalization. They showed the potential of CNNs for these applications but were insufficient to be considered for real world implementation due to the limited applicability and high execution latency. In this paper, we propose a novel complex number-based neural network architecture, a multi-task loss function, and a training scheme to learn channel estimation and equalization tasks simultaneously in an end-to-end manner. The resulting single-stage CNN architecture, Multi-Task Complex Network (MTCNet), achieves better performance than any other conventional methods. In addition, to the best of our knowledge, MTCNet is the first CNN-based channel estimation and equalization that can operate with various subcarrier settings with inferencing time below 1 ms.
\subsubsection{Domain Knowledge Aided Neural Network for Wireless Channel Estimation}
abstract:Channel estimation for Orthogonal Frequency Division Multiplexing (OFDM) transmission is well investigated with model-based approaches. Recent effort also explores the data driven approaches to exploit the capabilities of Neural Networks (NNs) to estimate the channel. These models are mostly being developed as black box without any anchor to the theory of wireless signal propagation. We propose a NN model, where the structure and parameters are derived from the domain knowledge of wireless signal and channel characteristics. Our model is developed in two stages: the first stage handles the noise reduction, while the second stage extracts the channel characteristics to reduce error caused by multipath. We have used the knowledge of signal to noise ratio and subcarrier correlation due to channel delay spread to empower the two stages of the proposed model. Our results also show that induction of domain knowledge results in reduction of data dependency by 60%. Our model outperforms the practical model based methods as well as blind data driven approaches. It achieves 10 dB improvement over Least Square estimate, which is most commonly implemented.
\subsubsection{Neural Calibration for Scalable Beamforming in FDD Massive MIMO with Implicit Channel Estimation}
abstract:Channel estimation and beamforming play critical roles in frequency-division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems. However, these two modules have been treated as two stand-alone components, which makes it difficult to achieve a global system optimality. In this paper, we propose a deep learning-based approach that directly optimizes the beamformers at the base station according to the received uplink pilots, thereby, bypassing the explicit channel estimation. Different from the existing fully data-driven approach where all the modules are replaced by deep neural networks (DNNs), a neural calibration method is proposed to improve the scalability of the end-to-end design. In particular, the backbone of conventional time-efficient algorithms, i.e., the least-squares (LS) channel estimator and the zero-forcing (ZF) beamformer, is preserved and DNNs are leveraged to calibrate their inputs for better performance. The permutation equivariance property of the formulated resource allocation problem is then identified to design a low-complexity neural network architecture. Simulation results will show the superiority of the proposed neural calibration method over benchmark schemes in terms of both the spectral efficiency and scalability in large-scale wireless networks.
\subsection{AI-based channel estimation II}
\subsubsection{SubSRNN: Tailored Neural Network for Channel Estimation with Robustness against Diversities}
abstract:The advance of deep learning in computer vision has been leveraged for channel estimation in radio receiver since high-resolution full channel response can be reconstructed from raw channel estimation achieved based on sparse pilots. Despite that significant performance gains have been derived by using powerful neural networks (NN) especially in given channel conditions, it is still challenging and crucial to augment NN's robustness in untrained scenarios in terms of user equipments' (UE) locations and velocities. In this paper, we proposed a super-resolution (SR) NN as the backbone structure for high-resolution channel response reconstruction from low-resolution raw estimation based on sparse pilots. On top of that, a specialized sub NN structure is embedded to adaptively combat against variant Dopplers induced diverse phase rotations between orthogonal frequency division multiplexing (OFDM) symbols in time domain. The Sub-NN with semantic information like UEs' velocities as the input can learn multi-path-propagation dependent Doppler decomposition, thus compensates the phase rotation. For the first time, we show how such specially tailored NN with extra information can be used for high-accuracy channel estimation using only 1 DMRS, which is robust to locations and Doppler scenarios that have not even been trained.
\subsubsection{Deep Autoencoder-based Massive MIMO CSI Feedback with Quantization and Entropy Coding}
abstract:Techniques which leverage channel state information (CSI) at a transmitter to adapt wireless signals to changing propagation conditions have been shown to improve the reliability of modern multiple input multiple output (MIMO) communication systems. To reduce overhead, previous works have proposed to compress CSI matrices using a trained deep autoencoder (AE) at the receiver before feeding it back to the transmitter, and recent work has proposed to quantize and perform entropy coding on the compressed CSI to further reduce communication complexity. While these methods are effective, they either do not incorporate quantization and lossless coding into their end-to-end optimization, or do not achieve performance comparable to methods that do not use quantization and entropy coding. In this work, we propose a new AE-based feedback method which uses an entropy bottleneck layer to both quantize and losslessly code the compressed CSI. This bottleneck layer allows us to jointly optimize bit-rate and distortion to achieve a highly-compressed CSI representation which preserves important channel information. Our method achieves better reconstruction quality than existing autoencoder-based CSI feedback methods for a wide range of bit-rates on simulated data, in both indoor and outdoor wireless settings.
\subsubsection{Learning-aided joint time-frequency channel estimation for 5G new radio}
abstract:In this paper, we propose a learning-aided signal processing solution for channel estimation in 5G new radio (NR). Channel estimation is an important algorithm for baseband modem design. In 5G NR, estimating the channel is challenging due to two reasons. First, the pilot signals are transmitted over a small fraction of the available time-frequency resources. Second, the real time nature of physical layer processing introduces a strict limitation on the computational complexity of channel estimation. To this end, we propose a channel estimation technique that integrates a small one hidden layer neural network between two linear minimum mean squared error (LMMSE) interpolation blocks. While the neural network leverages the advantages of offline data-driven learning, the LMMSE blocks exploit the second order online channel statistics along time and frequency dimensions. The training procedure tunes the weights of the neural network by back-propagating through the time domain LMMSE interpolation block. We derive bounds on the training loss with the proposed method and show that our approach can improve the channel estimate.
\subsubsection{Neural Augmentation of Kalman Filter with Hypernetwork for Channel Tracking}
abstract:We propose Hypernetwork Kalman Filter (HKF) for tracking applications with multiple different dynamics. The HKF combines generalization power of Kalman filters with expressive power of neural networks. Instead of keeping a bank of Kalman filters and choosing one based on approximating the actual dynamics, HKF adapts itself to each dynamics based on the observed sequence. Through extensive experiments on CDL-B channel model, we show that the HKF can be used for tracking the channel over a wide range of Doppler values, matching Kalman filter performance with genie Doppler information. At high Doppler values, it achieves around 2dB gain over genie Kalman filter. The HKF generalizes well to unseen Doppler, SNR values and pilot patterns unlike LSTM, which suffers from severe performance degradation.
\subsubsection{Temporal Averaging LSTM-based Channel Estimation Scheme for IEEE 802.11p Standard}
abstract:In vehicular communications, reliable channel estimation is critical for the system performance due to the doubly-dispersive nature of vehicular channels. IEEE 802.11p standard allocates insufficient pilots for accurate channel tracking. Consequently, conventional IEEE 802.11p estimators suffer from a considerable performance degradation, especially in high mobility scenarios. Recently, deep learning (DL) techniques have been employed for IEEE 802.11p channel estimation. Nevertheless, these methods suffer either from performance degradation in very high mobility scenarios or from large computational complexity. In this paper, these limitations are solved using a long short term memory (LSTM)-based estimation. The proposed estimator employs an LSTM unit to estimate the channel, followed by temporal averaging (TA) processing as a noise alleviation technique. Moreover, the noise mitigation ratio is determined analytically, thus validating the TA processing ability in improving the overall performance. Simulation results reveal the performance superiority of the proposed schemes compared to recently proposed DL-based estimators, while recording a significant reduction in the computational complexity.
\subsection{AI-based localization techniques}
\subsubsection{WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels}
abstract:We introduce WiCluster, a new machine learning (ML) approach for passive indoor positioning using radio frequency (RF) channel state information (CSI). WiCluster can predict both a zone-level position and a precise 2D or 3D position, without using any precise position labels during training. Prior CSI-based indoor positioning work has relied on non-parametric approaches using digital signal-processing (DSP) and, more recently, parametric approaches (e.g., fully supervised ML methods). However these do not handle the complexity of real-world environments well and do not meet requirements for large-scale commercial deployments: the accuracy of DSP-based method deteriorates significantly in non-line-of-sight conditions, while supervised ML methods need large amounts of hard-to-acquire centimeter accuracy position labels.In contrast, WiCluster is both precise and requires weaker label-information that can be easily collected. Our first contribution is a novel dimensionality reduction method for charting. It combines a triplet-loss with a multi-scale clustering-loss to map the high-dimensional CSI representation to a 2D/3D latent space. Our second contribution is two weakly supervised losses that map this latent space into a Cartesian map, resulting in meter-accuracy position results. These losses only require simple to acquire priors: a sketch of the floorplan, approximate location of access-point locations and a few CSI packets that are labeled with the corresponding zone in the floorplan.Thirdly, we report results and a robustness study for 2D positioning in a single-floor office building and 3D positioning in a two-floor home to show the robustness of our method.
\subsubsection{LTE Device Identification Based on RF Fingerprint with Multi-Channel Convolutional Neural Network}
abstract:Radio frequency fingerprint (RFF) identification technique has drawn great attention to wireless terminal authentication. Long-Term Evolution (LTE) has been widely deployed all over the world. RFF-based LTE terminal identifications can prevent the potential impersonation or denial of service (DoS) attacks in the physical layer. This paper proposes a novel multi-channel convolutional neural network (MCCNN) for LTE terminal identification. Differential constellation trace figure (DCTF) is extracted from the random access preamble of the physical random access channel (PRACH). To the best knowledge of the authors, this is the first work dedicated to RFF-based LTE terminal identification. The proposed scheme is evaluated in the hardware experimental system consisting of the LTE eNodeB implemented on the software-defined radio (SDR) platform and six LTE mobile phones. Experimental results show that the classification accuracy can reach 98.96% at the SNR level of 30 dB with the line-of-sight (LOS) scenarios. Furthermore, long-time evaluations show that the proposed DCTF-MCCNN scheme is robust over time.
\subsubsection{Open Set RF Fingerprinting using Generative Outlier Augmentation}
abstract:RF devices can be identified by unique imperfections embedded in the signals they transmit called RF fingerprints. The closed set classification of such devices, where the identification must be made among an authorized set of transmitters, has been well explored. However, the much more difficult problem of open set classification, where the classifier needs to reject unauthorized transmitters while recognizing authorized transmitters, has only been recently visited. So far, efforts at open set classification have largely relied on the utilization of signal samples captured from a known set of unauthorized transmitters to aid the classifier learn unauthorized transmitter fingerprints. Since acquiring new transmitters to use as known transmitters is highly expensive, we propose to use generative deep learning methods to emulate unauthorized signal samples for the augmentation of training datasets. We develop two different data augmentation techniques, one that exploits a limited number of known unauthorized transmitters and the other that does not require any unauthorized transmitters. Experiments conducted on a dataset captured from a WiFi testbed indicate that data augmentation allows for significant increases in open set classification accuracy, especially when the authorized set is small.
\subsubsection{EPC-TE: Explicit Path Control in Traffic Engineering with Deep Reinforcement Learning}
abstract:Selected Areas in Communications: Machine Learning for Communications
\subsection{Beamforming}
\subsubsection{Scalable Power Control/Beamforming in Heterogeneous Wireless Networks with Graph Neural Networks}
abstract:Machine learning (ML) has been widely used for efficient resource allocation (RA) in wireless networks. Although superb performance is achieved on small and simple networks, most existing ML-based approaches are confronted with difficulties when heterogeneity occurs and network size expands. In this paper, specifically focusing on power control/beamforming (PC/BF) in heterogeneous device-to-device (D2D) networks, we propose a novel unsupervised learning-based framework named heterogeneous interference graph neural network (HIGNN) to handle these challenges. First, we characterize diversified link features and interference relations with heterogeneous graphs. Then, HIGNN is proposed to empower each link to obtain its individual transmission scheme after limited information exchange with diversified neighboring links. It is noteworthy that HIGNN is scalable to wireless networks of growing sizes with robust performance after trained on small-sized networks. Numerical results show that compared with state-of-the-art benchmarks, HIGNN achieves much higher execution efficiency while providing strong performance.
\subsubsection{Learning Probing Beams for Fast mmWave Beam Alignment}
abstract:Beam alignment - the process of finding an optimal directional beam pair - is a challenging procedure crucial to millimeter wave (mmWave) communication systems. In this work, we propose a beam alignment method that learns a site-specific probing codebook and uses the probing codebook measurements to predict the optimal narrow beam. A novel neural network (NN) architecture is designed to jointly learn the probing codebook and the beam predictor in an end-to-end fashion. The learned codebook consists of site-specific probing beams that can capture particular characteristics of the propagation environment. The proposed method relies on beam sweeping of the learned probing codebook, does not require additional context information and is compatible with the beam sweeping-based beam alignment framework in 5G. We demonstrate using realistic ray-tracing data that the proposed method can achieve high beam alignment accuracy and signal-to-noise ratio (SNR) while significantly reducing the beam sweeping complexity and latency.
\subsubsection{Machine Learning for Robust Beam Tracking in Mobile Millimeter-Wave Systems}
abstract:Narrow beams in millimeter-wave (mmWave) communication introduce significant beam misalignment challenges. In this paper, we introduce MAMBA-X, an enhanced version of the MAMBA beam tracking scheme. Basically, MAMBA uses a restless multi-armed bandit framework to capture the dynamics of mmWave links by discounting the relevance of past observations using a "forgetting factor" ( 1) and increases the weight of recent observations via a "boost factor" ( 2). Because the original MAMBA uses fixed values for  1 and  2, it cannot quickly adapt to variations in user mobility. Moreover, if the time between consecutive beam selection instances is large compared to channel dynamics, past observations become obsolete. To tackle these issues, we first use the concept of beam coherence time to establish a bound on the beam selection intervals. Secondly, we show that the performance of MAMBA depends primarily on the value of  1 which, in turn, depends on UE mobility. We develop a Long Short-Term Memory (LSTM) model to dynamically predict and update the optimal value of  1. Through extensive simulations at 28 GHz and using publicly available 5G NR experimental dataset, we evaluate MAMBA-X. Our results indicate that the total delivered traffic is improved by up to 46.8% relative to the original MAMBA and 142% compared to the default beam management scheme in 5G NR.
\subsubsection{Hierarchical Beamforming in Random Access Channels}
abstract:Managing a massive number of terminals in a contention-based multiple access is challenging due to its intrinsic limited efficiency. For example, in the random access channel considered in LTE-A and 5G NR, Base Station (BS) is just aware of the collided and non-collided preambles. Several time-based protocols have been investigated to redistribute the overload under high terminal activity, thus avoiding the congestion. In this work, we explore the use of the spatial domain by means of a hierarchical codebook-based beamforming, where the BS selects the appropriate beams as a function of the number of non-collided and collided preambles. Since the activity and placement of terminals may be dynamic over time, the sequential selection of parameters can benefit from a reinforcement learning (RL) framework. We propose an algorithm that can exploit both domains, temporal and spatial, with the goal of reducing collisions and enhancing transmission delay. Our approach is able to efficiently learn whenever there is a non-homogeneous spatial distribution of terminals and adapt the spatial beams accordingly.
\subsubsection{Deep Learning Based Power Control for Cell-Free Massive MIMO with MRT}
abstract:Cell-Free Massive MIMO with MRT (Maximum-Ratio Transmission) has the advantage of decentralized beamforming with the smallest front-haul overhead. Its downlink power control plays a dual role of fair power distribution among users and interference mitigation. It is well-known that finding the optimal max-min power control relies on SOCP (Second Order Cone Programming) feasibility bisection search, whose large computational delay is not suitable for practical implementation. In this paper, we devise a deep learning approach for finding a practical near-optimal power control. Specifically, we propose a convolutional neural network that takes as input the channel matrix of large-scale fading coefficients and outputs the total transmit power of each AP (access point). Using this information, the downlink power control for each user is then computed by a low-complexity convex program. Our approach requires to generate far fewer training examples than existing schemes. The reason is that we augment the training dataset with magnitudes larger number of artificial examples by exploiting the special structure of the problem. The resulting deep learning model not only provides a near-optimal solution to the original problem, but also generalizes well for problems with different number of users and different propagation morphologies, without the need to retrain it. Numerical simulations validate the near optimality of our solution with a significant reduction in computational burden.
\subsection{Big Data and Edge}
\subsubsection{A Novel Cross-domain Access Control Protocol in Mobile Edge Computing}
abstract:With the development of smart mobile terminals and mobile communication technologies, Mobile Edge Computing (MEC) has been applied to a variety of fields. However, MEC also brings new data security threats including the data access threat. To solve the cross-domain access control problem in MEC, this paper proposes a cross-domain access control protocol, named CDAC. In CDAC, a new user reputation evaluation strategy is proposed, which dynamically evaluates the comprehensive reputation of users based on different access behaviors of users, so that gateway nodes can evaluate user cross-domain requests. Meanwhile, different priorities are assigned according to user security levels to encourage users to regulate access behaviors to improve their reputations. Then, different gateway nodes implement cross-domain access control for users. The experiment results show that the proposed CDAC can provide efficient cross-domain access controls and achieve excellent system performances.
\subsubsection{An Incentive Mechanism for Big Data Trading in End-Edge-Cloud Hierarchical Federated Learning}
abstract:As a compelling collaborative machine learning framework in the big data era, federated learning allows multiple participants to jointly train a model without revealing their private data. To further leverage the ubiquitous resources in end-edge-cloud systems, hierarchical federated learning (HFL) focuses on the layered feature to relieve the excessive communication overhead and the risk of data leakage. For end devices are often considered as self-interested and reluctant to join in model training, encouraging them to participate becomes an emerging and challenging issue, which deeply impacts training performance and has not been well considered yet. This paper proposes an incentive mechanism for HFL in end-edge-cloud systems, which motivates end devices to contribute data for model training. The hierarchical training process in end-edge-cloud systems is modeled as a multi-layer Stackelberg game where sub-games are interconnected through the utility functions. We derive the Nash equilibrium strategies and closed-form solutions to guide players. Due to fully grasping the inner interest relationship among players, the proposed mechanism could exchange the low costs for the high model performance. Simulations demonstrate the effectiveness of the proposed mechanism and reveal stakeholder's dependencies on the allocation of data resources.
\subsubsection{ICE: Intelligent Caching at the Edge}
abstract:The unprecedented growth of mobile data traffic brings unique challenges for network bandwidth and server resources to meet the diverse QoE (Quality of Experience). Caching becomes a promising way to alleviate these issues by storing a subset of data at the network edge, for which caching policy becomes critical. To this end, various caching schemes have been put forward, however, these schemes are either not intelligent lacking the ability of self-learning and self-decision-making, or inefficient with low data hit rate. Based on these observations, in this paper, we propose a novel Intelligent Caching framework at the Edge, named ICE, via deep reinforcement learning to capture certain valued information of the requested data. Notably, in our approach, the popularity of the data to be cached will be explored and considered. A Markov decision model is further developed to determine whether the data should be cached. The evaluation shows that ICE greatly improves the hit rate in comparison with the state-of-the-art approaches, and reduces the energy consumption for data transmission. Furthermore, based on ICE, the users' QoE is greatly improved. In conclusion, both theoretical analysis and experimental results prove the effectiveness and high performance of ICE compared with conventional strategies.
\subsubsection{TPA based content popularity prediction for caching and routing in edge-cloud cooperative network}
abstract:The rapid development and application of 5G/B5G generate tremendous amount of traffic which in turn cause great burden for the corresponding transmission network. One typical way to address such challenge is to sink the content (e.g., 4K and 8K videos) from the remote cloud to the edge servers. In this case, how to efficiently visiting and getting these contents becomes a new problem, in which the cooperation between cloud and edge should be taken into consideration. In this regard, this work builds an edge and cloud cooperative routing and caching system which consists of three main modules of content popularity prediction, cooperative caching and cooperative routing. Specifically, the content prediction is designed by jointly leveraging the technologies of Long Short-Term Memory (LSTM) and Temporal Pattern Attention (TPA) to dig the traffic features and predict the future content popularity. Based on the prediction results and the technology of reinforce learning, the cooperative caching module designs both a reactive content replacement and an active content caching strategies. After that, the cooperative routing is carried out to help customers visiting and obtaining these content efficiently with the objective of minimizing the overhead.The experimental results indicate that the proposed methods outperform the state-of-the-art benchmarks in terms of the caching hit rate, the average throughput, the successful content delivery rate and the average routing overhead.
\subsubsection{Distributed Service Migration in Satellite Mobile Edge Computing}
abstract:With the emergence of more and more latency-sensitive application and mobile devices pumped to the edge of network, the burden on the backhaul network is getting heavier and heavier due to the limited transmission resources. Mobile edge computing (MEC) considered as a promising technology becomes more and more popular, which can provide services at the edge of the network. In this paper, we take into account the mobility of users and focus on the problem of service migration. Most of existing works modeled a Markov Decision Process (MDP) model with a high-dimensional state space, and have to solve it by deep reinforcement learning. To tackle this issue, we propose a distributed two-layer decomposition model and generate a series of new MDP problem with Low-dimensional in order to replace original High-dimensional MDP. In our model, the size of state space is reduced from M^{2N} to N*M^2 by decomposing the original optimization problem. Simulation results show that the performance of the proposed two-layer decomposition model is better than the baseline models.
\subsection{Big Data and Privacy}
\subsubsection{Spatio-temporal Similarity based Privacy-preserving Worker Selection in Mobile Crowdsensing}
abstract:As one of the most fundamental problems in mobile crowdsensing (MCS), worker selection has drawn significant attention in recent years. However, very few studies consider the workers' spatio- and temporal-coverage for the sensing task. In this paper, we propose a novel top-k worker selection scheme such that the MCS platform can select qualified workers in terms of spatio-temporal similarity. Besides, we design a novel privacy-preserving approach for protecting participants' spatio-temporal information based on the modified Paillier encryption technique. Detailed security analysis showed that the task requestor's temporal information and the workers' spatio-temporal information are preserved and will not be revealed to any other parties. Extensive experiments are conducted, and the results demonstrate that our scheme outperforms the baseline methods regarding the selection of reliable workers.
\subsubsection{PPDTSA: Privacy-preserving Deep Transformation Self-attention Framework For Object Detection}
abstract:In order to perform competitive privacy-guarantee object detection, we propose an end-to-end model called Privacy-preserving Deep Transformation Self-attention (PPDTSA). The model consists of a low-complexity hierarchical structure with a relatively small number of hyper-parameters. This model ensures the privacy for the inference results while detecting, and achieves the consistency of the predictive variables through the encoding and decoding blocks of the self-attention mechanism. The remaining dense blocks in the model can retain image details and expand the Region Of Interest (ROI). Predominantly, PPDTSA can locate points of attention through the self-attention mechanism, and calculate the focus loss through the image feature of foreground-background imbalance. Meanwhile, the objects in the inference images can be protected through the privacy budget specified by the user. Experimental results demonstrate that PPDTSA achieves superior performance on the MOT20 dataset compared with other existing object detection models such as Faster-RCNN.
\subsubsection{Fog-Based Conditional Privacy-Preserving Data Batch Verification in Smart Grid}
abstract:The smart grid is already widespread for the purpose of managing energy generation and distribution. In this system, frequent interactions between devices generate mass data, which requires robust processing capability. Fog computing has the advantage of latency and accessibility that can be applied in smart grids for improving data throughput and energy management efficiency to make the system sustainable. Furthermore, smart meters (SMs) are responsible for collecting real-time power consumption reports then sending them to the service provider (SP). According to the reports, the SP adjusts the power distribution strategy and formulates the optimization scheme for the energy management thus achieving sustainable development. Unfortunately, the power consumption report usually includes sensitive information that users do not want to reveal. Therefore, we propose a conditional privacy-preserving data aggregation with batch verification scheme. Firstly, as an aggregator, fog devices aggregate masses of data from users into one with simple operation, which reduces communication complexity. Meanwhile, it supports batch verification on the SP side. Secondly, the scheme not only avoids certificates management and key escrow, but implements encrypting and signing in a logical step, thus satisfying confidentiality, integrity and authentication. Finally, our scheme provides conditional privacy-preserving, in which messages can be authenticated anonymously and malicious messages can be traced. Extensive performance evaluation details our scheme is efficient with low computation complexity and communication overheads.
\subsubsection{A Privacy-Preserving Incentive Mechanism for Federated Cloud-Edge Learning}
abstract:The federated learning scheme enhances the privacy preservation through avoiding the private data uploading in cloud-edge computing. However, the attacks against the uploaded model updates still cause private data leakage which demotivates the privacy-sensitive participating edge devices. Facing this issue, we aim to design a privacy-preserving incentive mechanism for the federated cloud-edge learning (PFCEL) system such that 1) the edge devices are motivated to actively contribute to the updated model uploading, 2) a trade-off between the private data leakage and the model accuracy is achieved. We formulate the incentive design problem as a three-layer Stackelberg game, where the server-device interaction is further formulated as a contract design problem. Extensive numerical evaluations demonstrate the effectiveness of our designed mechanism in terms of privacy preservation and system utility.
\subsubsection{AirMixML: Over-the-Air Data Mixup for Inherently Privacy-Preserving Edge Machine Learning}
abstract:Wireless channels can be inherently privacy preserving by distorting the received signals due to channel noise, and superpositioning multiple signals over-the-air. By harnessing these natural distortions and superpositions by wireless channels, we propose a novel privacy-preserving machine learning (ML) framework at the network edge, coined over-the-air mixup ML (AirMixML). In AirMixML, multiple workers transmit analog modulated signals of their private data samples to an edge server who trains an ML model using the received noisy-and superpositioned samples. AirMixML coincides with model training using mixup data augmentation achieving comparable accuracy to that with raw data samples. From a privacy perspective, AirMixML is a differentially private (DP) mechanism limiting the disclosure of each worker's private sample information at the server, while the worker's transmit power determines the privacy disclosure level. To this end, we develop a fractional channel inversion power control (PC) method, -Dirichlet mixup PC (DirMix()-PC), wherein for a given global power scaling factor after channel-inversion, each worker's local power contribution to the superpositioned signal is controlled by the Dirichlet dispersion ratio . Mathematically, we derive a closed-form expression clarifying the relationship between the local and global PC factors to guarantee a target DP level. By simulations, we provide DirMix()-PC design guidelines to improve accuracy, privacy, and energy-efficiency. Finally, AirMixML with DirMix()-PC is shown to achieve reasonable accuracy compared to a privacy-violating baseline with neither superposition nor PC.
\subsection{Big Data and Security}
\subsubsection{Towards Lightweight and Efficient Distributed Intrusion Detection Framework}
abstract:Federated learning (FL), as a promising distributed learning paradigm, has put many efforts in distributed intrusion detection systems (IDS), for defending against various malicious attacks, such as SQL injection and DDoS attacks.Compared with traditional IDS based on centralized deep learning (DP), FL-based solutions require not to share users' raw data while yielding better detection performance.However, state-of-the-art FL-based methods still suffer from two key limitations:1) insufficient detection performance on non-independent and identically distributed (non-IID) data, and2) high communication and computational overheads due to the utilization of large-scale neural network model.In this paper, we propose a lightweight collaborative intrusion detection framework, called CoLGBM, the first of its kind in the regime of decentralized IDS, where decision tree and light gradient boosting machine (LGBM) are combined for constructing the detection scheme.The main insight is that through combining user-trained decision trees (each user's decision tree is derived from its own data with unique distribution), our framework can perform effectively on non-IID data while working efficiently for handling enormous samples.Compared with the current FL-based methods, our CoLGBM achieves higher accuracy and lower overhead on both IID and non-IID data.Extensive experiment results demonstrate our scheme with high-level performance.
\subsubsection{Secure Data Sharing in UAV-assisted Crowdsensing: Integration of Blockchain and Reputation Incentive}
abstract:Unmanned aerial vehicles (UAVs) combining with crowdsensing technology has been viewed as a promising paradigm for performing sensing tasks in extreme scenarios such as earthquakes, etc. However, potential security issues could incur on data sharing between UAVs and task publishers owing to the vulnerability of central node and selfishness of distrusted UAVs. To cope with these problems, we propose a novel blockchain-based crowdsensing framework with reputation incentive (BCFR) in UAV-assisted mobile crowdsensing. Specifically, we first propose a novel reputation incentive scheme to choose UAVs with a high reputation to perform sensing tasks, thereby protecting data sharing between UAVs and task publishers from internal attack (i.e., some UAVs with insufficient resources may turn into malicious UAVs to provide wrong sensory data to the task publishers). Then, we design a blockchain-based secure data transmission scheme to securely record data transaction of UAVs. Furthermore, since UAVs with limited resources are difficult to perform compute-intensive mining tasks, edge computing is incorporated to increase the success probability of block creation. The interactions between the UAVs and edge computing provider (ECP) are modeled as a two-stage Stackelberg game to motivate UAVs participating in the block creation process while providing high-quality services. Finally, we conduct extensive simulations to demonstrate that the proposed BCFR scheme can effectively improve successful mining probabilities and utilities of UAVs, and ensure the security of data sharing among UAVs and task publishers.
\subsubsection{Multi-hop Graph Embedding for Botnet Detection}
abstract:We have developed a novel multi-hop graph embedding technique for botnet detection. It can detect the entire layered architecture of a botnet in the internet backbone traffic by starting from a small set of the components of the botnet. A botnet is a group of hosts collaborating each other to launch a variety of attacks, such as distributed denial-of-service attacks and phishing campaigns. Over 20 years of their existence, botnets have been evolved to employ layered architectures for robust operation and efficient management. Several existing methods leverage graph analysis to detect malicious communications. However, they cannot detect such botnet components that communicating each other through multiple layers, which are represented at more than one hop distance in graphs. To solve this problem, our technique trains separate graph embedding models with samples at different distances and select appropriate features from multiple models to represent multi-hop adjacency for each node. By applying our proposal to real-world Internet traffic, we have confirmed that it can outperform other methods in terms of detecting collaborating botnet components with higher accuracy even if their command and control communications are cascading through multiple layers.
\subsubsection{Active Learning Under Malicious Mislabeling and Poisoning Attacks}
abstract:Deep neural networks usually require large labeled datasets for training to achieve the start-of-the-art performance in many tasks, such as image classification and natural language processing. Though a lot of data is created each day by active Internet users, most of these data are unlabeled and are vulnerable to data poisoning attacks. In this paper, we develop an efficient active learning method that requires fewer labeled instances and incorporates the technique of adversarial retraining in which additional labeled artificial data are generated without increasing the labeling budget. The generated adversarial examples also provide a way to measure the vulnerability of the model. To check the performance of the proposed method under an adversarial setting, i.e., malicious mislabeling and data poisoning attacks, we perform an extensive evaluation on the reduced CIFAR-10 dataset, which contains only two classes: airplane and frog. Our experimental results demonstrate that the proposed active learning method is efficient for defending against malicious mislabeling and data poisoning attacks. Specifically, whereas the baseline active learning method based on the random sampling strategy performs poorly (about 50%) under a malicious mislabeling attack, the proposed active learning method can achieve the desired accuracy of 89% using only one-third of the dataset on average.
\subsubsection{Detecting Cryptojacking Traffic Based on Network Behavior Features}
abstract:Bitcoin and other digital cryptocurrencies have developed rapidly in recent years. To reduce hardware and power costs, many criminals use the botnet to infect other hosts to mine cryptocurrency for themselves, which has led to the proliferation of mining botnets and is referred to as cryptojacking. At present, the mechanisms specific to cryptojacking detection include host-based, Deep Packet Inspection (DPI) based, and dynamic network characteristics based. Host-based detection requires detection installation and running at each host, and the other two are heavyweight. Besides, DPI-based detection is a breach of privacy and loses efficacy if encountering encrypted traffic. This paper designs a lightweight cryptojacking traffic detection method based on network behavior features for an ISP, without referring to the payload of network traffic. We set up an environment to collect cryptojacking traffic and conduct a cryptojacking traffic study to obtain its discriminative network traffic features extracted from only the first four packets in a flow. Our experimental study suggests that the machine learning classifier, random forest, based on the extracted discriminative network traffic features can accurately and efficiently detect cryptojacking traffic.
\subsection{Big Data Applications (1)}
\subsubsection{A Blockchain-based Database System for Decentralized Information Management}
abstract:Blockchain has attracted wide attention in both industry and academia due to its decentralized and anti-tamper characteristics. To query the blockchain data, all the transactions in the blockchain must be iterated one by one, which is quite inefficient. Existing solutions improve the query efficiency by exporting transactions to external databases, and yet they incur the incompleteness and incorrectness of query results. To avoid above weaknesses, we propose an efficient query method based on the internal blockchain database and apply it to the construction engineering management to solve the problems of information management. In our design, we construct a dual-index based on the B+ tree and the key-value pair through smart contracts, and it supports multiple query operations, such as range query and file-type query. We implement the designed method by simulating the blockchain testbed and the experimental results demonstrate the efficiency of our design.
\subsubsection{CMF Net: Detecting Objects in Infrared Traffic Image with Combination of Multiscale Features}
abstract:Infrared image target detection technology has always been a research hotspot, but the research on infrared image target detection in the traffic field is less. To address this problem, we propose a new deep convolutional network to solve the multi-target detection problem of infrared images by using the transfer learning technique to transfer the target detection framework in the visible domain of the deep learning infrared domain. The detection network includes a transfer learning method, multi-scale feature extraction mechanism, and two-level feature fusion method. The final input of the complete convolution network to the classification network includes low-level visual features for target location and high-level semantic features for target recognition and multi-scale features. The experiment verifies the advantages of the whole detection network compared with the traditional multi-target detection algorithm. On the FLIR test dataset, the mAP reaches 71%, which is about 13% higher than the Faster R-CNN, About 6% higher than YOLO, and about 17% higher than SSD.
\subsubsection{Spatiotemporal Graph Neural Network for Traffic Prediction Exploiting Cascading Behavior}
abstract:As a critical part of the intelligent transportation system, traffic prediction is challenging due to the time-evolving cascading behavior, i.e., the fluctuation of traffic conditions on one road will affect neighboring roads in the future. To address this issue, we propose a novel learning framework, which is able to extract the most relevant historical information for prediction by capturing the underlying cascading behavior. An encoder-decoder architecture is adopted, where the historical contextual information of each road is encoded into a sequence of historical embeddings. A spatiotemporal attention mechanism is devised to model the cascading behavior in the embedding space so that the most relevant information for prediction is concentrated. Extensive experiments on a real-world large-scale highway dataset verify the effectiveness of our proposed approach, observing 3%  5% improvement over state-of-the-art methods.
\subsubsection{Combining Syntactic and Position Relation for Targeted Sentiment Analysis Using Graph Neural Network}
abstract:Targeted sentiment analysis aims to predict the sentiment polarity of the target in a sentence. Most traditional Graph Neural Network-based methods have focused only on the syntactic dependency information of sentences. However, they ignore the position information of words in the linear form of sentences, which leads the model to paying attention to the irrelevant syntactic dependency information to the target. To attenuate the irrelevant information, a novel model called Position-aware Dual Relational Graph Attention Network (P-DRGAT) is proposed. Firstly, introducing the position-aware weight window of the syntactic dependency information to make the model pay more attention to the local syntactic information of words that neighbor the target. Secondly, a dual relational attention mechanism combining syntactic and position information is proposed. Experiments show that our model can effectively attenuate the irrelevant syntactic information and outperform state-of-the-art baselines on Accuracy and Macro-F1.
\subsubsection{Accurate Spectrum Prediction Based on Joint LSTM with CNN toward Spectrum Sharing}
abstract:In cognitive radio context, spectrum sensing is the vital technique for the cognitive users to acquire the spectrum of frequency band. It requires cognitive users to determine the usage state of the spectrum through spectrum perception and access the spectrum for communication when the spectrum is idle. However, the accuracy of single radio spectrum is low. Thus, the proposed algorithm adopts the joint Long and Short Term Memory (LSTM) and Convolutional neural network (CNN) prediction model carries out combined design to obtain the prediction model, which is used for spectrum prediction under multi-channel. The simulation results show that and the sensing accuracy of radio spectrum is significantly improved.
\subsection{Big Data Applications (2)}
\subsubsection{Multi-object tracking for road surveillance without using features of image data}
abstract:Visual surveillance of dynamic objects on roads has been developed to ensure road safety for people. Particularly, vehicle tracking is considered as a key technology for the road safety; studies on multi-object tracking (MOT) are being actively pursued. However, when MOT is performed, raw vision data are not always available because of the technical limitation or the privacy concern of the system; MOT needs to be performed only using the coordinates obtained from the object detector without using features extracted from raw image data such as color of vehicles, which degrades the accuracy of MOT to the unsatisfactory level for road safety. This paper proposes an MOT scheme for moving vehicles that is inspired by cell tracking using the Viterbi algorithm. The proposed scheme extends the Brownian motion model, which was used in the base scheme of cell tracking, by weighting probability transitions in accordance with the direction of travel of vehicles on the road. We evaluate the proposed scheme using simulated vehicle-traffic data and verify that the proposed scheme performs better than benchmark schemes in terms of the accuracy of MOT. We also demonstrate an example of how the proposed scheme works well for real vehicle-traffic data.
\subsubsection{Kernel-Based Structural-Temporal Cascade Learning for Popularity Prediction}
abstract:One of the main objectives of information cascade popularity prediction is to forecast the future size of a cascade given the observed propagation information. It is an enabling step for many practical applications (e.g., advertisement, academic writing, etc.). Recent advances in neural networks have spurred a few deep learning-based cascade models, which preserve the structural features of information cascades with node embedding and graph neural networks. However, despite the substantial efforts in cascade graph learning as well as its internal temporal dependency, existing methods mainly focus on node-level similarity learning, ignoring the structural equivalence among different sub-graphs that are more informative for information diffusion prediction. Towards this, we present a kernel-based structural-temporal cascade learning model, called CasKernel, to explicitly estimate and encode the structural similarity of cascades with the graph kernels. Moreover, we employ a non sequential process to address the temporal dependency, which can be used to facilitate information popularity prediction. Experiments conducted on both tweets propagation network and academic citation network demonstrate the effectiveness of our method.
\subsubsection{Sharding for Blockchain based Mobile Edge Computing System: A Deep Reinforcement Learning Approach}
abstract:With the growth of data scale in the mobile edge computing (MEC) network, data security of the MEC network has become a burning concern. The application of blockchain technology in MEC enhances data security and privacy protection. However, throughput becomes the bottleneck of the blockchain-enabled MEC system. Hence, this paper proposes a novel hierarchical and partitioned blockchain framework to improve scalability while guaranteeing the security of partitions. Next, we model the joint optimization of throughput and security as a Markov decision process (MDP). After that, we adopt deep reinforcement learning (DRL) based algorithms to obtain the number of partitions, the size of micro blocks and the large block generation interval. Finally, we analyze the security and throughput performance of proposed schemes. Simulation results demonstrate that proposed schemes can improve throughput while ensuring the security of partitions.
\subsubsection{Data-Driven Quickest Change Detection for Securing Federated Learning for Internet-of-Vehicles}
abstract:Machine Learning (ML) is on the verge of transitioning from centralized, distributed to federated learning (FL) due to the inherent privacy-preserving framework by FL. FL enables collaborative learning among participants just by exchanging updated model parameters with the FL server while keeping training data local in the end devices. However, this learning framework makes the life of the server difficult to detect the malicious behavior of participants. Malicious model updates from participants may affect the accuracy of the learning model considerably and consequently may cause severe consequences in Internet of Vehicles (IoV) environment. To address this issue, we propose a novel approach to apply Shiryaev's quickest change detection (QCD) technique in the FL realm. QCD is applied to detect abnormal changes in statistical properties of model parameters in FL as quickly as possible. We apply QCD on the server-side in two ways. First, QCD is applied to detect a change in the statistical properties over the model parameters sent by the participating devices. Second, QCD is applied to the history of aggregated model parameters. The first approach facilitates identifying malicious clients which can be eliminated in future learning activities. The other approach assists server to roll back to an earlier version of the model in case of identifying the anomaly in the aggregated parameters value. As QCD is applied on the server-side, it does not add any computation overhead on the client-side as well as communication overheard during transmission. These two approaches are evaluated with the help of numerical results.
\subsubsection{Multi-dimensional indexes for point and range queries on outsourced encrypted data}
abstract:We present an approach for indexing encrypted data stored at external providers to enable provider-side evaluation of queries. Our approach supports the evaluation of point and range conditions on multiple attributes. Protection against inferences from indexes is guaranteed by clustering tuples in boxes that are then mapped to the same index values, so to ensure collisions for individual attributes as well as their combinations. Our spatial-based algorithm partitions tuples to produce such a clustering in a way to ensure efficient query execution. Query translation and processing require the client to store a compact map. The experiments, evaluating query performance and client-storage requirements, confirm the efficiency enjoyed by our solution.
\subsubsection{Improvement on a Traffic Data Generator for Networking AI Algorithm Development}
abstract:Recently, many Artificial Intelligence (AI) based schemes have been proposed to support network measurement and management, such as AI-based network traffic classification, intrusion detection, traffic prediction, etc. They have demonstrated promising performance in supporting the networking. However, the development of these AI schemes requires a massive amount of fresh datasets. The scarcity and futility of public datasets are straining the development of the networking AI model. Not to mention that most available datasets are not up-to-date. Collecting real-life datasets can be time-consuming and restricted by networking capabilities. To address the issues, we have introduced a real-application enabled network traffic generator. In this work, we further enhance the generator with more functionality. In particular, a traffic flow segment scheme is proposed for the quick establishment of traffic flow databases. An intelligent generator is implemented to enable point-to-point communications, network multiplexing, and network duplexing. The evaluation results demonstrate that the proposed intelligent traffic simulator can generate a large amount of complex network traffic with practical settings more efficiently than data collection in real life. A case study is given to demonstrate the quality of the generated traffic data and the generating efficiency.
\subsection{Big Data Processing}
\subsubsection{The Optimization of Model Parallelization Strategies for Multi-GPU Training}
abstract:Data parallelism (DP) is most widely used among all the parallel methods in largescale network training but the speedup from leveraging DP begins to scale poorly as the number of devices in data parallel training grows. Therefore, model parallelism (MP) is needed for further accelerating. In this paper, an integer linear programming based tool, NetPlacer, is proposed to find optimal operation-to-device placement. Unlike existing methods using time estimation, NetPlacer discards estimating simulated execution time and uses strict memory and computing balancing constraints on the network to find a scheme with a balance between memory and computation on different devices. As far as we know, we are the first to use equipment equalization as a target for model parallel strategy optimization. A speedup of at least 20% is achieved through NetPlacer for Inception-V3 on a 2-GPU machine compared to what one single GPU alone can achieve.
\subsubsection{Adaptive Participant Selection in Heterogeneous Federated Learning}
abstract:Federated learning (FL) is a distributed machine learning technique to address the data privacy issue. Participant selection is critical to determine the latency of the training process in a heterogeneous FL architecture, where users with different hardware setups and wireless channel conditions communicate with their base station to participate in the FL training process. Many solutions have been designed to consider computational and uploading latency of different users to select suitable participants such that the straggler problem can be avoided. However, none of these solutions consider the waiting time of a participant, which refers to the latency of a participant waiting for the wireless channel to be available, and the waiting time could significantly affect the latency of the training process, especially when a huge number of participants are involved in the training process and share the wireless channel in the time-division duplexing manner to upload their local FL models. In this paper, we consider not only the computational and uploading latency but also the waiting time (which is estimated based on an M/G/1 queueing model) of a participant to select suitable participants. We formulate an optimization problem to maximize the number of selected participants, who can upload their local models before the deadline in a global iteration. The Latency awarE pARticipant selectioN (LEARN) algorithm is proposed to solve the problem and the performance of LEARN is validated via simulations.
\subsubsection{Automated Primary Hyperparathyroidism Screening with Neural Networks}
abstract:Primary Hyperparathyroidism(PHPT) is a relatively common disease, affecting about one in every 1,000 adults. However, screening for PHPT can be difficult, meaning it often goes undiagnosed for long periods of time. While looking at specific blood test results independently can help indicate whether a patient has PHPT, often these blood result levels can all be within their respective normal ranges despite the patient having PHPT. Based on the clinic data from the real world, in this work, we propose a novel approach to screening PHPT with neural network (NN) architecture, achieving over 97% accuracy with common blood values as inputs. Further, we propose a second model achieving over 99% accuracy with additional lab test values as inputs. Moreover, compared to traditional PHPT screening methods, our NN models can reduce the false negatives of traditional screening methods by 99%.
\subsubsection{SamE: Sampling-based Embedding for Learning Representations of the Internet}
abstract:We have developed SamE, a novel sampling-based embedding technique for learning representations of the Internet. SamE can classify Internet hosts in a scalable and cost effective manner without sacrificing the classification performance.Machine learning has been applied to Internet traffic analysis for a variety of purposes, including botnet detection and application identification. For example, as a major threat on the Internet, a botnet is a group of computers that collaborate together to launch cyberattacks. To analyze related hosts such as the collaborating constituents of a botnet, graph embedding techniques seem to be promising. However, when applying existing graph embedding techniques to Internet-scale traffic data, the time and space complexities become prohibitively high for practical use. To make graph embedding applicable to Internet-scale problems, SamE only samples a subset of nodes to learn elemental representations and aggregates learned elemental representations to generate synthetic representations for all nodes. We have applied SamE to real-world Internet-scale traffic data, and the experimental results show that SamE outperforms existing methods by reducing the data samples required in representation learning by 99% while achieving the same level of classification performance in botnet detection and application identification.
\subsubsection{ATTL: An Automated Targeted Transfer Learning with Deep Neural Network}
abstract:Success of machine learning algorithms hinges on access to labeled dataset. Obtaining a labeled dataset is an expensive, challenging and time-consuming process, leading to the development of transfer learning (TL) methodology. TL incorporates gained knowledge from a previously trained source model into specific yet similar task models with limited data domain coverage. In this paper, we propose an automated targeted transfer learning (ATTL) method to resolve the transferability between source and target with minimal data requirements. The ATTL method decides how much target data is essential for model training, along with selected source data, to obtain the skateholder's specified performance metrics. The ATTL framework optimizes the system to select minimal target data based on two approaches: combinatorial coverage and adaptive selection methodology, along with specific source data for fine-tuning given a pre-trained source model. We evaluated the ATTL method on the Kaggle's 'planes in satellite imagery' dataset and the results identified that acquiring a small number of intentionally well-chosen samples from the target environment can achieve model performance of 97% in comparison to the baseline transfer learning accuracy of 92%.
\subsection{Cloud and Edge Computing I}
\subsubsection{Resource Allocation via Edge Cooperation in Digital Twin Assisted Internet of Vehicle}
abstract:In this paper, we propose a Digital Twin (DT) Supported Resource Allocation Scheme (DTS-RAS), which empower the intelligent edge cooperation in the Internet of Vehicle (IoV) environment possible. We focus on the latency minimization under the DT-IoV framework. Specifically, we formulate the mathematical expression for the response time of vehicle offloading tasks to cooperative edge nodes according to modelingthe edge server as a M/M/1/N queen. Then, we construct the optimization model aiming at reducing the response time. In view of the complexity, we apply a Double Deep Q-learning Network (DDQN) to training the edge server to get an optimal allocation action by modeling the cooperation process as an MDP. Simulation results demonstrate that our proposed scheme outperforms the existing schemes in terms of execution latency.
\subsubsection{Blockchain-based Trustworthy Service Caching and Task Offloading for Intelligent Edge Computing}
abstract:The upcoming 6G era involves an increasing level of data processing and capacity, where mobile edge computing (MEC) system is deployed in large scale to support more edge intelligent applications. As an effective way to provide better service to user equipments (UEs), MEC is required to allocate computation resources reasonably and utilize service caching to assist computation offloading. With service caching, collaboration can be established from two aspects: the service sharing between UEs and the caching collaboration between MEC servers, leading to a higher demand for system credibility. In order to allocate computation and caching resources effectively while ensuring the credibility of the cached services, the blockchain is introduced to the edge network as a distributed and self-organized database. A credibility evaluation mechanism based on Dempster-Shafer theory is designed in the overlaid blockchain network to ensure trustworthiness. Since it is challenging to optimize delay and credibility comprehensively in this dynamic system, we propose a deep reinforcement learning (DRL)-based approach to make offloading decision, and carry out service caching according to both credibility and offloading decisions of multiple users. Simulation results show that the proposed system can effectively improve the performance of the blockchain-based edge computing system.
\subsubsection{Energy-Efficient D2D-Aided Fog Computing under Probabilistic Time Constraints}
abstract:Device-to-device (D2D) communication is an enabling technology for fogcomputing by allowing the sharing of computation resources between mobile devices. However, temperature variations in the device CPUs affect the computation resources available for task offloading, which unpredictably alters the processing time and energy consumption. In this paper, we address the problem of resource allocation with respect to task partitioning, computation resources and transmit power in a D2D-aided fog computing scenario, aiming to minimize the expected total energy consumption under probabilistic constraints on the processing time. Since the formulated problem is non-convex, we propose two sub-optimal solution methods. The first method is based on difference of convex (DC) programming, which we combine with chance-constraint programming to handle the probabilistic time limitations. Considering that DC programming is dependent on agood initial point, we propose a second method that relies on only convex programming, which eliminates the dependence on user-defined initialization. Simulation results demonstrate that the latter method outperforms the former in terms of energy efficiency and run-time.
\subsubsection{Joint Caching and Transmission in the Mobile Edge Network: An Multi-Agent Learning Approach}
abstract:Joint caching and transmission optimization problem is challenging due to the deep coupling between decisions. This paper proposes an iterative distributed multi-agent learning approach to jointly optimize caching and transmission. The goal of this approach is to minimize the total transmission delay of all users. In this iterative approach, each loop includes caching optimization and transmission optimization. A multi-agent reinforcement learning (MARL)-based caching network is developed to cache popular tasks, such as answering which files to evict from the cache and which files to storage. Based on the cached files of the caching network, the transmission network with multi-agent Bayesian learning automata (MABLA) method transmits cached files by optimal transmission strategy, single transmission (ST) or joint transmission (JT) for the each user. The experimental results demonstrates the gains of the proposed multi-agent learning approach.
\subsection{Cloud and Edge Computing II}
\subsubsection{Predictable Bandwidth Slicing with Open vSwitch}
abstract:Software switching, a.k.a virtual switching, plays a vital role in network virtualization and network function virtualization, enhances configurability, and reduces deployment and operational costs. Software switching also facilitates the development of edge and fog computing networks by allowing the use of commodity hardware for both data processing and packet switching. Despite these benefits, characterizing and ensuring deterministic performance with software switches is harder, compared to physical switching appliances. In particular, achieving deterministic performance is essential to adopt software switching in mission-critical applications, especially those deployed in edge and fog computing architectures. In this paper, we study the impact of switch configurations on bandwidth slicing and predictable packet latency. We demonstrate that latency and predictability are dependent on the implementation of the bandwidth slicing mechanism and that the packet schedulers used in OVS Kernel-Path and OVS-DPDK each focus on different aspects of switching performance.
\subsubsection{Context-Aware Augmented Reality with 5G Edge}
abstract:Augmented Reality (AR) provides immersive user experiences by overlaying digital information on physical environments. Context-awareness is crucial for delivering relevant augmentations that best suits users' requirements and their environments. In this article, we combine context-aware reasoning with emerging AR applications to provide the most relevant information according to user and environment contexts. To support the best possible quality of experience, 5G edge enables the distribution of computation-intensive AR tasks to edge servers through 5G networks. We develop ConAR, a context-aware head-mounted display AR system which is deployed on the edge and Cloud leveraging environmental sensors and user profile context for navigation. ConAR is composed of a HoloLens application and a paired mobile client, which contains a context model for air quality forecasting, and rendering recommendations on holograms through a HoloLens 2 device. We evaluate our system performance by deploying our proposed air quality prediction algorithm on the edge and Cloud while communicating to them with 5G and LTE connections. We measure network quality metrics and find the deployment on the edge with a 5G connections significantly outperforms alternative solutions. Our results demonstrate that the 5G edge is suitable for supporting latency-sensitive analysis tasks for context-aware AR.
\subsubsection{EVBLB: Efficient Voronoi Tessellation-Based Load Balancing in Edge Computing Networks}
abstract:Edge computing (EC) is a promising solution to enable the next-generation delay-critical network services which are not conceivable in the traditional cloud-based architecture. EC takes the computing and storage resources closer to the end-users at the edge of the networks to eliminate the propagation delays caused by geographical distances. However, due to the lack of facilities such as cooling systems, the capacity of available resources in the edge is far less than that in the remote clouds. So, efficient utilization of the edge resources has a profound impact on the effectiveness of the edge computing paradigm. Load balancing is a key factor in achieving resource efficiency and high utilization. In this paper, we present the design of EVBLB, an efficient load balancing algorithm based on Voronoi tessellation (VT) that assigns the users' service requests to the edge servers while considering the density of edge resources in the area and the distance of the users from the assigned servers. Building on the notion of VT not only allows to achieve these goals, but is also computable in linear time, which significantly improves the scalability and responsiveness of our proposed method as compared to existing studies. Our simulation results show that EVBLB outperforms two conventional baselines in terms of throughput, response time, task completion time, and request blocking rate.
\subsubsection{OCDST: Offloading Chained DNNs for Streaming Tasks}
abstract:Considering the contradiction between limited resources in small devices and the high complexity of deep neural networks (DNNs), DNNs can hardly run on small devices such as smartphones and wearable devices. Therefore, offloading DNNs to computing units (fog/edge servers), where each unit executes a part of a DNN collaboratively, has gained increasing popularity. Notably, DNNs are generally in chained structure, while streaming tasks are the central part of artificial-intelligent applications. Thus it is crucial to reduce chained DNNs' delay for streaming tasks. Although existing works have advanced DNN offloading largely, the discussion about chained DNNs and streaming tasks is negligible. To address this issue, in this paper, we propose a layer-level offloading model called OCDST based on the analysis about them. After chained DNNs are offloaded to computing units, the involved units will handle streaming tasks as a pipeline. Consequently, the model significantly reduces the average task delay by paralleling each step in the pipeline. Moreover, the global optimal model solution is drawn by an improved depth-first search (DFS) algorithm, which utilizes DFS to achieve path establishment, calculation and record stages. Based on multi-threading programming and producer-consumer pattern, a program parallelization scheme is also devised to ensure the feasibility of the obtained optimum. Experimental results show that OCDST significantly outperforms recent works with higher inferring speed and faster response.
\subsection{Cloud Computing and Data Centers}
\subsubsection{HyperNAT: Scaling Up Network Address Translation with SmartNICs for Clouds}
abstract:Network address translation (NAT) is a basic functionality in cloud gateways. With the increasing traffic volume and number of flows introduced by the cloud tenants, the NAT gateway needs to be implemented on a cluster of servers. We propose to scale up the gateway servers, which could reduce the number of servers so as to reduce the capital expense and operation expense. We design HyperNAT, which leverage smartNICs to improve the server's processing capacity. In HyperNAT, the NAT functionality is distributed on multiple NICs, and the flow space is divided and assigned accordingly. HyperNAT overcomes a challenge that the packets in two directions of one connection need to be processed by the same NAT rule (named two-direction consistency, TDC) by cloning the rule to both data paths of the two directions. Our implementation and evaluation of HyperNAT show that HyperNAT could scale up cloud gateway effectively with low overhead.
\subsubsection{A Dynamic Trust-Game Theoretic Access Control Model for the Cloud}
abstract:The user's access history can be used as an important reference factor in determining whether to allow the current access request or not. And it is often ignored by existing access control models. To make up for this defect, a Dynamic Trust - game theoretic Access Control model is proposed based on the previous work. This paper proposes a method to quantify the user's trust in the cloud environment, which uses identity trust, behavior trust, and reputation trust as metrics. By modeling the access process as a game and introducing the user's trust value into the pay-off matrix, the mixed strategy Nash equilibrium of user and service provider is calculated respectively. Further, a calculation method for the threshold predefined by the service provider is proposed. Authorization of this access request depends on the comparision of the calculated probability of the user's adopting a malicious access policy with the threshold. Finally, we summarize this paper and make a prospect for future work.
\subsubsection{Tail-Latency and Cost Tradeoff Optimization for URLLC-Enabled Distributed Coded-Storage Data Centers}
abstract:In the existing frameworks of distributed systems indata centers, long-running delay-sensitive applications often sufferfrom unexpectedly high response time fluctuation due to long taillatency. Towards this end, it is crucial to support ultra-reliableand low-latency communications (URLLC) with stringent taillatencyquality-of-service (QoS) requirements. The QoS metric oftail latency for a latency-sensitive application is defined as theworst-case performance metric specified in its internal servicelevel objectives (SLOs) to characterize the worst-case scenarioof the response time. To guarantee stringent tail-latency QoSrequirements in data centers, accurately measuring the QoS metricof tail latency for distributed systems has received tremendousresearch attention. However, how to accurately quantify andoptimize the QoS metric of tail latency while minimizing thestorage cost using maximum distance separable (MDS) erasurecodes over distributed storage servers in supporting URLLC isstill an open problem due to the lack of mathematical modelsfor analyzing such systems. To overcome these problems, in thispaper we propose and develop an efficient algorithm for solving thetail-latency and cost tradeoff optimization problem in distributedcoded-storage servers in supporting URLLC services.
\subsubsection{Throughput Maximization of Virtual Machine Communications in Bandwidth-Constrained Data Centers}
abstract:In this paper we study a new algorithmic problem that maximizes the throughput of virtual machine (VM) communication in bandwidth-constrained data centers. Given a set of VM pairs that are already placed inside cloud data centers, each communicating with a particular bandwidth demand, we study how to allocate the network bandwidth to the VM pairs to accommodate maximum number of VM communication while considering that cloud data centers have limited bandwidths. We refer to this problem as VMB. Due to the massive growth of cloud communication traffic in recent years and that service providers attempt to accommodate as many VM applications as possible in order to maximize their profits, VMB is an important problem to study. First we prove that VMB is NP-hard. Then we propose a suite of algorithms to solve VMB. In particular, we propose an approximation algorithm that achieves constant approximation ratio. We show through simulations that our algorithms are effective in accommodating large number of VM communications under different network parameters. In particular, our approximation algorithm accommodate up to 38\% more VM pairs compared to existing research.
\subsection{Coding and secrecy}
\subsubsection{Adversarial Neural Networks for Error Correcting Codes}
abstract:Error correcting codes are a fundamental componentin modern day communication systems, demanding extremelyhigh throughput, ultra-reliability and low latency. Recentapproaches using machine learning (ML) models as the decodersoffers both improved performance and great adaptability to unknownenvironments, where traditional decoders struggle. We introduce a general framework to further boost the performance and applicability of ML models. We proposeto combine neural ML decoders with a discriminator that triesto distinguish between codewords and noisy words, and guidesthe decoding models to recover transmitted codewords. Ourframework is game-theoretic, motivated by generative adversarialnetworks (GANs), with the decoder and discriminator competingin a zero-sum game. The decoder learns to simultaneouslydecode and generate codewords while the discriminator learns totell the differences between decoded outputs andcodewords. Thus, the decoder is able to decode noisy receivedsignals into codewords increasing the probability of successfuldecoding. We show a strong connection of our frameworkwith the optimal maximum likelihood decoder by proving thatthis decoder defines a Nash's equilibrium point of our game.Hence, training to equilibrium has a good possibility of achievingthe optimal maximum likelihood performance. Moreover, ourframework does not require training labels, which are typically unavailable during communications,and, thus, seemingly can be trained online and adapt to channeldynamics. To demonstrate the performance of our framework,we combine it with the very recent neural decoders and showimproved performance compared to the original models andtraditional decoding algorithms on various codes.
\subsubsection{Machine Learning Aided Path Loss Estimator and Jammer Detector for Heterogeneous Vehicular Networks}
abstract:Heterogeneous vehicular communications aim to improve the reliability, security and delay performance of vehicle-to-vehicle (V2V) communications, by utilizing multiple communication technologies. Predicting the path loss through conventional fitting based models and radio frequency (RF) jamming detection through rule based models of different communication schemes fail to address comprehensive mobility and jamming scenarios. In this paper, we propose a machine learning based adaptive link quality estimation and jamming detection scheme for the optimum selection and aggregation of IEEE 802.11p and Vehicular Visible Light Communications (V-VLC) technologies targeting reliable V2V communications. We propose to use Random Forest regression and classifier based algorithms, where multiple individual learners with diversity are trained by using measurement data and the final result is obtained by averaging outputs of all learners. We test our framework on real-world road measurement data, demonstrating up to 2.34 dB and 0.56 dB Mean Absolute Error (MAE) improvement for V-VLC and IEEE 802.11p path loss prediction compared to fitting based models, respectively. The proposed jamming presence detection scheme yields 88.3% accuracy to detect noise interference injection for IEEE 802.11p links, yielding 3% better prediction performance than previously proposed deep convolutional neural network (DCNN) based scheme.
\subsubsection{Interleaver Design and Pairwise Codeword Distance Distribution Enhancement for Turbo Autoencoder}
abstract:This paper enhances the performance and training of the Turbo Autoencoder (TurboAE), an end-to-end jointly trained neural channel encoder and decoder. A novel interleaver for TurboAE with convolutional neural network (CNN) is proposed, which is shown to enhance the end-to-end performance of both TurboAE continuous and binary. Thanks to the proposed interleaver, TurboAE binary performs better than a benchmark Turbo code of length 100. In addition, a novel additional term to the loss function is proposed to improve the pairwise distance distribution of the codewords, which is shown to help the training convergence of TurboAE with CNN and recurrent neural network (RNN).
\subsubsection{FedEqual: Defending Model Poisoning Attacks in Heterogeneous Federated Learning}
abstract:With the upcoming edge AI, federated learning (FL) is a privacy-preserving framework to meet the General Data Protection Regulation (GDPR). Unfortunately, FL is vulnerable to an up-to-date security threat, model poisoning attacks. By successfully replacing the global model with the targeted poisoned model, malicious end devices can trigger backdoor attacks and manipulate the whole learning process. The traditional researches under a homogeneous environment can ideally exclude the outliers with scarce side-effects on model performance. However, in privacy-preserving FL, each end device possibly owns a few data classes and different amounts of data, forming into a substantial heterogeneous environment where outliers could be malicious or benign. To achieve the system performance and robustness of FL's framework, we should not assertively remove any local model from the global model updating procedure. Therefore, in this paper, we propose a defending strategy called FedEqual to mitigate model poisoning attacks while preserving the learning task's performance without excluding any benign models. The results show that FedEqual outperforms other state-of-the-art baselines under different heterogeneous environments based on reproduced up-to-date model poisoning attacks.
\subsection{Coding for Storage and Computing}
\subsubsection{Coding on Dual-Parameter Barrier Channels beyond Worst-Case Correction}
abstract:This paper studies coding on channels with the barrier property: only errors to and from a special barrier state are possible. This model is motivated by storage media that have heterogeneous state structure, not admitting the usual multi-bit scaling of the representation states. Our contributions include derivation of the channel capacity, efficient maximum-likelihood and list decoding algorithms, and finite-block-length analysis using random codes. This work is the first that addresses a barrier channel with separate parameters for the transitions into and out of the barrier state. Earlier work addressed special-case single-parameter models, and focused primarily on the worst-case coding performance.
\subsubsection{Variable Coded Batch Matrix Multiplication}
abstract:In this paper, we introduce the Variable Coded Distributed Batch Matrix Multiplication (VCDBMM) problem which tasks a distributed system to perform batch matrix multiplication where matrices are not necessarily distinct among batch jobs. Most coded matrix-matrix computation work has broadly focused in two directions: matrix partitioning for computing a single computation task and batch processing of multiple distinct computation tasks. While these works provide codes with good straggler resilience and fast decoding for their problem spaces, these codes would not be able to take advantage of the natural redundancy of re-using matrices across batch jobs. Inspired by Cross-Subspace Alignment codes, we develop Flexible Cross-Subspace Alignments (FCSA) codes that are flexible enough to utilize this redundancy. We provide a full characterization of FCSA codes which allow for a wide variety of system complexities including good straggler resilience and fast decoding. We theoretically demonstrate that, under certain practical conditions, FCSA codes are within a factor of two of the optimal solution when it comes to straggler resilience; our simulations demonstrate that our codes achieve even better optimality gaps in practice.
\subsubsection{Fault-Tolerant Computation Meets Network Coding: Optimal Scheduling in Parallel Computing}
abstract:We propose an optimal scheduling strategy to enable fault-tolerant reliable computation to protect the integrity of computation. Specifically, we determine the optimal redundancy-failure rate tradeoff to incorporate redundancy into parallel computing units running multiple-precision arithmetic that are useful for applications such as asymmetric cryptography and fast integer multiplication. Inspired by network coding, we propose coding matrices to strategically map partial computation to available computing units, so that the central unit can reliably reconstruct the results of any failed machine without recalculations to yield the final correct computation output. We propose optimization-based algorithms to efficiently construct the optimal coding matrices subject to fault tolerance specifications. Performance evaluation demonstrates that the optimal scheduling effectively reduces the overall running time of parallel computing while resisting wide-ranging failure rates.
\subsubsection{Coded Distributed Computation with Limited Resources}
abstract:A central issue of distributed computing systems is how to optimally allocate computing and storage resources and design data shuffling strategies such that the total execution time for computing and data shuffling is minimized. This is extremely critical when the computation, storage and communication resources are limited. In this paper, we study the resource allocation and coding scheme for the MapReduce-type framework with limited resources. In particular, we focus on the coded distributed computing (CDC) approach proposed by Li et al.. We first extend the asymmetric CDC (ACDC) scheme proposed by Yu et al. to the cascade case where each output function is computed by multiple servers. Then we demonstrate that whether CDC or ACDC is better depends on system parameters (e.g., number of computing servers) and task parameters (e.g., number of input files), implying that neither CDC nor ACDC is optimal. By merging the ideas of CDC and ACDC, we propose a hybrid scheme and show that it can strictly outperform CDC and ACDC. Furthermore, we derive an information-theoretic converse showing that for the MapReduce task using a type of weakly symmetric Reduce assignment, which includes the Reduce assignments of CDC and ACDC as special cases, the hybrid scheme with a corresponding resource allocation strategy is optimal, i.e., achieves the minimum execution time, for arbitrary amount of computing servers and storage memories.
\subsubsection{Communication-Efficient Coded Distributed Multi-Task Learning}
abstract:Consider a distributed multi-task learning (MTL) framework where the distributed users first train their own models based on the local data and then send the local updates to the server, and the server sends back helpful information by which each user can learn its task independently. Compared to the single-task case, the distributed MTL suffers more severely from the communication bottleneck, because the users wish to learn multiple models, causing the downlink communication load to linearly increase with the total number of tasks. In this paper, we propose a novel scheme named coded distributed multi-task learning, to reduce the communication loads both in the uplink and downlink. The key idea is to exploit the local information stored at the users during the local training, and utilize a particular repetitive placement and computation on the publicly shared dataset such that coded multicasting opportunities can be created at the server and users. Our method for the first time applies coding strategy to reduce the communication cost for distributed MTL framework. Experiments on real-world dataset show that the proposed scheme can substantially reduce the communication load compared to the traditional uncoded approach.
\subsection{Data-Driven Communications and Networking}
\subsubsection{Data-Driven Edge Resource Provisioning for Inter-Dependent Microservices with Dynamic Load}
abstract:This paper studies how to provision edge computing and network resources for complex microservice-based applications (MSAs) in face of uncertain and dynamic geo-distributed demands. The complex inter-dependencies between distributed microservice components make load balancing for MSAs extremely challenging, and the dynamic geo-distributed demands exacerbate load imbalance and consequently congestion and performance loss. In this paper, we develop an edge resource provisioning model that accurately captures the inter-dependencies between microservices and their impact on load balancing across both computation and communication resources. We also propose a robust formulation that employs explicit risk estimation and optimization to hedge against potential worst-case load fluctuations, with controlled robustness-resource trade-off. Utilizing a data-driven approach, we provide a solution that provides risk estimation with measurement data of past load geo-distributions. Simulations with real-world datasets have validated that our solution provides the important robustness crucially needed in MSAs, and performs superiorly compared to baselines that neglect either network or inter-dependency constraints.
\subsubsection{Load Balancing and User Association Based on Historical Data}
abstract:With the rapid increase of demand on mobile data traffic of user equipment (UE), network operators have begun to deploy abundant heterogeneous base stations (BSs) to ensure the quality of service (QoS) of UEs, which will cause new problems such as network congestion and load imbalance. If the pattern of user association (UA) can be adjusted in accordance with the traffic prediction results, the performance of system will be greatly improved. Therefore, a new neural network approach based on spatial and temporal characteristics of traffic data is firstly proposed for traffic prediction. The fluctuations of traffic in the future week are predicted by the proposed method. Then, UA is represented as a problem of maximizing the utility function of load balancing index, and a load-aware UA algorithm (LAUA) which aims to achieve a proactive load balancing is proposed, which can dynamically adjust the load status, ensure the QoS of UEs, and achieve the long-term stability of the system. Experimental results show that compared to the classic UA strategies, the most optimal load distribution is realized by LAUA.
\subsubsection{WiMate: Location-independent Material Identification Based on Commercial WiFi Devices}
abstract:Material identification is playing an increasingly important role in our daily lives such as public security checks. X-ray-based technologies are highly radioactive because they rely on specialized devices to transmit high-frequency signals. Ultrasound-based technologies are cumbersome due to their large size. RF-based approaches necessitate the use of RFID which is usually expensive to be used in home and office environments. To this end, WiFi-based material identification approach has emerged recently as a low-cost yet effective alternative. In this paper, we propose WiMate, a noncontact material identification system leveraging only off-the-shelf WiFi devices. The key enabler of WiMate is a novel theoretical model we build to characterize how the electromagnetic wave decays when penetrating different materials. Our model identifies a unique feature for each material that only depends on the material itself. Consequently, we can leverage this feature coupling with the machine learning techniques for robust and accurate material identification. We prototype WiMate using low-cost commodity WiFi devices and evaluate its performance in real-world. The empirical study shows that WiMate can identify six different materials, i.e., board, paperboard, nickel, wood chip, iron and titanium, with an average accuracy of 96.20%.
\subsubsection{Deep Reinforcement Learning Based Big Data Resource Management for 5G/6G Communications}
abstract:With the advent of the Internet of Everything era, communication data has exploded, which requires more communication resources, such as frequency, time, and energy. In this context, this paper presents a machine learning-based data packet scheduling scheme to achieve efficient data packet transmission in the 5G/6G communication systems. To minimize the average number of packet overflows (APNO), we propose distributed deep deterministic policy gradient (DDPG)-based algorithm for multidimensional resource scheduling. To improve the algorithm stability and training efficiency, the strategy of centralized training and distributed execution is adopted, and an Action Adjuster is designed. The proposed algorithm enables the multidimensional resource management of the 5G/6G communication systems without any information interaction between each agent. Simulation results show that the proposed Action Adjuster DDPG algorithm achieves faster convergence and less data overflow compared to other benchmark algorithms.
\subsubsection{Generative Adversarial Classification Network with Application to Network Traffic Classification}
abstract:Large datasets in machine learning often contain missing data, which necessitates the imputation of missing data values. In this work, we are motivated by network traffic classification, where traditional data imputation methods do not perform well. We recognize that no existing method directly accounts for classification accuracy during data imputation. Therefore, we propose a joint data imputation and data classification method, termed generative adversarial classification network (GACN), whose architecture contains a generator network, a discriminator network, and a classification network, which are iteratively optimized toward the ultimate objective of classification accuracy. For the scenario where some data samples are unlabeled, we further propose an extension termed semi-supervised GACN (SS-GACN), which is able to use the partially labeled data to improve classification accuracy. We conduct experiments with real-world network traffic data traces, which demonstrate that GACN and SS-GACN can more accurately impute data features that are more important for classification, and they outperform existing methods in terms of classification accuracy.
\subsection{eHealth - Session 1}
\subsubsection{V2X-based COVID-19 Pandemic Severity Reduction in Smart Cities}
abstract:In a jiffy after the outbreak of the 2019 novel coronavirus, also called COVID-19 or SARS-CoV-2, the World Health Organization (WHO) considered it as a pandemic that threatens the demise of humanity. This quick decision was in conjunction with a real situation of biological inability to find a vaccine that can eliminate the virus or at least limit its spread. For that reason, technological intervention and cooperation are needed more than ever to face this pandemic. In this same context, we propose a novel system that deploys Vehicle-to-everything (V2X) technology and Blockchain in collaboration to face such a pandemic. Our proposal is centered on a triple-stage processing i) zone identification and classification based on vehicles' thermal cameras detection, ii) Blockchain-based information storage for more patient medical information privacy, and iii) drones-based zone neutralization processes. Simulation results show that, thanks to the use of Blockchain technology, the network-related performance remain almost unchanged and hence, all Intelligent Transportation System (ITS) including inter-vehicles and inter-drones functionalities are not affected. In addition the additional overhead is very acceptable and does not exceed the 5 Kb in the worst case.
\subsubsection{Activity Detection using 2D LIDAR for Healthcare and Monitoring}
abstract:Monitoring elderly people living alone is of the utmost importance given the amount of risk they are exposed to. Being aware of the activities of the elderly person in real time could help prevent/detect dangerous event that might occur such as falling. In this paper, we propose a method for activity detection using a 2D LIght Detection and Ranging (LIDAR) and deep learning. Unlike conventional work, where an activity refers to moving from one position to another, we use the term "activity" to refer to a set of movements including walking, standing, falling and sitting. Not only does our approach detect these activities, but it also identifies a given person from his gait, and identifies unsteady gait (i.e., when he is about to fall or feeling dizzy). Throughout our experiments, we show that the proposed approach could reach an accuracy equal to 92.3% and 91.3% in activity and unsteady gait detection, respectively. It is also capable of identifying up to 3 people's gait with an accuracy equal to 92.4% using 10 seconds of walking data.
\subsubsection{Non-contact Heartbeat Detection Based on Beam Diversity Using Multibeam Doppler Sensor}
abstract:Heartbeat detection is a promising technology for health care and is in great demand in many applications. For non-contact and non-invasive heartbeat detection, Doppler sensor-based heartbeat detection methods have been investigated extensively. However, since the conventional methods use a single beam Doppler sensor, the accuracy of heartbeat detection tends to degrade, when the SNR (Signal-to-Noise Ratio) of heartbeat components for the beam direction is low. In this paper, we propose a non-contact heartbeat detection method based on beam diversity using a multibeam Doppler sensor. The effects of noises and the corresponding SNRs of heartbeat components over the received signal differ from one beam to another. Based on this fact, the proposed method detects heartbeat by exploiting beam diversity of the received signals. The experimental results showed that compared to the detection method using a single beam, our proposed method using multiple beams detected heartbeat accurately, which indicates that exploiting the beam diversity can lead to the improvement of heartbeat detection accuracy.
\subsubsection{Learning-Based Posture Detection Using Purely Passive Magneto-Inductive Tags}
abstract:We propose a novel low-cost and low-complexity approach that uses the magnetic near-field detuning caused by passive coils in order to detect and classify movements. This idea can enable ubiquitous health monitoring applications on or within the body. In this work, we employ our idea with the specific goal of detecting human body postures with only a few measuring nodes. More precisely, we place resonantly loaded passive coils (tags) on a person's limbs and investigate how the sole presence of these coils affects the input impedances of secondary coils (anchors) which are located on the torso. Due to the strong magnetic near-field coupling of all coils, each different posture changes the inter-coil links and hence leads to a unique set of impedance measurements. These impedance patterns can conversely be learned to use them for classification purposes. In order to verify this idea, we simulate a joint-based human body model and use numerical approximations for the magnetic-near field coupling to generate realistic noisy data sets of impedance measurements. Based on these data sets, we train support vector machines, which we subsequently use to classify the postures on a separate testing set. We investigate how robust this procedure is against increasing noise levels and variations of the postures. In this process, we identify appropriate operating points that lead to an average classification accuracy of 94 percent.
\subsection{eHealth - Session 2}
\subsubsection{Patient-Driven Network Selection in multi-RAT Health Systems Using Deep Reinforcement Learning}
abstract:The recent pandemic along with the rapid increase in the number of patients that require continuous remote monitoring imposes several challenges to support the high quality of services (QoS) in remote health applications. Remote-health (r-health) systems typically demand intense data collection from different locations within a strict time constraint to support sustainable health services. On the contrary, the end-users with mobile devices have limited batteries that need to run for a long time, while continuously acquiring and transmitting health-related information. Thus, this paper proposes an adaptive deep reinforcement learning (DRL) framework for network selection over heterogeneous r-health systems to enable continuous remote monitoring for patients with chronic diseases. The proposed framework allows for selecting the optimal network(s) that maximizes the accumulative reward of the patients, while considering the patients' state. Moreover, it adopts an adaptive compression scheme at the patient-level to further optimize the energy consumption, cost, and latency. Our results depict that the proposed framework outperforms the state-of-the-art techniques in terms of battery lifetime and reward maximization.
\subsubsection{An Experience with Mental Health Professionals using Long Lasting Memories Program}
abstract:The integration of Technologies in the Mental Health field helps to prevent the cognitive impairment of patients. The main objective of our paper is to analyze the usability of the Long Lasting Memories program applied to Mental Health professionals. This study sample consisted of 23 participants, Mental Health professionals, of which 52.2% are psychologists and 47.8% are qualified assistants. Participants were given a usability questionnaire with different variables once the intervention was concluded with the program. The results obtained from the questionnaire applied were analyzed taking into account different aspects as evaluation of use ease of the program, satisfaction and sustainability. From the results collected through the program it can be concluded that, from the professional's point of view in charge of the intervention, the usability degree is high.
\subsubsection{SDN-Controller Triggered Dynamic Decision Control Mechanism for Healthcare IoT}
abstract:Due to the lack of an integrated communication and computation architecture for Software-Defined Healthcare IoT (SD-HI), provisioning critical services is challenging. In this paper, we propose SD-Health, an edge-based decision making and task allocation (EDT) scheme for SD-HI. The proposed SD-HI network uses Machine Learning (ML)-based approach to predict the criticality of flows and location of mobile devices. Based on the predicted values, the controller delegates the required EDT module to the respective edge node. The controller identifies the future healthcare-related decisions for an edge node and prepares the module accordingly. The ML-based trajectory prediction allows to find the future location of mobile devices in the network. Once the location of the mobile device is predicted, a set of computation task is dynamically allocated to the edge node. Through performance analysis, we observe that SD-Health has a significant improvement in latency by 43.3% and energy consumption by 30%, compared to the existing state-of-the-art, along with a fair improvement in packet delivery ratio.
\subsubsection{Optical Fiber Fabry-Perot based Spirometer for Pulmonary Health Assessment: Concept Evaluation}
abstract:In this paper, we present an optical fiber based approach to accurately evaluate the respiratory capacity of individuals through spirometry. The proposed system converts the air flow rate into strain variations in an optical fiber, which will modulate the spectral response of a Fabry-Perot interferometer (FPI) micro-cavity, created in the fiber core. Simulation studies were performed to verify the proposed theoretical approach. Additionally, preliminary results concerning the study of the system's reliability are also presented, which confirm the suitability of using FPI sensors for the evaluation of spirometry parameters. The simulation and implementation results showcase the accuracy of the measurements performed by the proposed solution, confirming its appropriateness to be used as a smart low-cost eHealth spirometer. This is the first time such FPI approach is used with this purpose.
\subsection{eHealth - Session 3}
\subsubsection{Enhancement of COVID-19 Detection by Unravelling its Structure and Selecting the Optimal Attributes}
abstract:According to the current unprecedented pandemic situation, we realise that we cannot respond to every contagion novel virus as fast as possible, either by vaccination or medication. Therefore, it is paramount for the sustainable development of antiviral urban ecosystems to promote an outbreak's early detection, control and prevention. The structure of an antivirus-based multi-generational smart-city framework could be crucial to a post-COVID-19 urban environment. Humanitarian efforts in the pandemic's framework deployed novel technological solutions based on the Internet of Things (IoT), Machine Learning, Cloud Computing and Artificial Intelligence (AI). We aim to contribute through our research work by improving real-time detection using data mining in collaboration with machine learning technique. Initially, for detection, we propose an innovative system that could detect in real-time virus propagation based on the density of the airborne COVID-19 molecules-the proposal based on the detection through the isothermal amplification RT-Lamp [1]. We also propose real-time detection by spark-induced plasma spectroscopy during the internal airborne transmission process [17]. The novelty of this research work, called characteristic subset selection, is based on identifying irrelevant data. By deducting the unrelated information dimension, machine learning algorithms would operate more efficiently. Therefore, it optimises data mining and classification in high-dimensional medical data analysis, particularly in the effective detection of COVID-19. It can play an essential role in providing timely detection with critical attributes and high accuracy. We elaborate the teaching-learning method optimisation to achieve the optimal set of features for the detection.
\subsubsection{Exploiting Passive Beamforming of Smart Speakers to Monitor Human Heartbeat in Real Time}
abstract:Currently, cardiac diseases have become one of the biggest health concerns. Existing heartbeat monitoring methods either require dedicated intrusive devices (e.g., ECG devices) that suffer high costs or leverage video camera analyses that are light-sensitive. In this paper, leveraging the acoustic signals sent by a speaker and received by a microphone array, we develop a prototype system to achieve the contactless and low-cost heartbeat monitoring. In particular, while we exploit the passive beamforming to enhance the user's heartbeat signal, we design a filtering method in frequency domain to remove the line-of-sight (LoS) impact and retain the target-reflected signals, and propose a wideband time-delay method to estimate the direction of arrival of target-reflected signal. Thus, our prototype is able to robustly estimate the human heartbeat and push the limit of acoustic sensing range. The experimental results show that our prototype achieves a heart rate monitoring at 1.7 m with the estimation error of 0.5 bpm, which is comparable to ECG or other contact-based solutions.
\subsubsection{Remote Sensing of Heartbeat based on Space Diversity Using MIMO FMCW Radar}
abstract:Remote sensing of heartbeat offers various applications in the medical and health care fields. To realize non-contact heartbeat detection, an FMCW (Frequency Modulated Continuous Wave) radar-based heartbeat detection method has been investigated. The conventional FMCW radar-based heartbeat detection method estimates a range from an FMCW radar to a subject and extracts heartbeat components from phase changes for the range. However, the range suitable for extracting heartbeat components can change over time due to respiration and body fluctuation. Thus, when the SNR (Signal-to-Noise Ratio) of heartbeat components over phase changes is low at the estimated range, the accuracy of heartbeat detection tends to degrade. In this paper, we propose a MIMO (Multiple-Input Multiple-Output) FMCW radar-based heartbeat detection method based on space diversity. A MIMO FMCW radar can estimate the range for multiple beam directions and obtain phase changes for a space specified with the range and the beam direction. The SNR of heartbeat components over phase changes differs from one space to another. Taking it into account, the proposed method detects heartbeat by exploiting the space diversity of phase changes. The experimental results showed that compared to the detection method using only one phase changes, the proposed method using phase changes for multiple spaces detected heartbeat accurately, which is brought by the diversity effect of phase changes for multiple spaces.
\subsubsection{Energy Efficient Capacitive Body Channel Access Schemes for Internet of Bodies}
abstract:The Internet of bodies is a network of wearable, ingestible, injectable, and implantable smart objects located in, on, and around the body. Although radio frequency (RF) systems are considered a default choice for on-body communications, which should be within <5 cm vicinity of the human body, highly radiative RF propagation unnecessarily covers several meters beyond the human body. This intuitively degrades energy efficiency, leads to interference and co-existence issues, and exposes sensitive personal data to security threats. The capacitive body channel communications (BCC) is an alternative solution that confines transmission (between 10 kHz-100 MHZ) to the human body, which is more conductive than air. Since BCC has a negligible signal leakage and lower propagation loss, it has been reported to provide better physical layer security and reach nJ/bit to pJ/bit energy efficiency. Accordingly, this paper investigates orthogonal and non-orthogonal capacitive body channel access schemes for ultra-low-power IoB nodes. We present optimal uplink and downlink power allocations in closed-form, which deliver better fairness and network lifetime than benchmark numerical solvers. For a given bandwidth and data rate requirement, We also derive the maximum affordable number of IoB nodes for both directions of orthogonal and non-orthogonal schemes.
\subsection{eHealth - Session 4}
\subsubsection{Information Theoretic Key Agreement Protocol based on ECG signals}
abstract:Wireless body area networks (WBANs) are becoming increasingly popular as they allow individuals to continuously monitor their vitals and physiological parameters remotely from the hospital. With the spread of the SARS-CoV-2 pandemic, the availability of portable pulse-oximeters and wearable heart rate detectors has boomed in the market. At the same time, in 2020 we assisted to an unprecedented increase of healthcare breaches, revealing the extreme vulnerability of the current generation of WBANs. Therefore, the development of new security protocols to ensure data protection, authentication, integrity and privacy within WBANs are highly needed. Here, we targeted a WBAN collecting ECG signals from different sensor nodes on the individual's body, we extracted the inter-pulse interval (i.e., R-R interval) sequence from each of them, and we developed a new information theoretic key agreement protocol that exploits the inherent randomness of ECG to ensure authentication between sensor pairs within the WBAN. After proper pre-processing, we provide an analytical solution that ensures robust authentication; we provide a unique information reconciliation matrix, which gives good performance for all ECG sensor pairs; and we can show that a relationship between information reconciliation and privacy amplification matrices can be found. Finally, we show the trade-off between the level of security, in terms of key generation rate, and the complexity of the error correction scheme implemented in the system.
\subsubsection{PAMI-Anonymous Password Authentication Protocol for Medical Internet of Things}
abstract:With the continuous maturity of Internet of Things(IoT) technology, it has begun to be frequently used in all walksof life to improve people's work efficiency and living standards.The wide use of IoT in the medical field makes it convenient forpatients to obtain medical services, and also enables doctors toobtain patients' physical conditions more timely and accurately,so as to formulate more efficient treatment plans. However, whenpeople enjoy the convenience of medical IoT, how to ensure thesecurity communication of medical IoT devices and privacy ofpatients are all problems that can not be ignored. Based onthese two important issues, this paper proposes an anonymouspassword authenticated key exchange (APAKE) protocol PAMI,which only needs a low entropy password to complete the mutualauthentication between medical device and telemedicine server,so as to negotiate a high entropy session key. Under the standardmodel, we use formal proof to obtain the security conclusion ofPAMI protocol. Finally, compared with other protocols of thesame type, the computational cost of our protocol is greatlyreduced.
\subsubsection{Daily Activities Monitoring of Users for Well-Being and Stress Correlation Using Wearable Devices}
abstract:It has been largely demonstrated how human behaviour can have a great impact on the quality of the life. Some habits, such as those related to sleeping, exercising and working, directly affect people's psycho-physical health, either positively or negatively. The research has been increasingly focusing in better understanding how some behaviours, or changes in someone's usual habits, can be triggers to early recognising and predicting bad health conditions that might even be the warning signal of pathological conditions such as depressive disorders or neurodegenerative diseases. Non-invasive wearable health monitoring systems have the potential of being a key technology to this purpose, because they are easy-to-use and are not perceived as intrusive by users. In this paper, a system that makes use of popular commercial wrist-wearable devices to find the correlation between the monitored users' activities and their stress and well-being conditions, as subjectively self-assessed by them, is proposed. The paper aims to present a methodology to automatically learn which users' activities can be associated with positive and negative health conditions, so that they can be later predicted as soon as the first signals are detected by wearable devices. The paper further presents the implementation and preliminary results of a first prototype of the proposed system, which monitors users' sleep and activity and assesses the correlation with stress levels and illness conditions.
\subsubsection{A Wearable Wireless Monitoring System for the Detection of Pulmonary Edema}
abstract:In this paper we investigate the feasibility of a simple wearable system that can be used at home to detect or monitor fluid buildup in the lungs. This is a medical condition referred to as pulmonary edema. A methodology has been developed to computationally emulate human lungs with various levels of fluid in the alveoli. The proposed wearable system is composed of several small wearable antennas located on the chest and back area. The antennas will operate at MedRadio frequency band and will be optimized for signal penetration through the body. The frequency and time responses of the communication channel between these antennas for the lung models with varying levels of fluid have been measured and analyzed. The results show a correlation between the channel response and the level of fluids inside the lungs. This correlation can potentially be exploited by a simple wearable system to predict the onset of pulmonary edema for patients living in remote areas or people who need to be continuously monitored.
\subsection{eHealth - Session 5}
\subsubsection{An Efficient eNB Selection and Traffic Scheduling Method for LTE Overlay IoT Communication Networks}
abstract:Smart or electronic healthcare is undergoing rapid change from the traditional specialist and hospital- centered style to a disseminated patient-centered using Internet of Things (IoT). Presently, 4G and other advanced communication standards are utilized in healthcare for intelligent healthcare services and applications. Traffic handling is an essential feature for the flexible interoperability of the internet of things (IoT) with other heterogeneous communication networks. Efficient traffic handling controls latency and communication failures due to random access and collision in cellular network overlay IoT. It is challenging for existing communication technology to achieve the necessities of time-sensitive and very dynamic healthcare applications of the future. In this manuscript, adaptive eNB selection with traffic scheduling (AeS-TS) is proposed to improve the efficiency of IoT-long term evolution (LTE) networks. AeS-TS works in two phases: adaptive eNB selection and gateway traffic scheduling. In eNB selection, traffic-aware radio infrastructure selection with the offloading feature is presented. eNB selection is preceded by using a preference function to improve the acceptance rate of incoming IoT traffic and minimize transmission loss. In the traffic scheduling phase, sequential and level-based slot transmission is adapted to improve traffic forwarding quality. The slots are selected by analyzing the error in time function using the recurrent learning process.
\subsubsection{Collection and Classification of Human Posture Data using Wearable Sensors}
abstract:Analysis of human posture has many applications in the field of sports and medical science including patient monitoring, lifestyle analysis, elderly care etc. It is important to understand if a person is healthy (in terms of his everyday posture) or is suffering with a joint/bone disease as reflected by his incorrect posture. Many of the works in this area have been based on computer vision techniques. These are limited in providing real-time solution. The aim of the proposed work is to classify the human posture during three different activities (standing, sitting and sleeping/lying) as a healthy or unhealthy one. This is done by applying machine learning techniques on a large posture dataset which is collected with the help ofMPU-6050 sensors mounted on multiple positions on the body. The performance evaluation of the proposed work reveals that the proposed work is efficient enough to classify the postures accurately.
\subsubsection{BePOCH: Improving Federated Learning Performance in Resource-Constrained Computing Devices}
abstract:Inference with trained machine learning models is now possible with small computing devices while only a few years ago it was run mostly in the cloud only. The recent technique of federated learning offers now a way to do also the training of the machine learning models on small devices by distributing the computing effort needed for the training over many distributed machines. But the training on these low-capacity devices takes a long time and often consumes all the available CPU resource of the device. Therefore, for federated learning to be done by low-capacity devices in practical environments, the training process must not only target for the highest accuracy, but also on reducing the training time and the resource consumption. In this paper, we present an approach which uses a dynamic epoch parameter in the model training. We propose the BePOCH (Best Epoch) algorithm to identify what is the best number of epochs per training round in federated learning. We show in experiments with medical datasets how with the BePOCH suggested number of epochs, the training time and resource consumption decreases while keeping the level of accuracy. Thus, BePOCH makes machine learning model training on low-capacity devices more feasible and furthermore, decreases the overall resource consumption of the training process, which is an important aspect towards greener machine learning techniques.
\subsubsection{Enabling Real-Time Dashboards for Anxiety Risk Classification Using the Internet of Things}
abstract:The ubiquity of sensor technology and the Internet of Things prompted us to propose to develop a real-time digital dashboard to visualize the anxiety risks of populations during a pandemic, as in the case of COVID-19. To this end, here we provide an end-to-end communication architecture to detect physiological data related to heart rate, blood pressure, and SPO_2, using wearable sensors and communicate them to remote servers. Based on this collected data, the centralized dashboard will classify in real time the patients of each geographic region involved according to a specific attribute, i.e., normal, mild, moderate, high, severe, or extreme. In addition, we also propose to incorporate the emerging technologies of Space Time Frequency Spreading (STFS) and Space-Time Spreading-Aided Indexed Modulation (STS-IM) for the design of the communication links. It has been found that the integration of STFS and STS-IM promises to reduce the likelihood of data disruption for the proposed architecture.
\subsection{Federated learning}
\subsubsection{Entropy-Driven Stochastic Policy for Fast Federated Learning in Beyond 5G Edge-RAN}
abstract:Scalability and sustainability are the corner stones to unleash the potential of beyond fifth-generation (B5G) ultra-dense networks that are expected to handle massive and heterogeneous services. This implies that the transport of the underlying raw monitoring data should be minimized across the network, and urges to bring the analysis functions closer to the data collection points. While federated learning (FL) is an efficient tool to implement such a decentralized strategy, real networks are generally characterized by time- and space-varying users distributions, traffic profiles, and channel conditions. This makes the data collected across different points non independent and identically distributed (non-IID), which is challenging for FL tasks. To cope with this issue, we first introduce a new a priori metric that we call dataset entropy, whose role is to capture the distribution, the quantity of information, the unbalanced structure and the "non-IIDness" of a dataset independently of the models. This entropy is calculated using a clustering scheme based on a similarity matrix defined over both the features and the supervised output spaces, and is targeting classification as well as regression tasks. The FL aggregation server then uses the reported dataset entropies to devise i) an entropy-based federated averaging scheme, and ii) a stochastic participant selection policy to significantly stabilize the training, minimize the convergence time, and reduce the corresponding computation cost. Numerical results are provided to illustrate all these advantages.
\subsubsection{Bayesian AirComp with Sign-Alignment Precoding for Wireless Federated Learning}
abstract:In this paper, we consider the problem of wireless federated learning based on sign stochastic gradient descent (signSGD) algorithm via a multiple access channel. When sending locally computed gradient's sign information, each mobile device requires to apply precoding to circumvent wireless fading effects. In practice, however, acquiring perfect knowledge of channel state information (CSI) at all mobile devices is infeasible. In this paper, we present a simple yet effective precoding method with limited channel knowledge, called sign-alignment precoding. The idea of sign-alignment precoding is to protect sign-flipping errors from wireless fadings. Under the Gaussian prior assumption on the local gradients, we also derive the mean squared error (MSE)-optimal aggregation function called Bayesian over-the-air computation (BayAirComp). Our key finding is that one-bit precoding with BayAirComp aggregation can provide a better learning performance than the existing precoding method even using perfect CSI with AirComp aggregation.
\subsubsection{GDST: Global Distillation Self-Training for Semi-Supervised Federated Learning}
abstract:Federated Learning (FL) refers to the machine learning scheme that enables decentralized model training over massive separate data sources without privacy concerns. However, existing works rarely consider difficulty of obtaining sufficient data labels due to uncontrollable user behavior, especially in cross-device FL scenarios. In this paper, we consider semi-supervised federated learning (SSFL) setups and mainly focus on the disjoint scenario where local clients only have access to unlabeled data. By integrating self-training scheme for unlabeled data, we propose self-training loss as part of local training objective within federated learning framework. To further stablize and improve the learning process, we propose global distillation loss that utilize output logits of global model for per client-sample as supervision and also soften such distillation by temperature to obtain more discriminative information. Based on self-training and global distillation loss, combined with server-side training, we propose Global Distillation Self-Training (GDST) Federated Learning algorithm, which enables to distributedly learn a global model in the disjoint scenario of SSFL. Finally, we do sufficient ablation study to explore the role of each component of our GDST method, experimentally guarantee the interpretability.
\subsubsection{Energy-Efficient and Federated Meta-Learning via Projected Stochastic Gradient Ascent}
abstract:In this paper, we propose an energy-efficient federated meta-learning framework. The objective is to enable learning a meta-model that can be fine-tuned to a new taskwith a few number of samples in a distributed setting and at low computation and communication energy consumption. We assume that each task is owned by a separate agent, so a limited number of tasks is used to train a meta-model. Assuming eachtask was trained offline on the agent's local data, we propose a lightweight algorithm that starts from the local models of all agents, and in a backward manner using projected stochastic gradient ascent (P-SGA) finds a meta-model. The proposed method avoids complex computations such as computing hessian, double looping, and matrix inversion, while achieving high performance at significantly less energy consumption compared to the state-of-the-art methods such as MAML and iMAML on conducted experiments for sinusoid regression and image classification tasks.
\subsubsection{Privacy-Preserving Federated Reinforcement Learning for Popularity-Assisted Edge Caching}
abstract:In this paper, we investigate the problem of edge caching (EC) optimization in a multi-user privacy-preserving mobile edge computing (MEC) system. The time-varying content popularity is considered and the primary objective is to maximize the EC hit rate on each caching entity in the distributed network. To this end, we introduce the concept of local and global popularities and cast the time-varying local popularities as model-free Markov chains. Next, an unsupervised recurrent federated learning (URFL) algorithm is proposed to predict the popularities while achieving privacy-preserving goal. The underlying distributed optimization problem is then reformulated as a distributed Markov decision process and solved by the privacy-preserving distributed deep deterministic policy gradient algorithm incorporating the URFL algorithm. Simulation results demonstrate the superiority of the proposed scheme in terms of prediction error and hit rate over the baseline methods.
\subsection{Full Duplex Systems and Optimization}
\subsubsection{Non-Coherent Fast-Forward Relays for Full-Duplex Jamming Attack}
abstract:This work addresses a strategy to mitigate jamming attack on low-latency communication by a Full-Duplex (FD) adversary in fast-fading channel conditions. The threat model is such that the FD adversary can jam a frequency band and measure the jammed band's power level. We first point out that due to the presence of this FD adversary, Frequency Hopping (FH) fails. We then propose a fast-forward cooperative relaying scheme, wherein the victim node hops to the frequency band of a nearby FD helper node that fast-forwards the victim's symbol along with its symbol. At the same time, the victim and the helper cooperatively pour some fraction of their power on the jammed band to engage the adversary. Due to fast-fading channel conditions, the victim and the helper use amplitude based non-coherent signalling referred to as Non-Coherent Fast-Forward Full-Duplex (NC-F2FD) relaying. To minimize the error-rate of this strategy, we jointly design the constellations at the helper node and the victim node by formulating an optimization problem. Using non-trivial results, we first analyse the objective function and then, based on the analytical results, propose a low-complexity algorithm to synthesize the fast-forwarded constellations. Through simulations, we show that the error performance of the victim improves after employing our countermeasure.
\subsubsection{Robust Power and Position Optimization for the Full-Duplex Receiver in Covert Communication}
abstract:Covert communication achieved by using the full-duplex (FD) receiver has wide applications. Specifically, the FD receiver can emit artificial noise to prevent the signal of transmitter from detecting by the illegtimate warden. In this paper, we investigate the robust joint power and position optimization (JPPO) for the full-duplex receiver (i.e., Bob) in the presence of the uncertainties of the warden (i.e., Willie). We first analyze the effect of the warden's position uncertainty on the performance of covert communication. Then, by using robust optimization technique, we maximize the effective covert throughput of transceivers by optimizing the power and position of the Bob constraining the sufficient covertness requirment. Theoretical analysis indicate that the effective covert throughput between Alice and Bob is inversely proportional to the distance between them within the deployable zone (DZ) satisfying the covert condition, and the optimal position of Bob is on the boundary of the DZ near Alice. Finally, simulation results verified our conjecture.
\subsubsection{Analysis of Statistical CSI-based Optimized Phase-Shift IRS-aided FD mMIMO System}
abstract:We consider a multi-user system where a massive multi-input-multi-output (mMIMO) full-duplex (FD) base-station (BS) communicates with multiple FD users via an intelligent reflecting surface (IRS). We derive novel uplink and downlink spectral efficiency (SE) lower bound expressions when the BS estimates composite BS-IRS-user channels. The lower bounds are derived considering spatially-correlated IRS and user channels, and require only statistical channel state information (CSI). We propose a projected gradient ascent based IRS phase optimization algorithm, which also uses only statistical CSI, and enables the system to achieve a higher SE in the presence of the loop and inter-user interferences. We analytically investigate the dependence of SE on the IRS phase for spatially correlated channels considered herein. We show that the proposed analysis can help in increasing the SE by appropriate IRS placement.
\subsubsection{Energy-Efficient Beamforming Design for User-Centric Full-Duplex Wireless Backhaul Networks}
abstract:In this paper, we investigate an energy-efficient beamforming design in a user-centric full-duplex backhauling network, where simultaneous transmission and reception on the same frequency band for access and backhaul links are enabled by full-duplex access points. Specifically, multicast transmission is adopted by macro base station and cooperative transmission is adopted by access points to serve users. We first formulate an optimization problem to maximize the network energy efficiency while taking the power consumed by self-interference cancellation at full-duplex access points into consideration. This problem is formulated as a nonlinear fractional programming and is transformed into an equivalent subtractive form by Dinkelbach method. The equivalent optimization problem is still non-convex. To make it more tractable, we first adopt semidefinite relaxation to remove part of non-convexity, and then adopt the successive convex approximation method to further deal with the rest of non-convex parts, and finally propose an iterative algorithm. Simulation results demonstrate the superiority of the proposed algorithm and provide insights into the design of full-duplex backhauling network.
\subsection{Globecom 2021 Aerial Communications Session 1}
\subsubsection{Finite-Blocklength Multi-Antenna Covert Communication Aided By A UAV Relay}
abstract:We propose a UAV-relayed covert communication scheme with finite blocklength to maximize the effective transmission rate from the transmitter to the legitimate receiver against a flying warden. The transmitter adopts the maximum ratio transmission and the relay performs Gaussian signaling transmission to cause uncertainty at the warden. First, the optimal detection thresholds are derived at the warden towards the transmitter and the relay, respectively. Then, the hovering location of the warden is optimized to maximize the summation of relative entropies from the transmitter and the relay, which can greatly threaten the covertness.With this worst covert situation, the blocklength and transmit power at the transmitter and the relay are jointly optimized under the constraint of the error detection probability to maximize the effective transmission rate from the transmitter to the legitimate receiver. Numerical results are provided to demonstrate the effectiveness of the proposed UAV-relayed covert communication scheme.
\subsubsection{Joint Data Collection and Transmission in 6G Aerial Access Networks}
abstract:The aerial access network (AAN) is a significant issue in the sixth generation (6G) technologies. In this work, we focus on the terrestrial data collection and transmission by AAN. In detail, high altitude platforms (HAPs) are considered as the aerial access devices and low earth orbit (LEO) satellites assist the data collected by HAPs to complete transmission. To deal with the intractable dynamic topology of AAN, the time expanding graph (TEG) is employed to represent the multiple resources and depict the data flow transmission process. Based on TEG, we aim to maximize the total data received at the ground data processing center, considering the multiple resource restrictions of HAPs and LEO satellites, as well as the flow conservation constraints in TEG. The problem is in the form of mixed integer programming, and it is intractable to obtain the optimal solution, especially in large-scale AAN. To alleviate the intractability, we propose the Benders decomposition based algorithm to obtain the optimal solution within an acceptable time complexity. Simulations are conducted and numerical results verify the effectiveness and efficiency of the proposed algorithm.
\subsubsection{Learning to Navigate for Secure UAV Communication}
abstract:In this paper, we investigate the navigation for unmanned aerial vehicle (UAV) in the secure communication system, where we design the UAV's navigation/trajectory to ensure the Quality of Service (QoS) with the Base Station (BS) in the existence of multiple unknown-location dynamical eavesdroppers and jammers. To this end, we formulate a UAV trajectory optimization problem to minimize its mission completion time with QoS and security constraints. The imperfect information, dynamical communication environment, and non-convexity make the problem intractable. For these reasons, we propose a novel solution approach, namely Model-Assisted Reinforcement Learning (MARL) algorithm, where the communication system model is embedded into Deep Reinforcement Learning (DRL) framework to ensure secure communication and shorten the learning process. Numerical results show that our proposed methodology can safeguard the security and find the shortest way to finish the mission.
\subsubsection{Credibility Computation Offloading Based Task-Driven Routing Strategy for Emergency UAVs Network}
abstract:By offloading tasks on the drones in the transmission route over an emergency UAVs network, the rescue efficiency can be significantly enhanced. Most routing algorithms and task offloading strategies relied on the drones' future location information. However, due to the harsh conditions of the disaster scene, drones' future locations are unpredictable, which results in extremely dynamic topology and brings a great challenge for the credibility of the scheduling result. Motivated by the aforementioned problem, a credibility Computation Offloading based Task-driven routing (COT) strategy for emergency UAVs network is proposed, where a two-step Weighted time Expanded Graph (WEG) is constructed to cope with the network dynamics. Then, based on the two-step WEG, the COT is formulated to offload the tasks on the data transmission route to minimize the task processing latency and realize the computing while transmitting. Moreover, a novel credibility model is conceived to enhance the credibility of the scheduling result. Finally, the binary particle swarm optimization (BPSO) algorithm is adopted to solve this problem. The simulation results validate that the proposed COT leads to significant performance improvement in latency compared with cloud computing and local computing.
\subsubsection{Model-aided Deep Reinforcement Learning for Sample-efficient UAV Trajectory Design in IoT Networks}
abstract:Deep Reinforcement Learning (DRL) is gaining attention as a potential approach to design trajectories for autonomous unmanned aerial vehicles (UAV) used as flying access points in the context of cellular or Internet of Things (IoT) connectivity. DRL solutions offer the advantage of on-the-go learning hence relying on very little prior contextual information. A corresponding drawback however lies in the need for many learning episodes which severely restricts the applicability of such approach in real-world time- and energy-constrained missions. Here, we propose a model-aided deep Q-learning approach that, in contrast to previous work, considerably reduces the need for extensive training datasamples, while still achieving the overarching goal of DRL, i.e to guide a battery-limited UAV towards an efficient dataharvesting trajectory, without prior knowledge of wireless channel characteristics and limited knowledge of wireless node locations. The key idea consists in using a small subset of nodes as anchors (i.e. with known location) and learning a model of the propagation environment while implicitly estimating the positions of regular nodes.Interaction with the model allows us to train a deep Q-network (DQN) to approximate the optimal UAV control policy. We show that in comparison with standard DRL approaches, the proposed model-aided approach requires at least one order of magnitude less training data samples to reach identical data collection performance, hence offering a first step towards making DRL a viable solution to the problem.
\subsection{Globecom 2021 Aerial Communications Session 2}
\subsubsection{RIS-assisted Aerial Backhaul System for UAV-BSs: An Energy-efficiency Perspective}
abstract:In this paper, we propose a novel wireless backhaul architecture, mounted on a high-altitude aerial platform, which is enabled by reconfigurable intelligent surface (RIS). We assume a sudden increase in traffic in an urban area, and to serve the ground users therein, authorities rapidly deploy unmanned-aerial-vehicle base-stations (UAV-BSs). In this scenario, since the direct backhaul link from the ground source can be blocked due to several obstacles from the urban area, we propose reflecting the backhaul signal using aerial-RIS and the phase of each RIS element, which leads to an increase in energy-efficiency ensuring the reliable backhaul link for every UAV-BS. We optimize the placement and array-partitioning strategy of aerial-RIS and the phase of each RIS element, which leads to an increase of energy-efficiency under guaranteeing the reliable backhaul link for every UAV-BS. We show that the complexity of our algorithm is upper-bounded by the quadratic order, thus implying high computational efficiency. We verify the performance of the proposed algorithm via extensive numerical evaluations and show that our method achieves an outstanding performance in terms of energy-efficiency compared to benchmark schemes.
\subsubsection{Millimeter-Wave UAV Coverage in Urban Environments}
abstract:With growing interest in mmWave connectivity for unmanned aerial vehicles (UAVs), a basic question is whether networks intended for terrestrial service can provide sufficient aerial coverage as well. To assess this possibility in the context of urban environments,extensive system-level simulations are conducted using a generative channel model recently proposed by the authors.It is found that standard downtilted base stations at street level, deployed with typical microcellular densities, can indeed provide satisfactory UAV coverage. Interestingly, this coverage is made possible by a conjunction of antenna sidelobes and strong reflections. As the deployments become sparser, the coverage is only guaranteed at progressively higher UAV altitudes. The incorporation of base stations dedicated to UAV communication, rooftop-mounted and uptilted, would strengthen the coverage provided their density is comparable to that of the standard deployment, and would be instrumental for sparse deployments of the latter.
\subsubsection{Multimodal Wildfire Surveillance with UAV}
abstract:Wildfires are a growing problem in the US and worldwide. In the last decade we witnessed some of the costliest, most destructive, and deadliest wildland fires on record. This project proposes a vision-based multimodal fire detection system on an Unmanned Aerial Vehicle (UAV, drone) that can be used for early detection of new wildfires, and surveillance of existing ones. In this paper, we present a vision-based aerial sensing system with an on-board intelligent processor and precise fire sensing systems. We create a new open-source UAV system for joint autopiloting and multi-sensory object localization and detection with a tight power budget. We designed a Fire Perception Box multimodal perception hardware that can be installed on our UAV system. To improve the fire scene detection robustness and accuracy, we propose to perform fusion of Visual spectrum (RGB) and infrared (IR) sensors with on-board deep learning based algorithm. Overall, our proposed system is capable of fully onboard real-time visual processing and produces spatial results which can later be utilized to generate the much needed realtime wildfire maps. The effectiveness of the system evaluated based on our own collected Aerial Fire Dataset in a real fire training scenario (80-acre wildfire) in Sacramento, California, as well as other existing fire related datasets.
\subsubsection{User Fairness Optimization for Multi-UAV-Aided NOMA Networks: A Location-Aware Perspective}
abstract:In the blind areas of current fifth generation (5G)networks, e.g, the remote areas, unmanned aerial vehicles (UAVs)can be used to provide on-demand connectivity. To efficientlyserve the sparsely distributed users in these areas, non-orthogonalmultiple access (NOMA) could be adopted to exploit the userdistinguish ability in the power domain. In this paper, we considera NOMA-based multi-UAV-aided network, where a swarm ofcoordinated UAVs transmit messages to unevenly distributedusers through a virtual multiple-input-multiple-output (MIMO)channel. We formulate a power allocation problem to maximizethe minimum user rate to assure fairness in the transmission.Different from existing studies, we use only the large-scalechannel state information (CSI) in the transmission design, whichcharacterizes the basic channel feature, and can be obtainedusing the location information of UAVs/users. By leveraging therandom matrix theory and successive convex optimization tools,we propose an iterative algorithm to solve the problem aftera series of problem transformation. Simulation results showthat the proposed power allocation scheme outperforms existingmethods, which shows the potential of multi-UAV-aided NOMAcommunications for coverage enhancement in remote areas.
\subsubsection{Uplink Data Transmission Based on Collaborative Beamforming in UAV-assisted MWSNs}
abstract:Unmanned aerial vehicles (UAVs) have attracted growing attention in enhancing the performance of mobile wireless sensor networks (MWSNs) since they can act as the aerial base stations (ABSs) and have the autonomous nature to collect data. In this paper, we consider to construct a virtual antenna array (VAA) consists of mobile sensor nodes (MSNs) and adopt the collaborative beamforming (CB) to achieve the long-distance and efficient uplink data transmissions with the ABSs. First, we formulate a high data transmission rate multi-objective optimization problem (HDTRMOP) of the CB-based UAV-assisted MWSN to simultaneously improve the total transmission rates, suppress the total maximum sidelobe levels (SLLs) and reduce the total motion energy consumptions of MSNs by jointly optimizing the positions and excitation current weights of MSN-enabled VAA, and the order of communicating with different ABSs. Then, we propose an improved non-dominated sorting genetic algorithm-III (INSGA-III) with chaos initialization, average grade mechanism and hybrid-solution generate strategy to solve the problem. Simulation results verify that the proposed algorithm can effectively solve the formulated HDTMOP and it has better performance than some other benchmark methods.
\subsection{Globecom 2021 Aerial Communications Session 3}
\subsubsection{Resource Allocation for Covert Wireless Transmission in UAV Communication Networks}
abstract:In this paper, we propose an improper Gaussian signaling (IGS) empowered covert communication strategy in unmanned air vehicle (UAV) assisted communication systems. The ground user (Alice) covertly transmits confidential messages to the UAV amounted based station (Bob) over an overt channel, which is licensed to an existing communication system. While by superimposing the confidential message on the overt channel, the inter-system interference may be incurred. To alleviate the interference, we adopt IGS at the covert communication system, and proper Gaussian signaling (PGS) at the existing communication system. We derive closed-form expressions for the outage probability of the overt and covert transmission link, respectively. Moreover, we formulate a joint transmit power and IGS factor optimization problem to maximize the outage performance of the covert communication system under the constraints of covertness requirement and quality of service of the existing communication system. The optimal solution is derived by leveraging the monotonic properties of objective function and constraints. Finally, simulation results are provided to verify the effectiveness of the proposed covert communication strategy.
\subsubsection{Three-Dimensional Trajectory Design for Post-disaster UAV Video Inspection}
abstract:This paper considers unmanned aerial vehicle (UAV) trajectory design for post-disaster information collection based on video inspection. Unlike the conventional UAV photogrammetric applications that aims to cover the whole area of interest, we model the critical locations in the disaster area as points of interest (PoIs) with heterogeneous resolution requirements. An efficient algorithm is then proposed to design the three-dimensional (3D) UAV trajectory that minimizes the time required for inspecting all the PoIs, subject to the image quality constraints, in terms of the image resolution, produced blur and the number of shots for each PoI. The proposed modeling and trajectory design can greatly reduce the time required for UAV video inspection, which hence enables fast response and rescue planning after the disaster.
\subsubsection{Dynamic Leader Selection in a Master-Slave Architecture-Based Micro UAV Swarm}
abstract:In this paper, we present a method for dynamically selecting UAV leaders in a master-slave communication model in a swarm of micro-UAVs. With growing size of the UAV-swarm in complex missions, it becomes a challenge to control them for efficient execution of the task. In traditional centralized communication model where all UAVs in the swarm are controlled directly through ground control, channel capacity limits the number of UAVs in the swarm which restricts the scalability. In the context of low power miniature drones, we propose a greedy heuristic method to avoid computational complexities and account for the channel qualities and delays (among eligible UAVs) for selecting the UAV leader in real-time. The proposed master-slave model enhances the scalability of the swarm by improving utilization of channel resources. Additionally, the lifetime of the network also decreases on operating with a single UAV leader. Simulation results demonstrate that the proposed dynamic leader selection enhances the lifetime of the entire network with multifold decrease in energy consumption, compared to state-of-the-art. The proposed work also reduces delays by almost 60% and an increase in data rate by 50%.
\subsubsection{On The Modelling of UAV-Aided Networks Using NarrowBand-IoT}
abstract:In this paper, we consider a NarrowBand-Internet of Things (NB-IoT) network where an Unmanned Aerial Vehicle (UAV), acting as Unmanned Aerial Base Station (UAB), is employed to collect data from IoT nodes deployed in a service area. Due to the UAB inherent mobility and NB-IoT protocol constraints, the design of proper parameters setting is not an easy task. To try and solve this problem, we analyse the scenario through stochastic tools, allowing to introduce an appropriate relation between protocol and UAB flight parameters, together with application requirements.Specifically, we study a cluster-based scenario, where IoT nodes are assumed to be deployed according to a Thomas cluster process, and we apply a Traveling Salesman Problem to design the UAB trajectory over the area. Notably, our model considers the protocol constraints, in terms of resource units available in the NB-IoT NPUSCH channel, the random access procedure implemented in the NPRACH channel and the data rate that can be provided to IoT nodes. Results allow to quickly deduce optimal design parameters for achieving the maximum network throughput.
\subsubsection{Connectivity Analysis of UAV-To-Satellite Communications in Non-Terrestrial Networks}
abstract:Non-terrestrial Networks (NTNs) refer to the networks, where either satellites or unmanned aerial vehicles (UAVs) are deployed to extend the current terrestrial networks for serving the growing mobile broadband and machine-type communications. With the advantages of UAVs' flexibility and satellites' global coverage, the solution of UAV-To-satellite communications (U2SC) can provide promising global communication services for the emerging NTNs. Previous literature has explored many potential directions of U2SC, including channel tracking, deployment design, and link analysis. However, as a vital role in system performance, the connectivity of U2SC has not been well investigated yet. This research gap motivates us to present an analytical model to evaluate the connectivity of U2SC. In particular, we first present the system model of the U2SC by considering the distribution model of UAVs, antenna models, and the path loss model. We then utilize stochastic geometry to derive a theoretical formulation of the successful connection probability of U2SC. The comprehensive numerical results are given to evaluate the received power, the interference, and the successful connection probability of U2SC and analyze the impacts of system parameters, such as the number of frequency carriers, the type of frequency bands, the number of UAVs, and the satellite altitude.
\subsection{Globecom 2021 Aerial Communications Session 4}
\subsubsection{An Optimal Scheme to Recharge Communication Drones}
abstract:The adoption and integration of drones in communication networks is becoming reality thanks to the deployment of advanced solutions for IoT and cellular communication relay schemes. However, using drones introduces new energy constraints and scheduling issues in the dynamic management of the network topology, due to the need to call back and recharge, or substitute, drones that run out of energy. In this paper, we describe the design of a drone recharging scheme for realistically limited flight time of drones, and leverage the presence of recharging stations. Indeed, drones need to be recharged periodically, and maximizing the operational time of drones is paramount to minimize the size of the fleet of drones to be devoted to a drone mission, hence its cost. We design HRR, an optimal drone recharging scheduling that extends the coverage of a cellular network. HRR minimizes the number of back-up drones needed to guarantee a fixed number of operational drones, so as to support the operation of an underlying cellular network. Results show that operating a network of drones with our scheme provides reliable and stable performance over time.
\subsubsection{Fairness-Aware Closed-Form UL-DL Power Allocation for NOMA in UL Heavy UAV Systems}
abstract:In this paper, we investigate an unmanned aerialvehicle (UAV) assisted communication system for uplink (UL)heavy scenarios such as a football match, where intensive videouploading from the audience is demanded. Non-orthogonal multiple access (NOMA) is exploited to enhance system throughput,while it suffers a dynamic user rate gap within each NOMA groupdue to UAV movement. In light of the user fairness issue andthe UL heavy demands, we propose joint UL-DL optimizationof user scheduling, power allocation (PA) and UAV trajectory(JOSPT). In particular, closed-form optimal solutions are derivedfor dynamic PA in both UL and DL, which plays a dominantrole in system performance. The proposed PA scheme is moreeffective in maintaining user fairness than the previous work andalso computationally efficient and suitable for energy-limited UAVsystem. The average spectrum efficiency of the proposed NOMAUAV system in UL and DL is also much higher than that of theorthogonal multiple access (OMA) based UAV system, with theheterogeneous rate demands accommodated.
\subsubsection{On the Deployment Problem in Cell-Free UAV Networks}
abstract:Cell-free (CF) structures are expected to be agame changer for beyond-5G wireless networks. With all userspotentially communicating with all the base stations, cooperationat a central processing point is poised to provide much higherspectral efficiencies. At the same time, the growing interest inunmanned aerial vehicles (UAVs) makes CF-UAV networks anappealing scenario. In this paper, we investigate the uplink ofa CF network where UAVs serve as flying base stations. It isshown that the optimization of the UAV locations can markedlyincrease the minimum spectral efficiency without compromisingthe aggregated utility, with the improvements being associatedto pilot contamination and to irregularities in the user locations.
\subsubsection{Coverage Analysis of Cellular-Connected UAV Communications with 3GPP Antenna and Channel Models}
abstract:For reliable and efficient communications of aerial platforms, such as unmanned aerial vehicles (UAVs), the cellular network is envisioned to provide connectivity for the aerial and ground user equipment (GUE) simultaneously, which brings challenges to the existing pattern of the base station (BS) tailored for ground-level services. Thus, we focus on the coverage probability analysis to investigate the coexistence of aerial and terrestrial users, by employing realistic antenna and channel models reported in the 3rd Generation Partnership Project (3GPP). The homogeneous Poisson point process (PPP) is used to describe the BS distribution, and the BS antenna is adjustable in the down-tilted angle and the number of the antenna array. Meantime, omnidirectional antennas are used for cellular users. We first derive the approximation of coverage probability and then conduct numerous simulations to evaluate the impacts of antenna numbers, down-tilted angles, carrier frequencies, and user heights. One of the essential findings indicates that the coverage probabilities of high-altitude users become less sensitive to the down-tilted angle. Moreover, we found that the aerial user equipment (AUE) in a certain range of heights can achieve the same or better coverage probability than that of GUE, which provides an insight into the effective deployment of cellular-connected aerial communications.
\subsubsection{Dynamic Active-Passive Beamforming for Intelligent Reflecting Surface Aided UAV Communications}
abstract:This paper investigates the long-term effectiveness and stability of an integrated unmanned aerial vehicles (UAV)-intelligent reflecting surface (IRS) relaying dynamic system in the context of time-varying system states. Consequently, a dynamic optimization problem is constructed to minimize the frame-average transmit power by joint active beamforming at the base station (BS) and phase-shift tuning at the IRS under frame-average rate constraints. The original problem as an infinite-horizon time-average one can be solved by introducing the drift-plus-penalty (DPP) algorithm and then the optimal active beamforming and phase-shift can be obtained in an iterative manner. Simulation results demonstrate the theoretical analysis and assess the performance of the dynamic system.
\subsection{Globecom 2021 Aerial Communications Session 5}
\subsubsection{Joint Placement and Beamforming Design in Multi-UAV-IRS Assisted Multiuser Communication}
abstract:Intelligent reflecting surface (IRS) is a revolutionizing technology for improving the spectrum and energy efficiency in wireless communications. In this paper, a new communication framework enabled by multiple unmanned aerial vehicle (UAV)-carried intelligent reflecting surfaces (IRSs) is proposed to enhance multiuser downlink transmissions. To take full advantages of multi-UAV-IRS assisted system, we formulate the problem as maximizing the downlink sum rate by jointly optimizing the placement of UAVs, active beamforming and power allocationat the BS, and passive beamforming at the IRSs. An efficient algorithm is proposed so that the joint optimization problem can be decomposed into several subproblems, which can be individually optimized in an iterative manner. Simulation results show that the proposed scheme outperforms other benchmarkschemes, which corroborates the feasibility and effectiveness of our proposed algorithm.
\subsubsection{Collision-Aware UAV Trajectories for Data Collection via Reinforcement Learning}
abstract:Unmanned aerial vehicles (UAVs) are expected to be an integral part of wireless networks, and determining collision-free trajectories in multi-UAV non-cooperative scenarios is a challenging task. In this paper, we consider a path planning optimization problem to maximize the collected data from multiple Internet of Things (IoT) nodes under realistic constraints. The considered multi-UAV non-cooperative scenarios involve random number of other UAVs in addition to the typical UAV, and UAVs do not communicate with each other. We translate the problem into an Markov decision process (MDP). Dueling double deep Q-network (D3QN) is proposed to learn the decision making policy for the typical UAV, without any prior knowledge of the environment (e.g., channel propagation model and locations of the obstacles) and other UAVs (e.g., their missions, movements, and policies). Numerical results demonstrate that real-time navigation can be efficiently performed with high success rate, high data collection rate, and low collision rate.
\subsubsection{Federated Learning for UAV Swarms Under Class Imbalance and Power Consumption Constraints}
abstract:The usage of unmanned aerial vehicles (UAVs) in civil and military applications continues to increase due to the numerous advantages that they provide over conventional approaches. Despite the abundance of such advantages, it is imperative to investigate the performance of UAV utilization while considering their design limitations. This paper investigates the deployment of UAV swarms when each UAV carries a machine learning classification task. To avoid data exchange with ground-based processing nodes, a federated learning approach is adopted between a UAV leader and the swarm members to improve the local learning model while avoiding excessive air-to-ground and ground-to-air communications. Moreover, the proposed deployment framework considers the stringent energy constraints of UAVs and the problem of class imbalance, where we show that considering these design parameters significantly improves the performances of the UAV swarm in terms of classification accuracy, energy consumption and availability of UAVs when compared with several baseline algorithms.
\subsubsection{Joint Distributed Beamforming and Backscatter Cooperation for UAV-Assisted WPSNs}
abstract:Unmanned aerial vehicle (UAV)-assisted wireless powered sensor networks (WPSNs) emerge as a promising paradigm for charging sensor nodes' batteries in remote areas. However, the sum-throughput of overall sensor nodes can dramatically decrease due to the impact of the long-distance transmission to the UAV. In this paper, we propose a joint distributed beamforming and backscatter cooperation (BC) scheme to improve the sum-throughput of UAV-assisted WPSNs with various types of sensor nodes. In particular, we consider the BC mechanism which leverages sensor nodes of other types with constructive multi-path signals to improve the long-distance transmission of a group of same-type sensor nodes. We maximize the sum-throughput by jointly optimizing the backscattering coefficients, distributed beamforming and time allocation. Since the sum-throughput maximization problem is difficult to be solved directly due to the coupling among optimizing variables, we decompose the problem into a BC subproblem and a time allocation subproblem, and propose a two-step scheme to solve them. Specifically, we first use the semidefinite programming method to relax the BC subproblem into a convex one, which can be solved by the convex optimization. We obtain the distributed beamforming solutions and distributed backscattering solutions to maximize the signal-to-noise ratios of the same-type sensor nodes. Second, for the time allocation subproblem, we derive the closed-form expressions of the solutions according to KKT conditions. Simulation results are provided to demonstrate the proposed scheme can increase the sum-throughput comparing to conventional distributed beamforming schemes.
\subsubsection{Joint Power and Trajectory Optimization for IRS-aided Master-Auxiliary-UAV-powered IoT Networks}
abstract:In this paper, we propose a novel Master-Auxiliary-Unmanned Aerial Vehicle (UAV)-powered Internet of Things (IoT) Network (IRS-MAIN). Compared to the conventional terrestrial or aerial IRS-assisted communication networks, the IRS-MAIN not only benefits from wide communication range due to the mobility of UAVs, but also enjoys enhanced channel condition brought by the IRS. To be specific, we let the Auxiliary UAV (AUAV) carry an IRS to enhance signals from the Master UAV (MUAV) served as radio frequency (RF) transmitter. We focus on the problem to maximize the total throughput by jointly optimizing the trajectories and transmit power of the MUAV. A modified multi-agent deep reinforcement learning (MADRL) based algorithm, named as Pre-activation Penalty Multi-agent Deep Deterministic Policy Gradient (PP-MADDPG), is proposed to solve the formulated problem in an accurate and efficient way. Simulation results are provided to demonstrate that PP-MADDPG outperforms the baseline method in terms of the throughput as well as the convergence rate.
\subsubsection{Stable Online Offloading and Trajectory Control for UAV-enabled MEC with EH Devices}
abstract:In this paper, we study an unmanned aerial vehicle (UAV)-enabled mobile edge computing (MEC) system with multiple energy harvesting (EH) devices. Considering the stochastic energy and data arrivals in sequential time slots, we formulate the UAV propulsion energy minimization problem with long-term data queue stability and battery causality constraints as a multi-stage stochastic optimization programming. To facilitate online control without any prior knowledge of future information, we adopt the perturbed Lyapunov optimization method that decouples the control decisions made in sequential time slots and determines the real-time control decisions by solving a deterministic problem in each time slot. For the per-slot deterministic problem, we decouple it into three sub-problems: the optimal energy harvesting, the computation resource allocation and the UAV trajectory control, and propose a reduced-complexity method to solve them separately. Simulation results demonstrate that the proposed algorithm guarantees the data queue stability that is not achievable by the benchmark method when the two methods consume identical UAV propulsion energy.
\subsection{Machine Learning and Edge Computing}
\subsubsection{Convergence analysis and Design principle for Federated learning in Wireless network}
abstract:Recently, federated learning (FL) has been treated as an important and promising learning scheme in IoT, enabling devices to jointly learn a model without sharing their data sets. Different from centralized training on some collected data sets, FL training suffers a lot of constraints resulting from limited communication resources in the distributed system. Therein, limited bandwidth and random package loss restrict interactions in training. Meanwhile, the highly distributed data sets and limited computation could also affect its convergence. To figure out the specific impact, we analyze the convergence rate of FL training considering both communication network settings and training settings. Further taking in training costs in terms of time and power, the closed-form optimal settings for communication networks are proposed with principles to assist the parameter selection. The results build a bridge between AI and communication, giving us an intuitive knowledge of how the background system could influence the distributed training process.
\subsubsection{Early-exit deep neural networks for distorted images: providing an efficient edge offloading}
abstract:Edge offloading for deep neural networks (DNNs) can be adaptive to the input's complexity by using early-exit DNNs. These DNNs have side branches throughout their architecture, allowing the inference to end earlier in the edge. The branches estimate the accuracy for a given input. If this estimated accuracy reaches a threshold, the inference ends on the edge. Otherwise, the edge offloads the inference to the cloud to process the remaining DNN layers. However, DNNs for image classification deals with distorted images, which negatively impact the branches' estimated accuracy. Consequently, the edge offloads more inferences to the cloud. This work introduces expert side branches trained on a particular distortion type to improve robustness against image distortion. The edge detects the distortion type and selects appropriate expert branches to perform the inference. This approach increases the estimated accuracy on the edge, improving the offloading decisions. We validate our proposal in a realistic scenario, in which the edge offloads DNN inference to Amazon EC2 instances.
\subsubsection{CFL-HC: A Coded Federated Learning Framework for Heterogeneous Computing Scenarios}
abstract:Federated learning (FL) is a promising machine learning paradigm because it allows distributed edge devices to collaboratively train a model without sharing their raw data. In practice, a major challenge to FL is that edge devices are heterogeneous, so slow devices may compromise the convergence of model training. To address such a challenge, several recent studies have suggested different solutions, in which a promising scheme is to utilize coded computing to facilitate the training of linear models. Nevertheless, the existing coded FL (CFL) scheme is limited by a fixed coding redundancy parameter, and a weight matrix used in the existing design may introduce unnecessary errors. In this paper, we tackle these issues and propose a novel framework, namely CFL-HC, to facilitate CFL in heterogeneous computing scenarios. In our framework, we consider a computing system consisting of a central server and multiple computing devices with original or coded datasets. Then we specify an expected number of input-output pairs that are used in one round. Within such a framework, we formulate an optimization problem to find the best deadline of each training round and the optimal size of the computing task allocated to each computing device. We then design a two-step optimization scheme to obtain the optimal solution. To evaluate the proposed framework, we develop a real CFL system using the message passing interface platform. Based on this system, we conduct numerical experiments, which demonstrate the advantages of the proposed framework, in terms of both accuracy and convergence speed.
\subsubsection{Wireless Federated Learning with Limited Communication and Differential Privacy}
abstract:This paper investigates the role of dimensionality reduction in efficient communication and differential privacy(DP) of the local datasets at the remote users for over-the-air computation (AirComp)-based federated learning (FL) model. More precisely, we consider the FL setting in which clients are prompted to train a machine learning model by simultaneous channel-aware and limited communications with a parameter server (PS) over a Gaussian multiple-access channel (GMAC), so that transmissions sum coherently at the PS globally aware of the channel coefficients. For this setting, an algorithm is proposed based on applying (i) federated stochastic gradient descent (FedSGD) for training the minimum of a given loss function based on the local gradients, (ii) Johnson-Lindenstrauss (JL) random projection for reducing the dimension of the local updates and (iii) artificial noise to further aid user's privacy. For this scheme, our results show that the local DP performance is mainly improved due to injecting noise of greater variance on each dimension while keeping the sensitivity of the projected vectors unchanged. This is while the convergence rate is slowed down compared to the case without dimensionality reduction. As the performance outweighs for the slower convergence, the trade-off between privacy and convergence is higher but is shown to lessen in high-dimensional regime yielding almost the same trade-off with much less communication cost.
\subsection{Machine learning for source-channel coding}
\subsubsection{DNC-Aided SCL-Flip Decoding of Polar Codes}
abstract:Successive-cancellation list (SCL) decoding of polar codes has been adopted for 5G. However, the performance is not very satisfactory with moderate code length. Heuristic or deep-learning-aided (DL-aided) flip algorithms have been developed to tackle this problem. The key for successful flip decoding is to accurately identify error bit positions. In this work, we propose a new flip algorithm with help of differentiable neural computer (DNC). New state and action encoding are developed for better DNC training and inference efficiency. The proposed method consists of two phases: i) a flip DNC (F-DNC) is exploited to rank most likely flip positions for multi-bit flipping; ii) if decoding still fails, a flip-validate DNC (FV-DNC) is used to re-select error bit positions for successive flip decoding trials. Supervised training methods are designed accordingly for the two DNCs. Simulation results show that proposed DNC-aided SCL-Flip (DNC-SCLF) decoding demonstrates up to 0.34dB coding gain improvement or 54.2% reduction in average number of decoding attempts compared to prior works.
\subsubsection{Dynamic Optimal Coding and Scheduling for Distributed Learning over Wireless Edge Networks}
abstract:This paper proposes a novel framework that can effectively address key challenges for the development of distributed learning over wireless edge networks. In particular, we first introduce a highly effective distributed learning model leveraging the most recent advanced coded distributed computing algorithm together with collaborative computing resources from wireless edge nodes to securely and effectively execute learning tasks. To minimize the average delay of learning tasks, the coding and scheduling policies must be jointly optimized. However, determining the optimal coding scheme together with the optimal edge nodes for different learning tasks is NP-hard due to the dynamics and uncertainty of the wireless environment and straggling problems at the computing nodes. Thus, we develop a highly effective approach utilizing advances of both reinforcement learning algorithms and the dueling network architecture to quickly find the optimal coding scheme together with the best edge nodes for different learning tasks without requiring completed information about the surrounding environment and straggling parameters in advance. Through extensive simulation results, we show that our proposed framework can reduce the average delay for the whole system up to 66% compared with other conventional learning and optimization approaches.
\subsubsection{Negentropy-Aware Loss Function for Trainable Belief Propagation in Coded MIMO Detection}
abstract:We consider large multi-user detection (MUD) via deep unfolding-aided belief propagation (BP) in coded multi-user MIMO (MU-MIMO) systems. A BP detector optimized (trained) by data-driven-tuning of embedded internal parameters achieveslow-complexity and high-accuracy MUD while compensating practical imperfections. However, in actual implementation, these parameters should be optimized according to system parameters, e.g., modulation and coding scheme (MCS). In particular, whenchannel coding is used, it is vital not only to minimize the mean square error (MSE) but also to enhance the Gaussianity of the output log-likelihood ratio (LLR), in order to maximize the error correction capability of the subsequent soft-decisiondecoder. To that end, a novel loss function based on a weighted average of negentropy, which is a key measure to evaluate the Gaussianity, and MSE of the detector output is proposed. Simulation results show that the trainable Gaussian BP (T-GaBP) detector optimized with the proposed negentropy-aware loss function significantly improves the bit error rate (BER) performance of the decoder output and substantially outperforms the T-GaBP optimized with the typical MSE loss function.
\subsubsection{Deep Joint Source-Channel Coding and Modulation for Underwater Acoustic Communication}
abstract:Underwater communication is a promising technology to provide ubiquitous network connectivity, where acoustic waves are used as the primary carrier for long-range communication. It has been a challenging research topic to efficiently transmit images with underwater acoustic communication (UAC), due to its inherently narrow bandwidth, strong signal attenuation, time-varying multipath propagation, and low propagation speed. In this paper, we present a new approach to addressing these limitations in UAC, namely the joint source-channel coding and modulation (JSCCM) based on a deep neural network (DNN). We develop a training method of DNN-based encoder and decoder, which directly encode/decode image-pixel values to modulated symbols, unlike conventional separation-based source and channel coding and modulation. Through numerical simulations, the deep JSCCM is confirmed to achieve significantly higher data-rate than conventional schemes.
\subsubsection{DeepIC: Coding for Interference Channels via Deep Learning}
abstract:The two-user interference channel is a model for multi one-to-one communications, where two transmitters wish to communicate with their corresponding receivers via a shared wireless medium. Two most common and simple coding schemes are time division (TD) and treating interference as noise (TIN). Interestingly, it is shown that there exists an asymptotic scheme, called Han-Kobayashi scheme, that performs better than TD and TIN. However, Han-Kobayashi scheme has impractically high complexity and is designed for asymptotic settings, which leads to a gap between information theory and practice. In this paper, we focus on designing practical codes for interference channels. As it is challenging to analytically design practical codes with feasible complexity, we apply deep learning to learn codes for interference channels. We demonstrate that DeepIC, a convolutional neural network-based code with an iterative decoder, outperforms TD and TIN by a significant margin for two-user additive white Gaussian noise channels with moderate amount of interference.
\subsection{Massive access}
\subsubsection{Dual-Net for Joint Channel Estimation and Data Recovery in Grant-free Massive Access}
abstract:In massive machine-type communications (mMTC), the conflict between millions of potential access devices and limited channel freedom leads to a sharp decrease in spectral efficiency. The sparse nature of mMTC provides a solution by using compressive sensing (CS) to perform multiuser detection (MUD) but suffers conflict between the high computation complexity and low latency requirements. In this paper, we propose a novel Dual-network for joint channel estimation and data recovery. The proposed Dual-Net utilizes the sparse consistency between the channel vector and data matrix of all users. Experimental results show that the proposed Dual-Net outperforms existing CS algorithms and general neural networks in computation complexity and accuracy, which means reduced access delay and more supported devices.
\subsubsection{Learning-Based Multiplexing of Grant-Based and Grant-Free Heterogeneous Services with Short Packets}
abstract:In this paper, we investigate the multiplexing of grant-based (GB) and grant-free (GF) device transmissions in an uplink heterogeneous network (HetNet), namely GB-GF HetNet, where the devices transmit their information using low-rate short data packets. Specifically, GB devices are granted unique time-slots for their transmissions. In contrast, GF devices can randomly select time-slots to transmit their messages utilizing the GF non-orthogonal multiple access (NOMA), which has emerged as a promising enabler for massive access and reducing access latency. However, random access (RA) in the GF NOMA can cause collisions and severe interference, leading to system performance degradation. To overcome this issue, we propose a multiple access (MA) protocol based on reinforcement learning for effective RA slots allocation. The proposed learning method aims to guarantee that the GF devices do not cause any collisions to the GB devices and the number of GF devices choosing the same time-slot does not exceed a predetermined threshold to reduce the interference. In addition, based on the results of the RA slots allocation using the proposed method, we derive the approximate closed-form expressions of the average decoding error probability (ADEP) for all devices to characterize the system performance. Our results presented in terms of access efficiency (AE), collision probability (CP), and overall ADEP (OADEP), show that our proposed method can ensure a smooth operation of the GB and GF devices within the same network while significantly minimizing the collision and interference among the device transmissions in the GB-GF HetNet.
\subsubsection{Improving the Communication and Computation Efficiency of Split Learning for IoT Applications}
abstract:Distributed machine learning systems train neural network models by utilizing the devices and resources in the network. One such system that was recently introduced is split learning. It trains a deep neural network collaboratively between the server and the client without sharing the raw data, ensuring private and secure training. Implementing such systems on the edge devices adds computation and communication overhead, which might not suit many edge devices, especially in IoT systems, where resources are limited. In this paper, we introduce a modified split learning system that includes an autoencoder and an adaptive threshold mechanism. The modified system has less communication and computation overhead compared to the original split learning system. The modified system was deployed on an IoT system and the results proved the advantages of the proposed mechanism. The communication overhead and computation overhead were reduced with negligible performance loss.
\subsubsection{Graph Neural Network Based Access Point Selection for Cell-Free Massive MIMO Systems}
abstract:A graph neural network (GNN) based access point (AP) selection algorithm for cell-free massive multiple-input multiple-output (MIMO) is proposed. Two graphs, a homogeneous one which includes only AP nodes representing the structure of the APs in the network and a heterogeneous graph which includes both the AP nodes and user equipment (UE) nodes are constructed to represent the cell-free massive MIMO network. A GNN based on the inductive graph learning framework GraphSAGE is used to obtain the embeddings which are then used to predict the links between the nodes. Numerical results show that compared to proximity based AP selection algorithms, our proposed GNN based algorithm predict more potential links with a limited number of Reference Signal Receive Power (RSRP) measurements. Unlike the other AP selection algorithms in the literature proposed algorithm does not assume RSRP measurements for each AP are available at the UE for optimal AP selection.
\subsubsection{Multi-agent deep reinforcement learning (MADRL) meets multi-user MIMO systems}
abstract:A multi-agent deep reinforcement learning (MADRL) is a promising approach to challenging problems in wireless environments involving multiple decision-makers (or actors) with high-dimensional continuous action space. In this paper, we present a MADRL-based approach that can jointly optimize precoders to achieve the outer-boundary, called pareto-boundary, of the achievable rate region for a multiple-input single-output (MISO) interference channel (IFC).In order to address two main challenges, namely, multiple actors (or agents) with partial observability and multi-dimensional continuous action space in MISO IFC setup, we adopt a multi-agent deep deterministic policy gradient (MA-DDPG) framework in which decentralized actors with partial observability can learn a multi-dimensional continuous policy in a centralized manner with the aid of shared critic with global information.Meanwhile, we will also address a phase ambiguity issue with the conventional complex baseband representation of signals widely used in radio communications.In order to mitigate the impact of phase ambiguity on training performance, we propose a training method, called phase ambiguity elimination (PAE), that leads to faster learning and better performance of MA-DDPG in wireless communication systems. The simulation results exhibit that MA-DDPG is capable of learning a near-optimal precoding strategy in a MISO IFC environment.
\subsection{ML/AI for intelligent reflecting surface}
\subsubsection{Deep Learning-Empowered Predictive Beamforming for IRS-Assisted Multi-User Communications}
abstract:The realization of practical intelligent reflecting surface (IRS)-assisted multi-user communication (IRS-MUC) systems critically depends on the proper beamforming design exploiting accurate channel state information (CSI). However, channel estimation (CE) in IRS-MUC systems requires a significantly large training overhead due to the numerous reflection elements involved in IRS. In this paper, we adopt a deep learning approach to implicitly learn the historical channel features and directly predict the IRS phase shifts for the next time slot to maximize the average achievable sum-rate of an IRS-MUC system taking into account the user mobility. By doing this, only a low-dimension multiple-input single-output (MISO) CE is needed for transmit beamforming design, thus significantly reducing the CE overhead. To this end, a location-aware convolutional long short-term memory network (LA-CLNet) is first developed to facilitate predictive beamforming at IRS, where the convolutional and recurrent units are jointly adopted to exploit both the spatial and temporal features of channels simultaneously. Given the predictive IRS phase shift beamforming, an instantaneous CSI (ICSI)-aware fully-connected neural network (IA-FNN) is then proposed to optimize the transmit beamforming matrix at the access point. Simulation results demonstrate that the sum-rate performance achieved by the proposed method approaches that of the genie-aided scheme with the full perfect ICSI.
\subsubsection{Model-Driven GAN-Based Channel Modeling for IRS-Aided Wireless Communication}
abstract:Intelligent reflecting surface (IRS) is a promising new technology that is able to create a favorable wireless signal propagation environment by collaboratively reconfiguring the passive reflecting elements, yet with low hardware and energy cost. In IRS-aided wireless communication systems, channel modeling is a fundamental task for communication algorithm design and performance optimization, which however is also very challenging since in-depth domain knowledge and technical expertise in radio signal propagations are required, especially for modeling the high-dimensional cascaded base station (BS)-IRS and IRS-user channels (also referred to as the reflected channels). In this paper, we propose a model-driven generative adversarial network (GAN)-based channel modeling framework to autonomously learn the reflected channel distribution, without complex theoretical analysis or data processing. The designed GAN (also named as IRS-GAN) is trained to reach the Nash equilibrium of a minimax game between a generative model and a discriminative model, where the special structure of the reflected channels is incorporated to improve the modeling accuracy. Simulation results are presented to validate the effectiveness of the proposed IRS-GAN framework for IRS-related channel modeling.
\subsubsection{Enabling Efficient Scheduling Policy in Intelligent Reflecting Surface Aided Federated Learning}
abstract:Federated learning (FL) has been proposed to coordinate multiple edge user equipments (UEs) for training a global model. However, the FL's performance is affected by the channel state of wireless network. Specifically, the performance of the random selecting UE is seriously limited by the millimeter-wave (mmWave) channel. In this paper, we propose to deploy intelligent reflecting surface (IRS) toreconstruct the non-line-of-sight (NLoS) mmWave channel. A novel UE scheduling strategy is then proposed to optimize the FL system performance. For selecting the particular UEs and achieving higher convergence, we formulate an optimization problem that jointly optimizes the aggregation vector of the base station (BS) and the IRS phase shift matrix. To figure out the formulated problem, we propose a two-step difference-of-convex (DC) algorithm. Simulation results demonstrate that the proposed algorithm can achieve higher convergence and a lower training loss than the benchmark algorithm.
\subsubsection{Federated Distributionally Robust Optimization for Phase Configuration of RISs}
abstract:In this article, we study the problem of robust reconfigurable intelligent surface (RIS)-aided downlink communication over heterogeneous RIS types in the supervised learning setting. By modeling downlink communication over heterogeneous RIS designs as different workers that learn how to optimize phase configurations in a distributed manner, we solve this distributed learning problem using a distributionally robust formulation in a communication-efficient manner, while establishing its rate of convergence. By doing so, we ensure that the global model performance of the worst-case worker is close to the performance of other workers. Simulation results show that our proposed algorithm requires fewer communication rounds (about 50% lesser) to achieve the same worst-case distribution test accuracy compared to competitive baselines.
\subsubsection{Capacity Region of Intelligent Reflecting Surface Aided Wireless Networks via Active Learning}
abstract:Intelligent Reflecting Surface (IRS) is a promisingtechnology that is able to manipulate the wireless propagationchannels via smartly adjusting the signal reflection. With continuousphase shifts, IRS has been shown to be effective inenlarging the achievable rate region. In this paper, we investigatethe achievable rate region of a IRS-aided multi-user interferencechannel, where the phase shifts at the IRS can only take afinite number of discrete values. We formulate a multi-objectiveoptimization problem (MOOP) to characterize the achievablerate region. The commonly adopted approaches such as therate profile method fail to solve MOOP with optimizationvariables. Although the exhaustive search method can obtainthe Pareto-optimal solutions, it suffers from high computationalcomplexity. To this end, we propose a computationally efficientactive learning algorithm via Gaussian process (GP). By modelingthe objectives of MOOP as a draw from a GP distribution withonly a few randomly computed rate-tuples, the active learningalgorithm can quickly dominate the non-optimal points and findPareto-optimal points without calculating rate-tuples. Numericalsimulations demonstrate that the achievable rate region of IRS-aidedinterference channel is much larger than that without IRSand the proposed active learning framework obtains near-optimalPareto solutions with a much lower computational complexitythan the traditional exhaustive search algorithm.
\subsection{Molecular Communications: Design and Applications}
\subsubsection{Scaled Conjugate Gradient Algorithm for Neural Network Detector in Mobile Molecular Communication}
abstract:In this work, a neural network (NN)-based detection for mobile molecular communication via diffusion (MCvD) is proposed. The proposed detector employs scaled conjugate gradient (SCG) algorithm for updating the weights of the NN. Moreover, three different techniques are used in training and detection by the NN. These techniques correspond to i) filtered signal ii) slope values of the filtered signal, and iii) concentration difference of the filtered signal in a bit interval. More specifically, a sequence of transmitted bit pattern and each of the above three techniques are used separately for training the NN. After training, the NN-based detector performs detection under time-varying channel. The bit error rate (BER) performance of proposed SCG algorithm for NN-based detector is also compared with a first-order algorithm Gradient Descent (GD) and a second-order algorithm Broyden-Fletcher-Goldfarb-Shanno (BFGS) for different coherence time of the channel. Simulation results demonstrate that the NN detector using SCG outperforms the BFGS if slope values are used for training the NN. Further, SCG algorithm has a significant performance gain compared to the GD algorithm.
\subsubsection{Distance and Velocity Prediction by Extended Kalman Filter in Mobile Molecular Communication}
abstract:Distance and velocity estimation is challenging in mobile molecular communication, since estimates may be stale by the time the terminals gather the required information. To address both the difficulty and the delay of estimation, we propose an extended Kalman filter to predict both the distance between transmitter and receiver, and the drift velocity of the receiver, in a realistic blood-vessel-type flow regime. The extended Kalman filter is appropriate for the problem, as it can be used to predict quadratic quantities such as distance, and has manageable computational complexity. We derive the extended Kalman filter and show that it delivers excellent performance in predicting both distance and velocity in the presence of nonuniform flow.
\subsubsection{Leader-Follower Dynamics for Diffusion-based Molecular Communication}
abstract:Nanomachines are envisioned for a variety of applications in the industry and health sectors operating as sensors and actuators. Considering their potential mobility, it is relevant to study the capability of nanomachines to cooperate in molecular communication scenarios. To this end, we provide new insights into the leader-follower dynamics when a mobile leader node moves randomly in three-dimensional space and emits molecules into a diffusive environment to send information about its position to a follower node. In this paper, we investigate the random distance between the two nodes due to decision errors at the follower and analyze an upper bound for the average distance as a function of time. Simulations are provided to validate our analytical results. Moreover, by comparing to the benchmark scenario of uncoordinated movement of leader and follower, we investigate for which parameters the follower can reliably follow the leader.
\subsubsection{A Synchronization Protocol for Multi-User Cell Signaling-Based Molecular Communication}
abstract:Molecular Communications (MC) networks comprise multiple devices performing coordinated complex tasks, such as detecting types of cancer and smart drug delivery. Signaling-based MC uses molecules as information carriers between signaling cells. In this context, synchronization is jointly paramount and challenging since the system must overcome the limitation of molecular propagation to make sure computationally deprived bio-devices can communicate. On top of that, a multi-user increases this system challenges as possible co-channel interference causes errors or failures. Bio-devices present severe computational and communication limitations, being this last one essentially unidirectional. This paper presents the first synchronization protocol between signaling cells for multi-user MC. Results have shown the convergence time concerning different network sizes from 12 to 60 nodes.
\subsection{Molecular Communications: Theory and Modeling}
\subsubsection{Analysis of Molecular Communications on the Growth Structure of Glioblastoma Multiforme}
abstract:In this paper we consider the influence of intercellular communication on the development and progression of Glioblastoma Multiforme (GBM), a grade IV malignant glioma which is defined by an interplay Grow i.e. self renewal and Go i.e. invasiveness potential of multiple malignant glioma stem cells. Firstly, we performed wet lab experiments with U87 malignant glioma cells to study the node-stem growth pattern of GBM. Next we develop a model accounting for the structural influence of multiple transmitter and receiver glioma stem cells resulting in the node-stem growth structure of GBM tumour. By using information theory we study different properties associated with this communication model to show that the growth of GBM in a particular direction (node to stem) is related to an increase in mutual information. We further show that information flow between glioblastoma cells for different levels of invasiveness vary at different points between node and stem. These findings are expected to contribute significantly in the design of future therapeutic mechanisms for GBM.
\subsubsection{Channel Modeling for Drug Carrier Matrices}
abstract:Molecular communications is a promising framework for the design of controlled-release drug delivery systems. In this framework, drug carriers are modeled as transmitters, the diseased cells as absorbing receivers, and the channel between transmitter and receiver as diffusive channel. However, existing works on drug delivery systems consider only simple drug carrier models, which limits their practical applicability. In this paper, we investigate diffusion-based spherical matrix-type drug carriers, which are employed in practice. In a matrix carrier, the drug molecules are dispersed in the matrix and diffuse from the inner to the outer layers of the carrier once immersed in a dissolution medium. We derive the channel response of the matrix carrier transmitter for an absorbing receiver and validate the results through particle-based simulations. Moreover, we show that a transparent spherical transmitter, with the drug molecules uniformly distributed over the entire volume, is as special case of the considered matrix system. For this case, we provide an analytical expression for the channel response. Finally, we compare the channel response of the matrix transmitter with those of point and transparent spherical transmitters to reveal the necessity of considering practical models.
\subsubsection{Optimizing Information Transfer Through Chemical Channels in Molecular Communication}
abstract:The optimization of information transfer through molecule diffusion and chemical reactions is one of the leading research directions in Molecular Communication (MC) theory. The highly nonlinear nature of the processes underlying these channels poses challenges in adopting analytical approaches for their information-theoretic modeling and analysis. In this paper, a novel iterative methodology is proposed to numerically estimate achievable information rates. Based on the Nelder-Mead optimization, this methodology does not necessitate analytical formulations of MC components and their stochastic behavior, and, when applied to well-known scenarios, it demonstrates consistent results with theoretical bounds and superior performance to prior literature. A numerical example that abstracts communications between genetically engineered cells via simulation is presented and discussed in light of possible future applications to support the design and engineering of realistic MC systems.
\subsubsection{Spatiotemporal Control of Genetic Circuit with Pulse Generation for Molecular Communication}
abstract:Synthetic biology offers a tool to build biological entities that are capable of carrying out desired signal processing functionalities by controlling and engineering biochemical signaling pathways. The design and study of genetic circuits that exhibit natural behavior can be helpful for an improved understanding of the principles and kinetics behind the gene expression behavior, as well as for engineering cellular systems for synthetic biology. In this paper, we propose a synthetic system capable of generating a pulse-shaped signal, which is a prevalent behavior in natural environment. In particular, our proposed system is based on an engineered cell with NOR logic operation, and the pulse generation exploits the alteration in logic states of NOR inputs that are controlled by two types of diffusive signaling molecules. To quantitatively describe the generated pulse, we not only derive the propagation channel of the signaling molecules but also analyze the behavior of the engineered cell using Shea-Ackers formalism. Simulation results demonstrate that the pulse-shaped signal can be successfully produced in a controllable manner.
\subsection{Near-Earth Satellite Communications}
\subsubsection{Multi-layer LEO Satellite Constellation Design for Seamless Global Coverage}
abstract:In this paper, we investigate the traffic-sensitive multi-layer low Earth orbit (LEO) satellite-terrestrial network. Massive terrestrial user access to the core network is realized via the backhaul supported by multi-layer LEO satellites. Theultra-dense satellite topology enables a promising solution for the high-capacity backhaul data transmission for terrestrial users. Jointly considering the backhaul capacity requirement and traffic dynamics of terrestrial satellite terminals, we analyze their average backhaul capacity using both stochastic geometry and queueing theory. Aiming to minimize the total required satellite number for fulfilling the backhaul capacity and seamless global coverage requirements, we propose a multi-layer LEO satellite constellation deployment scheme considering the satellite mobility. Simulation results verify the backhaul capacity analysis and the advantage of multi-layer constellation for saving satellites. The optimized multi-layer LEO satellite constellation with any coverage requirement and traffic rate is presented.
\subsubsection{5G Massive Machine Type Communication Performance in Non-Terrestrial Networks with LEO Satellites}
abstract:An Internet of Things (IoT) non-terrestrial network (NTN) refers to an IoT network utilizing airborne or spaceborne payload for communication. Serving IoT devices using NTN has the potential of further expanding the use cases as well as coverage for IoT. This has motivated the work in 3GPP on evolving Narrowband Internet of Things (NB-IoT) and Long Term Evolution for machine type communications (LTE-M) wireless access technologies to support NTN. While the topic of IoT over NTN has attracted much attention, the performance of IoT NTN has not been well studied. In this paper, we analyze theperformance of IoT low Earth orbit (LEO) NTN utilizing LTE-M by evaluating the connection density. The evaluation results show that for a LEO satellite at 600 and 1200 km altitude, LTE-M can support 364 and 78 users per km 2 for a single narrowband in its coverage area, respectively. This implies that a LTE-M based LEO NTN has the potential of serving devices in remote areas, thereby complementing the coverage of terrestrial networks.
\subsubsection{Downlink Analysis of LEO Multi-Beam Satellite Communication in Shadowed Rician Channels}
abstract:The coming extension of cellular technology to base-stations in low-earth orbit (LEO) requires a fresh look at terrestrial 3GPP channel models. Relative to such models, sky-to-ground cellular channels will exhibit less diffraction, deeper shadowing, larger Doppler shifts, and possibly far stronger cross-cell interference: consequences of high elevation angles and extreme "sectorization" of LEO satellite transmissions into partially-overlapping spot beams. To permit forecasting of expected signal-to-noise ratio (SNR), interference-to-noise ratio (INR) and probability of outage, we characterize the powers of desired and interference signals as received by ground users from such a LEO satellite. In particular, building on the Shadowed Rician (SR) channel model, we observe that co-cell and cross-cell sky-to-ground signals travel along similar paths, whereas terrestrial co- and cross-cell signals travel along very different paths. We characterize SNR, signal-to-interference ratio (SIR), and INR using transmit beam profiles and linear relationships that we establish between certain SR random variables. These tools allow us to simplify certain density functions and moments, facilitating future analysis. Numerical results yield insight into the key question of whether emerging LEO systems should be viewed as interference- or noise-limited.
\subsubsection{Uplink Transmission Probability Functions for LoRa-Based Direct-to-Satellite IoT: A Case Study}
abstract:Direct-to-Satellite IoT allows devices on the Earth surface to directly reach Low-Earth Orbit (LEO) satellites passing over them. Although an appealing approach towards a truly global IoT vision, scalability issues as well as highly dynamic topologies ask for dedicated protocol adaptations supported by novel models. This paper contributes to this research by introducing estimators and a transmission probability function to dynamically control the contending set of devices on a framed slotted Aloha model compatible with the LoRaWAN specification. In particular, we discuss techniques that account for particularities in the dynamics of sparse DtS-IoT constellations. Simulation analyses of a realistic case study show that >86% of the theoretical throughput is achievable in practice.
\subsubsection{Twin-Resolution Phase Shifters Based Massive MIMO Hybrid Precoding for LEO SATCOM with Nonlinear PAs}
abstract:Massive multiple-input multiple-output (MIMO) technology has attracted much attention in low earth orbit (LEO) downlink satellite communication (SATCOM) systems recently since the energy efficiency (EE) and spectral efficiency can be significantly improved. In order to reduce the power consumption for the massive MIMO LEO SATCOM systems, we focus on the hybrid analog/digital architecture in this work. Considering the limited resolution of the phase shifters in practical MIMOSATCOM systems, a twin-resolution phase shifting (TRPS) network is proposed to make a meticulous tradeoff between the power consumption and array gains. In addition, we examine the impact of the distortion, introduced by the power amplifiers (PAs) to the system design, by considering nonlinear PA models. Moreover, we propose an efficient algorithm for TRPS-based hybrid precoding with nonlinear PAs. Numerical results show the EE gains considering nonlinear distortion and the performance superiority of the proposed hybrid architecture compared with the baselines.
\subsection{Next-Generation Satellite Networks}
\subsubsection{Multi-Agent DRL for User Association and Power Control in Terrestrial-Satellite Network}
abstract:In the past few years, satellite communications have greatly affected our daily lives. Because the resources of terrestrial-satellite network are limited, how to allocate resources of terrestrial-satellite network through effective methods have become a major challenge. We propose a framework for energy efficiency optimization of terrestrial-satellite network based on Non-orthogonal multiple access (NOMA). In our framework, we adopt a multi-agent deep deterministic policy gradient (MADDPG) method to obtain the maximum energy efficiency by user association and power control. Finally, the simulation results show that the proposed method has better optimization performance compared with the traditional single-agent deep reinforcement learning algorithm and can efficiently solve the problems of user association and power control in the integrated terrestrial-satellite network.
\subsubsection{Beamforming Design for IRS-assisted Uplink Cognitive Satellite-Terrestrial Networks with NOMA}
abstract:Integrating non-orthogonal multiple access (NOMA) in intelligent reflecting surface (IRS) is expectedly an effective solution to enhance system's spectrum efficiency. In this paper, we investigate joint beamforming and power allocation for uplink NOMA transmission in an IRS-assisted cognitive satellite and terrestrial network operating at millimeter wave frequency band. Specifically, based only on imperfect channel state information in terms of the angular information of both primary users (PUs) and secondary users (SUs), we formulate an optimization problem to maximize the sum rate of the PUs in terrestrial network. To handle the resulting intractable optimization problem, we first transform the uncertainty channel vectors into a deterministic form with the aid of angular discretization. Then, by combining successive convex approximation with Taylor expansion and S-procedure methods, we propose an optimization scheme to jointly optimize the beamforming weight vector and power coefficients. Finally, simulation results show that the proposed scheme can achieve outstanding sum rate performance compared to state-of-the-art schemes.
\subsubsection{Weighted Sum-Rate Maximization for Multi-IRS Aided Integrated Terrestrial-Satellite Networks}
abstract:This paper investigates a multiple intelligent reflecting surfaces (IRSs) aided integrated terrestrial-satellite network (ITSN), where the IRSs are deployed to cooperatively assist the low channel gain users in the co-existing transmission system. In such a network, the coordinated beamforming and frame based transmission scheme are considered for the terrestrial network and the satellite network, respectively. We aim at maximizing the weighted sum rate (WSR) of all users by jointly designing the frame based beamforming at the small base stations (SBSs) and the phase shifts at the IRSs, subject to the individual maximum SBS's transmit power constraints and the IRSs' reflection constraints. This non-convex problem is firstly decomposed via fractional programming (FP) technique in the objective function, then transmit beamforming vectors and reflective phase shifts matrix are optimized alternatingly. A block coordinate descent (BCD) method is proposed to obtain the stationary solution. Simulation results verify the effectiveness of the proposed algorithm compared with different benchmark schemes.
\subsubsection{Dual-DNN Assisted Optimization for Efficient Resource Scheduling in NOMA-Enabled Satellite Systems}
abstract:In this paper, we apply non-orthogonal multiple access (NOMA) in satellite systems to assist data transmission for services with latency constraints. We investigate a problem to minimize the transmission time by jointly optimizing power allocation and terminal-timeslot assignment for accomplishing a transmission task in NOMA-enabled satellite systems. The problem appears non-linear/non-convex with integer variables and can be equivalently reformulated in the format of mixed-integer convex programming (MICP). Conventional iterative methods may apply but at the expenses of high computational complexity in approaching the optimum or near-optimum. We propose a combined learning and optimization scheme to tackle the problem, where the primal MICP is decomposed into two learning-suited classification tasks and a power allocation problem. In the proposed scheme, the first learning task is to predict the integer variables while the second task is to guarantee the feasibility of the solutions. Numerical results show that the proposed algorithm outperforms benchmarks in terms of average computational time, transmission time performance, and feasibility guarantee.
\subsubsection{Adaptive Access Mode Selection in Space-Ground Integrated Vehicular Networks}
abstract:Space-ground integrated vehicular networks (SGIVN) are envisioned as a promising architecture to support multifarious vehicular services with enhanced network flexibility and reliability. Access mode selection (AMS) is of capital importance in the SGIVN for the ingenious cooperation between different network segments to exploit their complementary advantages. In this paper, we investigate the AMS problem for vehicles in the SGIVN by considering distinct features of satellite networks (long propagation delay) and terrestrial networks (frequent handover). In light of the high vehicle/satellite mobility and dynamic data packet arrivals, we formulate a stochastic integer programming problem of sequential AMS to maximize vehicles' long-term data rate. To cope with the time-varying network dynamics, we leverage a Markov decision process framework to model the evolution of vehicle states. For the special case with the known stochastic model of data packet arrivals, we transform the problem into a linear programming problem that can be solved with low complexity. For the general case without the data packet arrival model, we propose a reinforcement learning-based algorithm to make adaptive AMS decisions to keep pace with network dynamics. Simulation results demonstrate that the proposed adaptive AMS algorithm outperforms benchmark algorithms in terms of data rate under different data packet arrival patterns and vehicle velocities.
\subsection{Physical Layer Full Duplex Techniques}
\subsubsection{Effect of Non-Resolvable Multipath on Full-Duplex Self-Interference Cancellation}
abstract:In full-duplex multipath self-interference (SI), each of the resolvable multipath SI components consists of a group of non-resolvable (NR) components with similar delays. Since the transceiver cannot distinguish the NR components in reality, the canceller can only generate the approximate constructed SI to imperfectly cancel the received SI. This leads to residual SI remaining. This letter analyzes the effect of the NR components on the SI cancellation, and proposes a practical scheme to eliminate the residual SI caused by the NR components. First, a closed-form expression of the residual SI is obtained. Next, the average power of the residual SI and the SI cancellation ratio are derived to characterize the effect of NR components on SI cancellation in the multipath SI channel. Last, a switch scheme is proposed to eliminate the residual SI.
\subsubsection{Direction-Assisted Beam Management in Full Duplex Millimeter Wave Massive MIMO Systems}
abstract:Recent applications of the Full Duplex (FD) technology focus on enabling simultaneous control communication and data transmission to reduce the control information exchange overhead which impacts spectral efficiency. In this paper, we present a simultaneous direction estimation and data transmission scheme for millimeter Wave (mmWave) massive Multiple-Input Multiple-Output (MIMO) systems, enabled by the FD MIMO technology with reduced hardware complexity Self-Interference (SI) cancellation. We apply the proposed framework in the mmWave analog beam management problem, considering a base station equipped with a large transmit antenna array realizing downlink analog beamforming and few digitally controlled receive antenna elements used for uplink Direction-of-Arrival (DoA) estimation. A joint optimization framework for designing the DoA-assisted analog beamformer and the analog as well as digital SI cancellation is presented with the objective to maximize the achievable downlink rate. Our simulation results showcase that the proposed scheme outperforms its conventional half-duplex counterpart, yielding reduced DoA estimation error and superior downlink data rate.
\subsubsection{Hybrid Beamforming and Combining for Millimeter Wave Full Duplex Massive MIMO Interference Channel}
abstract:Full-Duplex (FD) communication can revolutionize wireless communications as it avoids using independent channels for bi-directional communications. In this work, we generalize the point-to-point FD communication in millimeter wave (mmWave) band consisting of K-pair of massive MIMO FD nodes operating simultaneously. To enable the coexistence of massive MIMO FD links cost-efficiently, we present novel joint hybrid beamforming (HYBF) and combining scheme for weighted sum-rate (WSR) maximization. The proposed algorithm is based on alternative optimization based on the minorization-maximization method. Moreover, we present a novel SI and massive MIMO interference channel aware power allocation scheme to include the optimal power control. Simulation results show significant performance improvement compared to a traditional bidirectional half-duplex communication system.
\subsubsection{Hybrid Beamforming for Full-Duplex Enabled Cellular System in the Unlicensed mmWave Band}
abstract:In this paper, we consider a full-duplex enabled cellular system operated in the unlicensed mmWave band. In particular, the full-duplex gNB is equipped with multiple antennas and applies hybrid beamforming for downlink transmissions, meanwhile mobile users send uplink traffic to the gNB with an omni-directional antenna. In the unlicensed bands, the cellular networks need to ensure a fair coexistence with other systems, such as WiGig. Therefore, a sum rate maximization problem is formulated to improve the throughput of the cellular system meanwhile suppressing its interference to the WiGig system. The formulated problem is non-convex and of high complexity. In order to efficiently address it, the sum rate maximization problem is first reformulated into a weighted mean square error minimization problem, and then the penalty dual decomposition method is applied to jointly optimize the uplink transmit power and downlink hybrid beamforming. Numerical results show that the performance of the unlicensed full-duplex enabled cellular network degrades by nearly 29% in order to fairly coexist with the WiGig system. However, compared with half-duplex systems, applying full-duplex technology to unlicensed mmWave systems can improve the sum rate by nearly 56%.
\subsection{Quantum Communications & Computing}
\subsubsection{QuantumFed: A Federated Learning Framework for Collaborative Quantum Training}
abstract:With the fast development of quantum computing and deep learning, the quantum neural network has attracted great attention recently. By conducting the powerful quantum computational power into the field of deep neural networks, it holds great promise for overcoming computational power limitations in classic machine learning. However, because of the no-cloning theorem, when multiple quantum machines who have local quantum data want to train a global model, it is very difficult to copy the data into one machine and train the model. Therefore, a collaborative quantum neural network framework is necessary. In this article, we borrow the core idea of federated learning to propose QuantumFed, a quantum federated learning framework to collaborate multiple quantum nodes with local quantum data. Our experiments show the feasibility and robustness of our framework.
\subsubsection{A Variational Quantum Algorithm for Ordered SVD}
abstract:Singular value decomposition (SVD) is fundamentally important and broadly useful in both quantum and classical computing. There are several quantum algorithms known for SVD. Variational quantum algorithms with parametric quantum circuits (PQC) are the most promising approach to find SVD with near-term quantum computers. This paper reports a new method for quantum SVD (QSVD) that finds the singular vectors ordered by the magnitude of singular values by adding extra CNOT gates and a new local cost function design. This ordering is important because the singular vectors with the larger singular values is more representative for the given information. The ordering property of this approach is investigated with the standard Iris dataset by numerical simulations of 4-qubit states. Methods for choosing appropriate choices for the cost function hyperparameter and cost function terms as well as an application example of a quantum encoder are also discussed.
\subsubsection{From the Environment-Assisted Paradigm to the Quantum Switch}
abstract:The quantum switch has been witnessing growing attention in the last years due to its advantage in several quantum technologies applications. In particular, it has been proven that the quantum switch can significantly improve the communication rates beyond the limits of conventional quantum Shannon theory. In this paper, we theoretically prove that the quantum switch can be interpreted as a particular instance of the Environment-assisted quantum communication paradigm. The developed analysis is crucial to better understand the limitations of the quantum switch. Furthermore, the analysis is key to shed the light on control strategies within the Environment-assisted communication paradigm.
\subsubsection{Enhancing continuous variable quantum teleportation using non-Gaussian resources}
abstract:Continuous Variable (CV) non-Gaussian resources are fundamental in the realization of quantum error correction for CV-based quantum communications and CV-based computing. In this work, we investigate the use of CV non-Gaussian states as quantum teleportation resource states in the context of the transmission of coherent and squeezed states through noisy channels.We consider an array of different non-Gaussian resource states, and compute the fidelity of state teleportation achieved for each resource. Our results show that the use of non-Gaussian states presents a significant advantage compared to the traditional resource adopted for CV teleportation; the Gaussian two mode squeezed vacuum state. In fiber-based quantum communications the range of quantum teleportation is increased by approximately 40% via the use of certain non-Gaussian states. In satellite-to-ground quantum communications, for aperture configurations consistent with the Micius satellite, the viable range of quantum teleportation is increased from 700 km to over 1200 km. These results represent a significant and important increase in the performance of pragmatic and realizable quantum communications in both terrestrial and space-based communication networks.
\subsubsection{Decomposition of Clifford Gates}
abstract:In fault-tolerant quantum computation and quantum error-correction one is interested on Pauli matrices that commute with a circuit/unitary. This information is encoded by the \(\it{support}\) (Pllaha et al., 2020) of the given circuit/unitary.We provide a fast algorithm that decomposes any Clifford gate as a \(\it{minimal}\) product of Clifford transvections. The algorithm can be directly used for computing the support of any given Clifford gate. To achieve this goal, we exploit the structure of the symplectic group with a novel graphical approach.
\subsubsection{Private Set Intersection with Delegated Blind Quantum Computing}
abstract:Private set intersection is an important problem with implications in many areas, ranging from remote diagnostic to private contact discovery. In this work, we consider the case of two-party PSI in the honest-but-curious setting. We propose a protocol that solves the server aided PSI problem using delegated blind quantum computing. More specifically, the proposed protocol allows Alice and Bob (who do not have any quantum computational resources or quantum memory) to interact with Steve (who has a quantum computer) in order for Alice and Bob to obtain set intersection such that privacy is preserved. In particular, Steve learns nothing about the clients' input, output, or desired computation. The proposed protocol is correct, secure and blind against a malicious server, and characterized by a quantum communication complexity that is linear in the input size.
\subsection{Radio Resource Management in Satellite Networks}
\subsubsection{Power and Bandwidth Minimization for Demand-Aware GEO Satellite Systems}
abstract:Smart radio resource allocation combined with therecent advances of digital payloads will allow to control thetransmit power and bandwidth of the satellites depending onthe demand and the channel conditions of users. The systemflexibility is important not only to handle divergent demand requirements but also to efficiently utilize the limited and expensivesatellite resources. In this paper, we propose a demand-awaresmart radio resource allocation technique, where the transmitpower and the bandwidth of the GEO satellite are minimizedwhile satisfying the user demand. The formulated optimizationproblem is non-convex mixed-integer nonlinear program which isdifficult to solve. Hence, we apply a quadratic transform to solvethe problem iteratively. The numerical results showed that theproposed scheme outperforms the benchmark schemes in termsof bandwidth utilization while accurately providing capacity-on-demand.
\subsubsection{Data-driven Network Orchestrator for 5G Satellite-Terrestrial Integrated Network: The ANChOR Project}
abstract:Satellite communications (SatCom) have a role of advanced service enablers in the new virtual networks, following 3GPP specifications. Specifically, the satellite peculiar characteristics are of paramount importance to dynamically activate capabilities, such as multicast and broadcast channels, sudden traffic offloading, capacity bonding, and cost-efficient coverage of uncovered areas.A key aspect to achieve a seamless and efficient integration between satellite and terrestrial infrastructures is to make satellite resource management dynamic and with a centralised control of a single orchestrator that has visibility of the end-to-end network.Considering the adoption of Software Defined Networking (SDN) and Network Function Virtualization (NFV) paradigms and the upcoming 5th generation of mobile communications (5G), this paper presents the ongoing work within the European Space Agency (ESA) ANChOR project, whose main output will be a data-driven Network Controller and Orchestrator for SatCom networks. This tool will be based on Artificial Intelligence (AI) / Machine Learning (ML)-based techniques to properly allocate resources exploiting some feedback knowledge of the network and it aims to support different services over 5G integrated satellite-terrestrial networks.
\subsubsection{Beamforming and Power Allocation in NOMA-Based Multibeam Satellite Systems with Outage Constraint}
abstract:In this paper, we propose a joint beamforming (BF) and power allocation scheme for non-orthogonal multiple access (NOMA) based multibeam satellite systems. Our objective is to minimize the maximum individual antenna powers subject to the users' outage constraints. Unlike the existing works where perfect channel state information (CSI) is required, we use imperfect CSI in formulating the constrained optimization problem. Since the original problem is mathematically intractable, we first adopt Bernstein-Type II inequality to convert the outage constraints into deterministic forms. Then, an alternating optimization algorithm is proposed to jointly design BF vectors and power allocation coefficients. Finally, simulation results are provided to demonstrate the robustness and superiority of the proposed scheme compared with benchmark schemes.
\subsubsection{Resource Scheduling in Satellite Networks: A Sparse Representation Based Machine Learning Approach}
abstract:With the growth of global communication service demand, constructing large-scale satellite networks has become the future development trend for improved system performance. However, due to the high-speed orbit motion of satellites, the connection relationship of network topology (CRNT) is complex and changeable. This phenomenon is particularly pronounced in large-scale satellite networks and the existing representation schemes of CRNT for large-scale satellite networks have high space complexity. Therefore, we explore the sparse characterization of the inter-satellite visibility matrix and propose an integrated sparse space-time resource representation (ISST-RR) scheme to efficiently characterize the satellite network communication resources with low complexity from the dimension of time and space. On the basis of the proposed ISST-RR scheme, we further propose a multi-agent reinforcement learning with sparse representation based resource scheduling (MARLSR-RS) algorithm to obtain the optimal resource scheduling policy. Simulations demonstrate the efficiency of the proposed MARLSR-RS algorithm in terms of communication resource utilization. In addition, we investigate the impact of several typical netwrok parameters, e.g., transmission rate of observation satellites on network performance, which can provide a theoretical guidance for system design.
\subsection{Resource allocation}
\subsubsection{Deep Reinforcement Learning for Wireless Resource Allocation Using Buffer State Information}
abstract:As the number of user equipments (UEs) with various data rate and latency requirements increases in wireless networks, the resource allocation problem for orthogonal frequency-division multiple access (OFDMA) becomes challenging. In particular, varying requirements lead to a non-convex optimization problem when maximizing the systems data rate while preserving fairness between UEs. In this paper, we solve the non-convex optimization problem using deep reinforcementlearning (DRL). We outline, train and evaluate a DRL agent, which performs the task of media access control scheduling for a downlink OFDMA scenario. To kickstart training of our agent, we introduce mimicking learning. For improvement of scheduling performance, full buffer state information at the base station (e.g. packet age, packet size) is taken into account. Techniques like input feature compression, packet shuffling and age capping further improve the performance of the agent. We train and evaluate our agents using Nokia's wireless suite and evaluate against different benchmark agents. We show that our agents clearly outperform the benchmark agents.
\subsubsection{Client Selection with Bandwidth Allocation in Federated Learning}
abstract:Federated learning (FL) is emerging as a promising paradigm for achieving distributed machine learning while protecting users' privacy. The accuracy and convergence speed of the global model benefit from involving as many clients as possible during the model training. On the other hand, the scarcity of wireless spectrum restricts the number of clients involved at each round. In this paper, we aim to maximize the number of participating clients in each round with fixed wireless bandwidth. Instead of assuming that the prior information about wireless channel state is available, we consider a more practical scenario under the absence of prior information. We first reformulate the client selection problem with limited bandwidth as a combinatorial multi-armed bandit (CMAB) problem and then propose an online learning algorithm with elegant bandwidth allocation based on the framework of combinatorial upper confidence bound. The proposed algorithm can make full use of the scarce bandwidth to increase the number of involved clients in each round and minimize the training latency for a given training accuracy. Numerical results validate the efficiency of the proposed algorithm.
\subsubsection{Hierarchical Deep Reinforcement Learning based Dynamic RAN Slicing for 5G V2X}
abstract:Radio Access Network (RAN) slicing is getting increasing attention as a resource allocation technique for satisfying diverse Quality-of-Service (QoS) requirements in 5G vehicular networks. Hierarchical Reinforcement Learning (HRL), such as hierarchical-DQN (h-DQN), is a promising slice management approach that decomposes performance constraints into a subroutine hierarchy and uses Deep Reinforcement Learning (DRL) at different temporal scales for online-learningan optimal policy of bandwidth allocation. In this paper, we tackle RAN slicing problem in 5G vehicle-to-everything (V2X) communications and present h-DQN based Soft Slicing (HSS) method for model-free opportunistic slice management. HSSconsists of a multi-controller learning framework where a high-level meta-controller takes state input for determining a subgoal and a low-level controller decides on the action based on the given subgoal and the state. We compare performance of HSS with model-free and model-based Reinforcement Learning (RL)methods in terms of Age of Information (AoI), service delay and network throughput. Our results show that proposed scheme improves sample-efficiency and outperforms traditional and RL-based V2X RAN slice management methods in terms of network utility maximization.
\subsubsection{Radar Aided mmWave Vehicle-to-Infrastructure Link Configuration Using Deep Learning}
abstract:The high overhead of the beam training process isthe main challenge when establishing mmWave communicationlinks, especially for vehicle-to-everything (V2X) scenarios wherethe channels are highly dynamic. In this paper, we obtainprior information to speed up the beam training process byimplementing two deep neural networks (DNNs) that realizeradar-to-communication (R2C) channel information translationin a vehicle-to-infrastructure (V2I) system. Specifically, the firstDNN is built to extract the information from the radar azimuthpower spectrum (APS) to reconstruct the communication APS,while the second DNN exploits the information extracted fromthe spatial covariance of the radar channel to realize R2Ccovariance prediction. The achieved data rate and the similaritybetween the estimated and the true communication APS areused to evaluate the prediction performance. The covarianceestimation method generally provides higher similarity, as theAPS predictions cannot always capture the mismatch betweenthe radar and communication APS. Compared to the beamtraining method which exploits directly the radar APS withoutan attempt to translate it to the communication channel, ourproposed deep learning (DL) aided methods remarkably reducethe beam training overhead, resulting in a 13.3% and 21.9%rate increase when using the communication APS predictionand covariance prediction, respectively.
\subsubsection{Fundamental limits of over-the-air optimization: Are analog schemes optimal?}
abstract:We consider convex optimization on a d dimensionalspace where coded gradients are sent over an additive Gaussiannoise channel with variance \sigma^2. The codewords satisfy an averagepower constraint P, resulting in the signal-to-noise ratio (SNR)of P/\sigma^2. Termed over-the-air optimization, many schemes havebeen proposed for this problem in recent years. We presentlower and upper bounds for the convergence rates for over-the-airoptimization. Our first result is a lower bound for the convergencerate showing that any code must slowdown the convergence rateby a factor of roughly \sqrt{d/log(1+SNR)}. Next, we consider apopular class of schemes called analog coding, where a linearfunction of the gradient is sent. We show that a simple scaledtransmission analog coding scheme results in a slowdown inconvergence rate by a factor of \sqrt{d(1 + 1/SNR)}. This matches theprevious lower bound up to constant factors for low SNR, makingthe scaled transmission scheme optimal at low SNR. However,we show that this slowdown is necessary for any analog codingscheme. In particular, a slowdown in convergence by a factor of\sqrt{d} remains even when SNR tends to infinity, a clear shortcoming of analog coding schemes at high SNR. Remarkably, we present a simple quantize-and-modulate scheme that uses Amplitude Shift Keying and almost attains the optimal convergence rate at all SNRs.
\subsection{Resource optimization}
\subsubsection{TTL-Based Cache Utility Maximization Using Deep Reinforcement Learning}
abstract:Utility-driven caching opened up a new design opportunity for caching algorithms by modeling the admission and eviction control as a utility maximization process with essential support for service differentiation. Nevertheless, there is still to go in terms of convergence speed and adaptability. Slow convergence to an optimal state may degrade actual user-experienced utility, which gets even worse in non-stationary scenarios where cache control should be adaptive to time-varying content request traffic. This paper proposes to exploit deep reinforcement learning (DRL) to enhance the convergence speed and adaptability of utility-driven time-to-live (TTL)-based caching. Employing DRL with long short-term memory helps a caching agent learn how it adapts to the temporal correlation of content popularities to shorten the transient-state before the optimal steady-state. In addition, we elaborately design the state and action spaces of DRL to overcome the curse of dimensionality, which is one of the most frequently raised issues in machine learning-based approaches. Experimental results show that policies trained by DRL can outperform the conventional utility-driven caching algorithm under some non-stationary environments where content request traffic changes rapidly.
\subsubsection{Deep Reinforcement Learning for cell ON-Off energy saving on Wireless Networks}
abstract:Increased traffic demands are leading to extremely dense network deployments in current and future wireless networks. This translates to significant growth in energy consumption at the radio access networks (RAN), resulting in high network operation costs (OPEX). In this work, we apply deep reinforcement learning to reduce the energy consumption at the base station in dense wireless network deployments by allowing cells that overlap in geographical areas to be put in standby mode according to the changing network conditions. We start by first formulating the problem of cell ON/OFF energy saving on dense wireless networks as a Markov decision process. Then, a deep reinforcement learning (DRL) solution is proposed. This DRL solution takes into account different key performance indicators (KPIs) of both the network and user equipment (UEs), and aims to reduce the energy consumed by the network without significantly impacting the overall KPIs. The performance of the proposed solution is evaluated using a network simulator.
\subsubsection{BEAR: Reinforcement Learning for Throughput Aware Borrowing in Energy Harvesting Systems}
abstract:Energy Borrowing (EB) aided Energy harvesting (EH) systems provide a greener alternative to self-sustaining electronic devices in a complex, unprecedented environment by borrowing energy from a supplementary source to regulate the data transmission flow. We propose a reinforcement learning-based algorithm for energy scheduling policy which jointly optimizes the EB and utilizes harvested energy for efficient data transfer at every time instant. As the exact pattern of harvested energy and channel conditions at any time slot is unknown, the proposed algorithm, BEAR (Borrowing Energy with Adaptive Rewards), based on actor-critic architecture, learns the optimal power allocation policy for the transmission node. Our designed reward function accommodates the concept of adaptive penalty to punish the transmission node for selecting unfavourable actions. Our simulations show that the BEAR algorithm providing efficient energy management with a focus on throughput maximization yields a 35.45% enhancement in sum throughput over a typical non-borrowing system. Lastly, nontrivial design insights are outlined via numerical results to quantify the practical efficacy of BEAR for EH systems.
\subsubsection{Improved UCB-based Energy-Efficient Channel Selection in Hybrid-Band Wireless Communication}
abstract:While hybrid-band wireless systems recently gained prominence to achieve high capacity, selecting the best channel in these systems in real-time is still a formidable research challenge that requires further investigations. In this paper, we address this challenge in terms of an optimization problem, which is reformulated as a stochastic multi-armed bandit (MAB). Then, we introduce online learning-based solutions to solve the MAB problem for the multi-band/channel selection (MBS). Improved variants of the upper confidence bound (UCB) scheme are investigated and modified to be energy-aware. Hence, we propose Energy-Aware Randomized UCB-MBS (EA-RUCB-MBS) and Energy-Aware Kullback-Leibler UCB-MBS (EA-KLUCB-MBS) methods, which demonstrate near-optimal results. Also, EA-KLUCB-MBS exhibits the fastest convergence, while the convergence of EA-RUCB-MBS is similar to that of the original UCB. Based on extensive simulation results, we evaluate the performance of our proposed algorithms against benchmark MBS schemes including UCB and Thompson sampling (TS).
\subsubsection{Learning Assisted Identification of Scenarios Where Network Optimization Algorithms Under-Perform}
abstract:We present a generative adversarial method that uses deep learning to identify network load traffic conditions in which network optimization algorithms under-perform other known algorithms: the Deep Convolutional Failure Generator (DCFG). The spatial distribution of network load presents challenges for network operators for tasks such as load balancing, in which a network optimizer attempts to maintain high quality communication at the same time abide capacity constraints. Testing a network optimizer for all possible load distributions is challenging if not impossible. We propose a novel method that searches for load situations where a target network optimization method under-performs baseline, which are key test cases that can be used for future refinement and performance optimization. By modeling a realistic network simulator's quality assessments with a deep network and, in parallel, optimizing a load generation network, our method efficiently searches the high dimensional space of load patterns and reliably finds cases in which a target network optimization method under-performs a baseline by a significant margin.
\subsection{Satellite Networking}
\subsubsection{Controller Placement in SDN-enabled 5G Satellite-Terrestrial Networks}
abstract:SDN-enabled Integrated satellite-terrestrial networks (ISTNs), can provide several advantages including global seamless coverage, high reliability, low latency, etc., and can be a key enabler towards next-generation networks. To deal with the complexity of the control and management of the integrated network, leveraging the concept of software-defined networking (SDN) will be helpful. In this regard, the SDN controller placement problem in SDN-enabled ISTNs becomes of paramount importance. In this paper, we formulate an optimization problem for the SDN controller placement with the objective of minimizing the average failure probability of SDN control paths to ensure the SDN switches receive the instructions in the most reliable fashion. Simultaneously, we aim at deploying the SDN controllers close to the satellite gateways to ensure the connection between the two layers occurs with the lowest latency. We first model the problem as a mixed integer linear program (MILP). To reduce the time complexity of the MILP model, we use submodular optimization techniques to generate near-optimal solutions in a time-efficient manner. Finally, we verify the effectiveness of our approach by means of simulation, showing that the approximation method results in a reasonable optimality gap with respect to the exact MILP solution.
\subsubsection{Age-Critical Pilot Allocation Random Access Protocol for Space-Air-Ground Integrated Networks}
abstract:Due to ubiquitous coverage inherited from the satellites, space-air-ground integrated networks (SAGIN) has been viewed as a promising enabler to provide "anywhere and anytime" broadband access for the next generation of mobile network.Nevertheless, the status updating to the satellite of terrestrial sensing devices could be hindered by the propagation delay. As a result, it becomes crucial to investigate the timeliness of information for massive machine type communications (mMTC) random access in SAGIN.In this paper, we analyse the timeliness of information for the mMTC random access scenario via a new performance metric named age of information (AoI), and propose an age-critical pilot allocation (ACPA) random access protocol aiming to lower the system average AoI.By tracking the AoI evolution of each device via Markovian analysis, the closed-form expression of the system average AoI is derived, then we conduct an optimal number of slots to achieve the lowest system average AoI with the increasing of the system load.Simulation results validate the accuracy of our theoretical analysis, and also show that our ACPA protocol can significantly outperform other relevant random access protocols in terms of reducing the AAoI in overload cases.
\subsubsection{Analysis of Pointing Loss Effects in Deep Space Optical Links}
abstract:Owing to the extremely narrow beams, a main issue in optical deep space communications is represented by miss-pointing errors, which may severely degrade the system performance and availability. In this paper, we address pointing losses in the case in which both the receiver and the transmitter are affected by angular errors. Pointing losses are evaluated through two approaches. The first approach is deterministic and only requires knowledge of a maximum angular error. The second approach requires knowledge of the angular error statistical distribution and tackles the problem from an outage probability viewpoint. These tools are then applied to analyze the impact of pointing losses in deep space optical links in which both terminals suffer from miss-pointing effects. The antenna gains are first optimized to maximize the effective system gain. The optimum antenna gains are then applied to evaluate maximum achievable ranges and to perform link design by means of optical link budgets.
\subsubsection{Improving goodput on shared satellite links with coded tunnels}
abstract:The Internet's main transport protocol, TCP, struggles to make full use of the capacity of geostationary (GEO) and medium-earth orbit (MEO) satellite links due to their long latency and bottleneck bandwidth, in particular if these are shared by many parallel TCP connections. Large TCP transfers may back off to effective rates orders of magnitude below the link's available bandwidth. Conventional mitigation measures either violate the Internet's end-to-end principle or contribute to bufferbloat. A hardware-based simulation of coded tunnels in this paper shows that there is another way: Using the erasure-correcting properties of random linear network codes, one can increase both goodput of large transfers and overall link utilisation in both GEO and MEO settings without extra buffer or end-to-end principle violations.
\subsection{Signal Processing for Satellite Systems I}
\subsubsection{Preamble detection in NB-IoT via Satellite: a Wavelet based approach}
abstract:Satellite Communications systems are a promising solution to extend and support terrestrial networks in un- or under-served areas. In Release 17, 3GPP initiated a Study Item for IoT over Non-Terrestrial Networks (NTN) to assess and adapt the NB-IoT air interface to the NTN characteristics. One of the main objective of the study is the evaluation of the Random Access procedure and the estimation of the up-link synchronization parameters at the satellite. In this context, it is essential that the detection algorithms of the NB-IoT preamble, at the receiver, satisfy the user detection requirements as well as the timing synchronization accuracy. This is not a trivial task, especially in satellite channels where the carrier frequency offset (CFO) is more severe than that of terrestrial links. In order to cope with this problem, we propose a new algorithm based on a non-decimated dyadic wavelet transform. This method is able to detect the incoming preambles and to estimate their time of arrival, without having to compensate the CFO first. The performance of the proposed algorithm is compared with a classical estimation approach based on the Fast Fourier Transform, substantiating the significant advantage obtained in the considered NTN scenarios.
\subsubsection{A Novel PSO-Based Pattern Synthesis Method for Conformal Array with Dynamic Range Ratio Constraint}
abstract:Particle swarm optimization (PSO) has received great attention for its flexibility and brevity in conformal antenna array pattern synthesis (CAAPS). However, it also suffers from local optimum and increased computational complexity. In this paper, a solution space pruning particle swarm optimization(SSP-PSO) dedicated to the CAAPS is proposed, which selectively optimizes the peak sidelobe level (PSLL) among the multiple optimization objectives and accordingly applies iterative fast Fourier transform (IFT) at the first stage, and then avoids ineffective search by accomplishing SSP on the basis of dynamic range ratio constraints and the element excitation that is obtained from the foregoing PSLL optimization. The simulation results verify the superiority of the proposed method in terms of both convergence accuracy and convergence speed.
\subsubsection{Channel Modeling and Signal Transmission for Land Mobile Satellite MIMO}
abstract:In this paper, a land mobile satellite (LMS) multiple-input multiple-output (MIMO) is considered, where two satellites simultaneously communicates with a mobile user terminal (UT). Spatial degree of freedom brought by the two satellites is introduced in the channel modeling, aside of other channel parameters including time correlation, shadowing, multipath fading and Doppler effect. Then an algorithm table using Markov multiple-state transition is provided to generate the LMS MIMO channels. Based on the modeled LMS MIMO channels, signal transmission between two satellites and the UT using space-time block coding is considered. Simulation results show that compared to the single satellite communications, the dual-satellite MIMO communications can achieve better bit error rate performance under the same signal-to-noise-ratio condition. In particular, the performance of dual-satellite single-polarization communications is slightly worse than that of single-satellite dual-polarization communications, since the spatial correlation is stronger than the polarization correlation.
\subsubsection{Precoding Design for Joint Positioning and Synchronization in 5G Integrated Satellite Communications}
abstract:The development of an integrated satellite-terrestrial communication network has become one of the focuses in both academic and industry in order to provide genuine seamless coverage. For the integrated satellite and terrestrial 5G communication systems, positioning information of user terminals (UTs) can be beneficial in addressing several challenges. In this paper, we propose to utilize 5G new radio synchronization signals to perform positioning. To simultaneously guarantee positioning and synchronization performances for UTs in any place of a cell coverage, we investigate the precoding design at the satellite side for joint positioning and synchronization (JPS) in 5G integrated satellite-terrestrial networks. By considering the angle of departure estimation and missed detection probabilities for the UTs, we provide the precoding design criteria for positioning and synchronization, respectively. Then we introduce the constraint of equal transmit power on every antenna. Based on the criteria and constraint, we formulate the optimization problem for JPS and exploit the conjugate gradient algorithm under the manifold optimization framework to design the precoder. Simulation results show that the proposed precoder can ensure that JPS achieves satisfactory performances within the whole cell coverage.
\subsubsection{Inter-Plane Inter-Satellite Connectivity in LEO Constellations: Beam Switching vs. Beam Steering}
abstract:Low Earth orbit (LEO) satellite constellations rely on inter-satellite links (ISLs) to provide global connectivity. However, one significant challenge is to establish and maintain inter-plane ISLs, which support communication between different orbital planes. This is due to the fast movement of theinfrastructure and to the limited computation and communication capabilities on the satellites. In this paper, we make use of antenna arrays with either Butler matrix beam switching networks or digital beam steering to establish the inter-plane ISLs in a LEO satellite constellation. Furthermore, we present a greedy matching algorithm to establish inter-plane ISLs with the objective of maximizing the sum of rates. This is achieved by sequentially selecting the pairs, switching or pointing the beams and, finally, setting the data rates. Our results show that, by selecting an update period of 30 seconds for the matching, reliable communication can be achieved throughout the constellation, where the impact of interference in the rates is less than 0.7 % when compared to orthogonal links, even for relatively small antenna arrays. Furthermore, doubling the number of antenna elements increases the rates by around one order of magnitude.
\subsection{Signal Processing for Satellite Systems II}
\subsubsection{A Novel Deep Learning GPS Anti-spoofing System with DOA Time-series Estimation}
abstract:Many critical systems and infrastructure rely on the Global Positioning System (GPS) for synchronization of clocks that is essential for their operation. Sophisticated GPS spoofing techniques can mimic legitimate GPS transmissions soclosely making it difficult for even the most advanced anti-spoofing methods to detect them. This paper reports a novel deep-learning (DL)-based GPS anti-spoofing technique that relies only on the physical attributes unique to a signal originated at an orbiting satellite. It uses a multi-element antenna array receiver to estimate the instantaneous signal direction-of-arrival (DOA)using a subspace-based statistical signal processing. Time-series of the estimated DOAs of signals are input to a convolutional neural network deep-learning classifier that learns the embedded signatures unique to the trajectories of signal sources to separate authentic and spoofed GPS signals. A software implementation of the designed system using actual GPS orbital data demonstrated on average 95% accuracy even against dynamic airborne spoofing systems. A hardware implementation, using a 4-element antenna array, an RF-transceiver and a microprocessor, was shown to detect spoofed signals with above 93% accuracy. Unlike existing methods, this DL-based anti-spoofing system does not require knowledge of receiver's location and orientation or manual thresholds making it suitable for moving platforms. The proposed technique is capable of countering even the most advanced spoofing systems since it is difficult to exactly replicate the DOA time-series of a satellite even by an airborne spoofing transmitter.
\subsubsection{Blind Symbol Timing and Carrier Phase Estimation for PCMA Satellite Signals via Cyclic Statistics}
abstract:For the design of a blind receiver for paired carrier multiple access (PCMA) signals in satellite communications, several detection schemes have been proposed that require knowledge of the channel. In a frequency-nonselective environment,the overall channel is determined by the synchronization and transmit signal parameters of the respective carrier signals. In this paper, schemes for estimation of the carrier and symbol timing phases at the receiver are proposed, assuming that knowledge of the frequency offsets and symbol rate is obtained by previous estimations. Two different approaches for inferring the desired parameters are introduced. The proposed schemes exploit the cyclostationary nature of the respective carrier signals and the difference in their frequency offsets, and are based on estimates of the time-varying moments of the received signal andtheir cycle coefficients.
\subsubsection{Channel Estimation for mmWave Satellite Communications with Reconfigurable Intelligent Surface}
abstract:We consider an mmWave satellite communication system with a reconfigurable intelligent surface (RIS) to enhance the signal coverage, where both the satellite and the served users are equipped with phased arrays. Different from the existing methods that separately estimate the uplink channel from the user to the RIS and that from the RIS to the satellite, we directly estimate the cascaded channel by proposing two schemes. In the first scheme with two stages, we power off the last antennas of the satellite, user and the RIS in the first stage and then transceive some pilot symbols, while in the second stage we power off the first antennas of the satellite, user and the RIS and then transceive the same pilot symbols. Then we perform the channel estimation based on the estimating-signal-parameter-via-rotational-invariance-techniques (ESPRIT) method. In the second scheme that does not power off any antenna and needs only one stage, we propose a null space projection (NSP) algorithm, where the equivalent channel matrix is estimated through projecting the dictionary steering vectors to the null space of the received signal covariance matrices. Simulation results show that the NSP scheme needs much fewer pilots and much lower hardware complexity than the ESPRIT scheme but with some sacrifice in channel estimation performance.
\subsubsection{Doppler Shift Estimation in 5G New Radio Non-Terrestrial Networks}
abstract:Evolving 5G New Radio (NR) to support non-terrestrial networks (NTNs), particularly satellite communication networks, is under exploration in 3GPP. The movement of the spaceborne platforms in NTNs may result in large timing varying Doppler shift that differs for devices in different locations. Using orthogonal frequency-division multiple access (OFDMA) in the uplink, each device will need to apply a different frequency adjustment value to compensate for the Doppler shift. To this end, the 3GPP Release-17 work on NTNs assumes that an NTN device is equipped with a global navigation satellite system (GNSS) chipset and thereby can determine its position and calculate the needed frequency adjustment value using its position information and satellite ephemeris data. This makes GNSS support essential for the NTN operation. However, GNSS signals are weak, not ubiquitous, and susceptible to interference and spoofing. We show that devices without access to GNSS signals can utilize reference signals in more than one frequency position in an OFDM carrier to estimate the Doppler shift and thereby determine the needed frequency adjustment value for pre-compensating the Doppler shift in the uplink. We analyze the performance, elaborate how to utilize the NR reference signals, and present simulation results. The solution can reduce the dependency of NTN operation on GNSS with reasonable complexity and performance trade-off.
\subsubsection{Exploiting topology awareness for routing in LEO satellite constellations}
abstract:Low Earth Orbit (LEO) satellite constellations combine great flexibility and global coverage with short propagation delays when compared to satellites deployed in higher orbits. However, the fast movement of the individual satellites makes inter-satellite routing a complex and dynamic problem.In this paper, we investigate the limits of unipath routing in a scenario where ground stations (GSs) communicate with each other through a LEO constellation. For this, we present a lightweight and topology-aware routing metric that favors the selection of paths with high data rate inter-satellite links (ISLs). Furthermore, we analyze the overall routing latency in terms of propagation, transmission, and queueing times and calculate the maximum traffic load that can be supported by the constellation. In our setup, the traffic is injected by a network of GSs with real locations and is routed through adaptive multi-rate inter-satellite links (ISLs). Our results illustrate the benefits of exploiting the network topology, as the proposed metric can support up to 53% more traffic when compared to the selected benchmarks, and consistently achieves the shortest queueing times at the satellites and, ultimately, the shortest end-to-end latency.
\subsection{Smart Grid Optimization}
\subsubsection{Towards Cost-Optimal Energy Procurement for Cooling as a Service: A Data-Driven Approach}
abstract:Cooling as a service (CaaS) is an emerging business that provides air conditioning services to buildings. With the rapid development of business and the increasing energy load, service providers need effective energy procurement solutions. In this paper, we propose a data-driven approach to energy procurement for CaaS providers. First, we focus on two dominant variables of cooling energy cost, including outdoor temperature and electricity price, and predict their trends in the next day. Accordingly, we estimate the energy usage for the next day and purchase energy in the day-ahead energy market (DAM) one day before the actual usage. During the real-time operation, we adjust the service quality without breaching the service standards as the demand response to the real-time energy market (RTM). We conducted experimental studies to verify the performance of the proposed solution. The results show that the solution provides high-quality cooling services with minimal energy expenditure and helps to improve the stability of the power grid.
\subsubsection{Optimal Cost Network Design for Bounded Delay Data Transfer from PMU to Control Center}
abstract:Communication network topology design problem in a smart grid environment has received considerable attention in recent times, because in this environment, power transmission control data, generated by Phasor Measurement Units (PMUs), needs to be exchanged between Substations (SS) and Controls Centers (CC) in real time. In this paper, we formalize this design problem studied in a previous paper as the Rooted Delay Constrained Minimum Spanning Tree (RDCMST) problem. While other researchers have studied the RDCMST problem in a topological setting, we study it in a geometric setting. We provide a modified version of the well-known Prim's algorithm for construction of a Minimum Spanning Tree of a graph to solve the RDCMST problem. We (i) establish the necessary and sufficient condition for the existence of a solution for the RDCMST problem, (ii) demonstrate that our algorithm may fail to find the optimal solution for some problem instances, (iii) characterize conditions on the input data which will ensure that our algorithm will find the optimal solution, and (iv) demonstrate that under some pathological condition, the ratio between our algorithm and the optimal solution can be arbitrarily large. We provide an Integer Linear Programming formulation for the problem for computation of the optimal solution. We evaluate the performance of our algorithm with real substation location data of Arizona. In our experiments, our algorithm always produced either optimal or near optimal solutions.
\subsubsection{Energy Demand Prediction with Optimized Clustering-Based Federated Learning}
abstract:The rapid growth in pervasive Internet-of-Things (IoT) and Deep Learning (DL) is creating a huge demand for applying DL on IoT systems. However, it is non-trivial to train highly accurate DL models in such scenarios due to the following two challenges: (1) individual IoT devices may not have sufficient training data, and (2) simply combining all sensory data across all devices may cause performance degradation due to data imbalance and varying temporal patterns across different devices. The objective of this paper is to achieve high-accurate prediction models for each device in an IoT system. We propose a federated learning approach for IoT systems driven by trend based clustering for energy demand prediction for Electric Vehicle (EV) charging station network. We first apply a time-series clustering method to identify stations with similar temporal demand patterns. Using time-series data from stations in a cluster, a single Long-Short Term Memory (LSTM) network is trained using FedAvg algorithm for energy demand prediction for all the stations in the cluster. Experimental results on a real-world energy usage dataset from an EV charging station network show that our proposed approach is very competitive against baseline federated learning approaches. In particular, the energy demand prediction error decreases by 80\% compared with the best existing algorithm.
\subsubsection{Multi-objective Mobile Charging Scheduling on the Internet of Electric Vehicles: a DRL Approach}
abstract:Mobile charging services (MCSs) have been developed as a supplement charging method for electric vehicles (EVs), wherein energy replenishment is provided by mobile charging vehicles (MCVs). An MCV has an internal storage system employed to replenish the energy of a certain number of EVs. Charging scheduling of MCV is one of the key issues on the Internet of EVs for providing efficient and convenient charging services, which requires determining the charging sequence and the amount of energy when serving multiple EVs by one MCV. In this paper, a multi-objective MCV scheduling problem is investigated. By optimizing the charging sequence and the actual amount of energy being charged, the proposed framework aims to minimize the EV waiting time while simultaneously to maximize the charging benefits of all EVs. To solve the multi-objective optimization problem (MOP), a deep reinforcement learning (DRL) based framework is further explored. The MOP is first decomposed into a set of subproblems. Each subproblem is modelled as a neural network, wherein an actor-critic algorithm and a modified pointer network are adopted to solve each subproblem. Pareto optimal solutions can be directly obtained through the trained models. The experimental results demonstrate that the proposed method can efficiently and effectively solve the MCV scheduling problem and outperform NSGA-II and MOEA/D in terms of solution convergence, solution diversity, and computing time. In addition, the trained model can be applied to newly encountered problems without retraining.
\subsection{Smart Grid Security}
\subsubsection{Honeypot-Enabled Optimal Defense Strategy Selection for Smart Grids}
abstract:Smart girds have been increasingly spotted as high-profile targets of cyber assaults over the years.To better understand the cyber threat landscape, honeypots have been widely used in the smart gird security community, i.e., identifying unauthorized penetration attempts and observing the behaviors in such activities.In this paper, we propose a honeypot-enabled optimal defense strategy selection approach for smart girds, based on a novel stochastic game. Specifically, the interactions between the attacker and smart gird defender are captured using our designed stochastic game, a non-cooperative two-player game with incomplete information. We take into account various possible defenses from a smart gird defender and offensive strategies from the attacker. Then the Nash equilibrium is calculated by the stochastic game model, which is derived exhibiting an optimal defense strategy for the smart gird defender.Extensive simulation experiments demonstrate the effectiveness of the proposed scheme.
\subsubsection{A Multi-Agent Reinforcement Learning Approach for Blockchain-based Electricity Trading System}
abstract:In microgrid, peer-to-peer (P2P) electricity trading has quickly ascended to the spotlight and gained enormous popularity. However, there are inevitable credit problems and system security problems. Besides, the current model in the electricity trading system cannot balance the utilities of multiple trading entities. In this paper, we propose a blockchain-based distributed P2P electricity trading system. We define elecoins as currency in circulation within our trading system. In order to jointly optimize the utilities of both parties in the elecoins trading, we formulate the elecoins purchasing problem as a hierarchical Stackelberg game. Then, we design a distributed multi-agent utility-balanced reinforcement learning (DMA-UBRL) algorithm to search the Nash equilibrium. Finally, we factually build a blockchain system with a blockchain explorer and deploy an electricity trading smart contract (ETSC) on Ethereum, with a website interface for operating. The numerical results and the implemented realistic system show the advantages of our work.
\subsubsection{More Power to Save Lives: Distributed Power Scheduling in Smart Microgrid for Disaster Management}
abstract:An independent microgrid plays a significant rolein supporting disaster management, especially during a poweroutage. However, the limited power supply needs sophisticatedscheduling to manage distributed power consumption and supplyfor a long service period. This paper proposes a distributedpower scheduling strategy in the smart microgrid to extendthe power supply after a disaster happens. We first investigatethe power supply and consuming devices in microgrid andmodel the distributed scheduling as a multi-armed bandit (MAB)problem. To solve the MAB problem, we design an explorationalgorithm to formulate the optimal scheduling strategy. Extensivesimulation results show that the proposed method outperformsother strategies in extending the power supply period during thepower outage after a disaster happens.
\subsection{Social Internet of Things}
\subsubsection{Efficient Online Decentralized Learning Framework for Social Internet of Things}
abstract:Online Decentralized Learning (ODL) is suitable for Internet-of-Things (IoT) devices since only parameter updates are exchanged with neighbors to avoid uploading private data to a central server and the training data is allowed to arrive at the devices sequentially. However, the current ODL frameworks cannot support the emerging Social IoT (SIoT) paradigm favorably since the SIoT devices exchange parameter updates with only trustworthy neighbors based on specific social relations (e.g., parental object relation and ownership object relation). Conversely, sharing parameter updates with untrustworthy neighbors could speed up the training process but may violate social relations. Differential privacy (DP) is thus used to ensure data security while excessive devices engaging DP may downgrade the training performance. However, most research neglects the effect of neighbor selection for each device based on social networks, physical networks, and DP. Thus, in this paper, we innovate an ODL framework ODLF-PDP to allow only a part of devices to engage DP (i.e., partially DP) to improve training performance. Then, an algorithm BeTTa is proposed to build an adequate communication topology based on the interplay among the social networks, physical networks, and DP. Last, the experiment results manifest that ODLF-PDP saves more than 20% physical training time compared to the current frameworks via the benchmark of MNIST.
\subsubsection{A Multi-tiered Social IoT Architecture for Scalable and Trusted Service Provisioning}
abstract:In the Social Internet of Things paradigm, the Trust Management System computes trust values of involved social objects, identifies trusted relationships, and selects the most suitable object able to provide a target service. State-of-the-art mechanisms conceived to address these tasks generally avoid considering the actual availability of social objects and demand the implementation of complex algorithms to constrained nodes.This work presents a novel multi-tiered and fog-based Social Internet of Things architecture to solve these open issues, ensuring fast service provisioning, high scalability, fault tolerance, and security. On the one hand, the Trust Management System hosted at the first fog layer of the architecture jointly addresses the trustworthiness of service providers and monitors the resource availability exposed by social objects, thus simplifying the forwarding of service requests to trusted and unloaded nodes. From another hand, to securely implements advanced services at a large scale, a second fog layer exploits a Blockchain-based storage for sharing services, relationships, and trust values across organizations and service domains. Computer simulations demonstrate the effectiveness of the proposed architecture in a realistic Social Internet of Things while showing the performance gain obtained against a baseline approach.
\subsubsection{Explainable Health State Prediction for Social IoTs through Multi-Channel Attention}
abstract:The core technology of Industry 4.0 is to enable the intelligence of manufacturing. One of the important tasks is anomaly detection. Although existing anomaly detection methods have achieved high accuracy, the basis of judgments cannot provide explainability, which greatly reduces the possibility for improving the model or facilitating human-machine cooperation. Therefore, in this paper, the goal is to provide the explainability for machine fault detection for social IoTs and realize the health monitoring and prognosis of the bearings simultaneously. Specifically, vibration signals from multiple sensors are visualized through short-time Fourier transform. Afterward, the features of frequency-domain data are extracted by the Squeeze-and-Excitation block and self-attention mechanism to assess the degradation of whole system. Once the process enters the early degradation, we can identify the source of components that causes the abnormality through the attention weight distribution. Experimental results show that the proposed approach achieves high accuracy in run-to-failure tests. Moreover, the proposed approach shows a better ability to explain the predicted results than the state-of-the-art bearing detection methods.
\subsubsection{LOAN: Latency-Aware Task Offloading in Association-Free Social Fog-IoV Networks}
abstract:We present a latency-aware task offloading scheme, named LOAN, in a fog-enabled association-free social Internet of Vehicle (IoV) network that aims to minimize the delay of time-critical tasks while saving the starvation of best-effort tasks. A social fog-IoV network involves time-critical tasks because it is highly dynamic due to rapid changes in the network topology. Our work considers different priorities for the service of time-critical tasks while efficiently utilizing the fog resources. Different from the works in the literature that provide privilege to the time-critical tasks, LOAN handles the service of best-effort tasks efficiently without making them suffer conditions such as starvation. LOAN manages the priority levels of the tasks by incrementing their priorities based on their waiting time for the task service. We formulate the problem of efficient task service by suitable fog node as a coalition formulation game. Numerical results show that the LOAN achieves a reduction in delay compared to the existing and traditional methods by 27%, 36.5%, and 44%.
\subsection{Social Network Analysis I}
\subsubsection{HGENA: A Hyperbolic Graph Embedding Approach for Network Alignment}
abstract:Cross-network alignment aims at identifying users who participate in different social networks, which benefits a variety of downstream social applications such as precise content delivery, fraud detection, and content/user recommender systems. Recent advances in network representations and graph neural networks have spurred various network structure-based methods for capturing underlying node similarities across social networks, thereby addressing the network alignment problem. However, most of the existing solutions rely on embedding methods that compute node similarity in Euclidean space, resulting in severe distortion or semantic loss when representing real-world social networks, which are usually scale free and with hierarchical structures. We address these issues by presenting a novel model: Hyperbolic Graph Embedding for Network Alignment (HGENA), which learns the structural semantics more efficiently by embedding nodes in hyperbolic space instead of Eucledian. HGENA overcomes the scalability issue since it requires far fewer dimensions in Riemannian manifolds and increases the capability of learning hierarchical structures, while enabling smaller distortion for tree-liked networks to facilitate node alignment. We also introduce alternative network mapping functions to compute node similarity across-network based on its distance on the Poincare ball. Experimental evaluations conducted on real world datasets demonstrate that HGENA achieves superior performance on social network alignment, especially for more tree-liked networks.
\subsubsection{Joint Connection and Content Embedding for Link Prediction in Social Networks}
abstract:In social network analysis, link prediction is a task to predict the link possibility through the known information of the network structure. However, most current methods focus on the linear superposition of few social network attributes, which makes it difficult for relational content attributes to fully participate in the prediction. Moreover, obtaining low dimensional dense edge representation and edge weight from high-dimensional sparse social network plays a critical role in the improvement of prediction accuracy. In this paper, we propose a general framework that can predict the presence and weight of edges according to the local structure, topology and content of social networks. Firstly, we mine the representation of each node providing an exciting opportunity to advance our knowledge of feature extraction. Besides, based on the sparsity and high dimension, we use the joint embedding method to express the connection information and semantics information to learn the node representation. Furthermore, this study makes a significant contribution to research on convolutional neural network by encoding the corresponding type of node features and preserving the similarity between the original associated nodes. The prediction performance of edge presence and edge weight was experimentally investigated by large real-world datasets. The F1 index, which can measure the prediction effect of edge presence, is improved by at least 0.03. In addition, the MSE index and the PCC index of edge weight prediction are improved by at least 0.03 and 0.04 respectively. Our scheme could effectively capture the diversity of content embedding in different relational patterns.
\subsubsection{MRAInf: Multilayer Relation Attention based Social Influence Prediction Net with Local Stimulation}
abstract:Social networks have been part of human being's daily life and influence nearly every whit of our lives. Social influence prediction is an interesting topic to predict users will or won't be activated by current social spreading events, and deep learning-based approaches could obtain outstanding accuracy by graph neural networks (GNNs). However, GNN models are restricted by the 1-Weisfeiler-Lehman (WL) test, and represent the node structure by only the first layer neighbors but can't discriminate nodes with different second or more layer neighbors. To this end, we propose a multilayer relation attention based social influence prediction Net using GAT with local stimulation, named after MRAInf. Specifically, we design an enhanced node representation (ENR) to describe the original node structure vector by three-layer-neighbor adjacency relations, with more details for attention in GAT. Moreover, in GAT, we design a local stimulation (LS) mechanism with multi 1D convolutions to reinforce the feature map of the target node being predicted and to weaken feature maps of non-target nodes. The detailed latent information in ENR and local stimulation for target in GAT benefit the social influence prediction. We conduct extensive experiments on three benchmark datasets: Twitter, Open Academic Graph, and Digg, and the experimental results show that our approach outperforms existing comparison methods in terms of classification performance and predictive accuracy.
\subsubsection{On Comparing and Enhancing Two Common Approaches to Network Community Detection}
abstract:In this work, we explore two common algorithms for community detection in networks, namely Agglomerative Hierarchical Clustering and the Louvain Method. We investigate their mechanics and compare their differences in terms of implementation and results of the clustering behavior on a standard dataset. We further propose some enhancements to these algorithms that show promising results in our evaluations, such as self-neighboring for Neighbor Matrix constructions and a deterministic and slightly faster version of the Louvain Method that favors fewer bigger clusters.
\subsection{Social Network Analysis II}
\subsubsection{IGCN: Infected Graph Convolutional Network based Source Identification}
abstract:Source identification has a wide range of applications in daily life, including locating the rumor source in online social networks and finding origins of a rolling blackout in smart grids. Despite great success over the past decade, most prior arts are proposed based an assumption that the underlying propagation model is known in advance. However, this assumption maybe impracticable on real scenarios, since it is usually difficult to acquire the actual underlying propagation model. To avoid this limitation, in this paper, we propose the Infected Graph Convolutional Network (IGCN) layer by combining infection network with GCN (Graph Convolutional Network) layers to locate the rumor source without prior knowledge of underlying propagation model. For the first time, we define the problem of source identification as a special graph classification problem with source node as the label. By introducing the feature update method of GCN layer with the idea of attention, we build an IGCN model to adapt the infection networks such that the prediction accuracy on the source is improved under model independent scenarios. We conduct experiments on several real datasets and the results show the superiority of IGCN model to baseline algorithms
\subsubsection{MHCNC: A Novel Framework for Multi-Source Heterogeneous Cross-Network Node Classification}
abstract:In this paper, we study the problem of multi-source heterogeneous cross-network node classification, which leverages the abundant labeled nodes from multiple source networks to help the classification of unlabeled nodes in a target network. The existing single-source cross-network node classification methods mainly focus on the scenario where the source network and the target network share the same feature space, and the current multi-source transfer learning methods generally fail to model the structural information of networks. Thus, both of them cannot be applied to the problem of multi-source heterogeneous cross-network node classification. In this paper, we propose a novel framework for multi-source heterogeneous cross-network node classification (MHCNC) which integrates multi-source heterogeneous transfer learning with graph convolutional neural network so as to learn network-invariant and label-discriminative node representations. In MHCNC, we first devise the heterogeneous feature transformation which transform the multiple source feature spaces onto the target network to obtain new feature representations to well mitigate the distribution discrepancy between networks. In addition, we incorporate an inductive learning based convolutional neural network to predict the unlabeled nodes in the target network. Moreover, we propose a new model fusion strategy based on the classification weight of each model. Extensive experiments conducted on real-world social networks verify that the proposed algorithms outperform the state-of-the-art approaches in terms of classification accuracy.
\subsubsection{Empirical Analysis of Aging Effects on Preferential Attachment with a Massive Twitter Dataset}
abstract:Social networks are prominently studied for addressing many pressing challenges, including the spread of diseases and misinformation, orchestrated influence campaigns, and evolution of biological species. A striking common feature of these diverse real networks is the growth of topological structure that is widely understood by the preferential attachment property of scale-free networks that conform to the power law. However, preferential attachment by itself cannot fully explain the phenomenon of aging observed in many complex networks. Therefore, in this paper, we empirically analyzed a massive Twitter dataset and proved that the popularity of nodes tends to "age" over time and that recent nodes get a better chance of forming new connections. We further corroborate these findings with a simple yet effective optimization technique that generates a more accurate in-degree distribution of the Twitter content network. Results show that our approach optimizes the degree distribution of preferential attachment, reducing the RH distance measurement below 0.5 for more than 50% of the Twitter topics in the dataset. By quantifying as an aging coefficient, we also demonstrate that the aging effect is non-uniform across the network during the same time period.
\subsubsection{Pri-PGD: Forging privacy-preserving graph towards spectral-based graph neural network}
abstract:The development of Graph Neural Network (GNN) enables people to explore the value of graph data better. However, graph data often reveals more private information than data in Euclidean space. Some scholars have proposed that differential privacy can be used to add noise to the graph. However, the availability of the data will decrease with the increase of privacy. Some works utilize graph data through federated learning, which is computationally complex and heavily burdened by communication. Therefore, based on the principle of the spectral-based GNN model (Graph convolutional network, GCN), we proposed a method (Pri-PGD) to forge a privacy-preserving graph by disturbing. We set a user-selectable parameter m to convert the first m-order neighbor nodes to the first-order neighbor for each node. Pri-PGD achieves two goals: preserving convolution basis space and preserving first-order neighbor information of the real graph. This allows Pri-PGD to guarantee the privacy of the graph data while keeping the availability of it. We evaluated the effectiveness of our algorithm on three classical graph datasets (Cora, Citeseer, Pubmed). The accuracy of our method on the GCN model for the three datasets decreases only by about 1% when the parameter m is two. This verifies the validity of the forged graph obtained by Pri-PGD. As the experiment shows, our method is also valid for the Graph attention network model that is sensitive to first-order neighbor nodes.
\subsection{Social Network Applications}
\subsubsection{SWill-TAC: Skill-oriented Dynamic Task Allocation with Willingness for Complex Job in Crowdsourcing}
abstract:Allocating tasks to the best-fit candidates is a classical problem in crowdsourcing (CS). Most of the existing approaches assume that the task and candidate knowledge are known in advance and ignore the effect of enrolled candidates' willingness on the CS system's selection decision. For instance, an unwilling candidate assigned to a task may quit without completing it, thus depreciating the utility of the CS platform. In practice, a task or candidate may arrive or leave the CS system dynamically. Moreover, a complex task may be broken into smaller sub-tasks, each requiring a variety of computations and expertise. To overcome these challenges, based on a greedy algorithm, we propose a novel approach for skill-oriented dynamic task allocation with willingness factor for complex assignments (SWill-TAC). This approach iteratively attempts to delegate candidates (workers) to tasks depending on the skills required for executing the tasks and the candidates' skill set. SWill-TAC also considers the willingness of eligible candidates and keeps track of the budget constraints of tasks. Finally, the feasibility and efficiency of our approach are demonstrated using the UpWork dataset. Experimental results show that SWill-TAC outperforms Online Greedy, TM-Uniform, Random selection-based and Minimum payment-based task allocations in terms of the completed tasks count, the utility gained, and success ratio.
\subsubsection{Game Theoretic Opinion Models and Their Application in Processing Disinformation}
abstract:Disinformation, fake news and unverified rumors spread very fast in online social networks (OSNs) and manipulate people's opinions and decisions towards life events. The solid mathematical solutions of the strategic decisions in OSNs have been provided under game theory models including multiple roles and features. In this work, we investigate how different game theoretic opinion models of updating people's subject opinions can influence a way for people to handle disinformation. We compared the opinion dynamics of the five different opinion models (i.e., uncertainty, homophily, assertion, herding, and encounter-based) where an opinion is formulated based on Subjective Logic that offers the capability to deal with uncertain opinions. Via our extensive experiments, we observed that uncertainty-based opinion model shows the best performance in combating disinformation among all.
\subsubsection{Distributed Coordination by Social Learning in the Multi-Robot Systems of a Smart Factory}
abstract:Smart factories powered by Multi-Robot Systems (MRSs) play a central role in Industry 4.0 and smart manufacturing.MRS operating under dynamic task assignments of collaborative robots in production flows suggests a new technology paradigm to achieve productivity, flexibility, and energy efficiency to revolutionize the industry.However, dynamic collaborative MRSs, given resource-limited wireless communication, robotic AI computing, and lacking globally accurate references lead to a technological challenge to facilitate resilient, reliable, and precise operation of smart factories.This paper innovatively resolves this cyber-physical challenge by aiming to align robotic actions in dynamic production flows based on the concept of grouping and propose a social learning-based method that utilizes Bayesian network and reinforcement learning (RL) to provide robust coordination that improves productivity and resilience against bursty cyber and physical inaccuracies in difficult dynamic systems.Numerical experiments demonstrate successful fulfillment of technical requirements for the smart factory.
\subsubsection{A Privacy-Preserving Protocol for Proximity-Based Services in Social Networks}
abstract:A number of real-life social networks provide the users with proximity-based services. This feature exposes to serious privacy threats, because it could allow massive tracking from an honest-but-curious provider.Whilst proximity-based services have been deeply studied in the literature in a general setting, no solution (to the best of our knowledge) has been provided to the problem of delivering privacy-preserving proximity-based servicesentirely within existing social networks.The problem is not trivial, because a social network provider can play as a global passive adversary, monitoring the flow of all the messages exchanged in the network.Therefore, to allow proximity testing between Alice and Bob not requiring that they reveal their position to the social network provider is not enough.Indeed, even the fact that proximity testing is performed between Alice and Bob (independently of the result) is a serious privacy leakage.In this paper, we provide a solution preventing also this privacy leak,giving thus a concrete way to implement privacy-preserving proximity-based services in social networks.
\subsubsection{DRAGON: Detection of Related Account Groups for Online services with uncertain graphs}
abstract:With the rising influence of current online services, it is important for service providers to discover related accounts because it helps detect suspicious account groups. Existing research on this topic mostly focuses on a variety of account behaviors. Little attention has been paid to relations among account identity, which unveils the relationship between accounts and real-world people. In this paper, we propose DRAGON for modeling an account identity network and detecting suspicious account groups among this network. To this end, identifiers for tracking physical devices are collected and uncertain graph is used for modeling uncertainty in the network. Within this network, a strategy for detecting suspicious account groups is also investigated in DRAGON. We evaluate DRAGON using a real-world dataset. The results indicate that DRAGON achieves a 280% improvement in precision and 150% improvement in recall compared to a binary classifier.
\subsection{Social Recommendation}
\subsubsection{Attention-aware Multi-encoder for Session-based Recommendation}
abstract:In session-based recommendation, the user's next possible click can solely be predicted based on historical interaction behavior in the ongoing session. Previously representative works mainly use sequence models and graph neural networks to model user's behaviors of the session. These works have achieved promising results, but each also has certain defects. In view of the shortcomings of the previous works, we propose a multi-encoder framework, under which the advantages of each encoder are retained. Different encoders are used to mine different session features and finally generate a more powerful session representation to improve the recommendation result. Furthermore, in order to improve the performance of recommendation, we introduce the inter-session collaboration information by designing a Inter-session Collaboration Module. Extensive experiments on two real-world datasets demonstrate the superiority of our method over state-of-the-art algorithms.
\subsubsection{Semantic Analysis and Preference Capturing on Attentive Networks for Rating Prediction}
abstract:Nowadays, people receive an enormous amount of information from day to day. However, they are only interested in information which matches their preferences. Thus, retrieving such information becomes an significant task.Matrix Factorization (MF) based methods achieve fairly good performances on recommendation tasks. However, there exist several crucial issues with MF-based methods such as cold-start problems and data sparseness. In order to address the above issues, numerous recommendation models are proposed which obtained stellar performances. Nonetheless, we figured that there is not a more comprehensive framework that enhances its performance through retrieving user preferenceand item trend. Hence, we propose a novel approach to tackle the aforementioned issues. A hierarchical construction is employed in this proposed framework. The performance excels in comparison to state-of-the-art models by testing onseveral real-world datasets. Experimental results verified that our framework can extract useful features even under sparse data.
\subsubsection{Social Recommendation System with Multimodal Collaborative Filtering}
abstract:The widespread use of social network applications has led to an information explosion, making it difficult for users to find target information smoothly and accurately. The personalized recommendation has become a key solution in this situation, and it helps users to easily obtain the information that they are interested in. In this study, we propose a novel framework to provide recommendations more precisely, which is an attention-based recommendation system with multimodal graph collaborative filtering. Our system first exploits the high-order connectivity representations using graph convolutional networks, and then uses the relations between users and items to obtain features. After that, our system utilizes LSTM-based component to capture user's habits and preferred items. By leveraging the user and item information to create multimodal graphs, the proposed system is able to better capture users' habits and provide better recommendations for the users of social network applications. Through experiments on three real-world datasets, we demonstrate that our proposed system is able to significantly outperform the baseline methods.
\subsubsection{Rectilinear Range Query Processing on SpatialHadoop Platform}
abstract:Querying information from big geographic data attracts a lot of attentions. As the data are extremely large, it still takes a lot of computing time even by parallel computing with hadoop/MapReduce and cannot meet the requirement of prompt response. For storing spatial big data, putting geographically close data together are easier for indexing as well as information querying, which is most adopted in the nowadays database systems like SpatialHadoop or Hadoop-GIS. There are a heavy demand on querying data within a polygonal administrative regions like country and city. However, most database management systems do not support polygon query. In the paper, we study the problem of polygon query in the big geographic data and adopt a filtering-refinement strategy to reduce the computing time. Specifically, since it is easy to determine whether a data point is within a rectilinear polygon, we propose to use a rectilinear polygon to cover the inner area of a polygon region so as to filter out most data at the first stage. Then, we use the traditional ray-casting algorithm to examine the rest data within the rectangles that are crossed by the polygon border. To verify our design, we conduct experiments by using the earthquake datasets and the results show that our approach significantly reduce the computing on the Hadoop or Spatial Hadoop platforms, especially when the polygon is complex.
\subsection{System design}
\subsubsection{An LSTM-based Approach for Holdover Clock Disciplining in IEEE 1588 PTP Applications}
abstract:This paper discusses the application of long short-term memory (LSTM) neural networks to maintain the synchronization of a real-time clock in holdover operation, that is, while the timing reference input of the clock is unavailable. The approach trains the LSTM network based on timestamps acquired while the slave clock is locked to its reference input coming from a master clock. When the slave clock loses its reference and enters holdover mode, the LSTM takes over and controls the clock. We evaluate the method on a testbed consisting of IEEE 1588 precision time protocol (PTP) clocks based on field-programmable gate arrays (FPGA), where we collect nanosecond-accurate timestamps for offline analysis. We evaluate two oscillator stability scenarios: when the PTP clocks rely on oven-controlled crystal oscillators (OCXOs) and when they use crystal oscillators (XOs). In both cases, we demonstrate that the algorithm can sustain the clock synchronization accuracy within reasonable limits over intervals of 1000 seconds in two different temperature scenarios.
\subsubsection{Federated Learning Beyond the Star: Local D2D Model Consensus with Global Cluster Sampling}
abstract:Federated learning has emerged as a popular technique for distributing model training across the network edge. Its learning architecture is conventionally a star topology between the devices and a central server. In this paper, we propose two timescale hybrid federated learning (TT-HF), which migrates to a more distributed topology via device-to-device (D2D) communications. In TT-HF, local model training occurs at devices via successive gradient iterations, and the synchronization process occurs at two timescales: (i) macro-scale, where global aggregations are carried out via device-server interactions, and (ii) micro-scale, where local aggregations are carried out via D2D cooperative consensus formation in different device clusters. Our theoretical analysis reveals how device, cluster, and network-level parameters affect the convergence of TT-HF, and leads to a set of conditions under which a convergence rate of O(1/t) is guaranteed. Experimental results demonstrate the improvements in convergence and utilization that can be obtained by TT-HF over state-of-the-art federated learning baselines.
\subsubsection{An Unsupervised Learning-Based Approach for Symbol-Level-Precoding}
abstract:This paper proposes an unsupervised learning-based precoding framework that trains deep neural networks (DNNs) with no target labels by unfolding an interior point method (IPM) proximal `log' barrier function. The proximal `log' barrier function is derived from the strict power minimization formulation subject to signal-to-interference-plus-noise ratio (SINR) constraint. The proposed scheme exploits the known interference via symbol-level precoding (SLP) to minimize the transmit power and is named strict Symbol-Level-Precoding deep network (SLP-SDNet). The results show that SLP-SDNet outperforms the conventional block-level-precoding (Conventional BLP) scheme while achieving near-optimal performance faster than the SLP optimization-based approach.
\subsubsection{CE-SGD: Communication-Efficient Distributed Machine Learning}
abstract:Training large-scale machine learning models usually demands a distributed approach to efficiently process the huge amount of training data. However, the high network communication cost introduced by parallel stochastic gradient descent (SGD) algorithms is a well-known bottleneck. To this end, we propose CE-SGD, a communication efficient distributed machine learning algorithm that aggressively reduces the amount of gradient data exchanged among the training workers. CE-SGD belongs to the family of gradient sparsification schemes. It adaptively adjusts the gradient sparsity according to the model's feedback and selectively transmits the gradient based on their degree of participation in backpropagation. We mathematically prove the convergence of CE-SGD for both convex and non-convex cases and conduct a series of experiments on our CE-SGD implementation. Compared to the state-of-the-art algorithms, our experiments reveal that CE-SGD can achieve fast convergence, desirable gradient compression ratio, and high accuracy with low network bandwidth cost.
\subsubsection{Learning via Denoising Autoencoder on 5G NR Phase Noise Estimation}
abstract:In this paper, on phase noise of 5G NR mmWave systems, we propose a learning-based common phase error (CPE) estimation algorithm based on the denoising autoencoder serving as a nonlinear filter on the existing CPE estimators. The proposed algorithm learns the low dimension manifold of phase noise distributions with high probability. Traditional CPE methods have limitation when the time domain pilots are few, which causes degraded system performance with CPE interpolation. Besides accurate CPE estimation, the proposed method is more robust to various FR2 channels, Doppler effects, and the numerologies. Simulation results show that block error rate (BLER) could be improved up to 1.43 dB on SNR.
\subsection{Traffic management I}
\subsubsection{AFB: Improving Communication Load Forecasting Accuracy with Adaptive Feature Boosting}
abstract:Prediction of key system characteristics, such as the communication load, is required to overcome the delays inwireless communication systems. State-of-The-Art (SOTA) approachesmostly apply existing Neural Network (NN) structures,and extract latent features purely based on their sensitivity to theforecasting accuracy. This way of feature extraction may neglectsome non-obvious yet informative dimensions in the model input,leading to inaccurate forecasting results. In this paper, we presentan Adaptive Feature Boosting (AFB) approach, which integratesmultiple AutoEncoders (AEs) to automatically extract robust andcomprehensive latent features for communication load forecasting.The recurrent and residual connections among the AEsmake sure that the extracted latent features are representativefor all input dimensions. With more comprehensive informationextracted from the history, the forecasting accuracy is thusimproved. We evaluate AFB against existing approaches on areal-world dataset that contains Call Detail Records (CDRs) ofthe Milan city over a period of two months. The evaluation showsthat our AFB-based approach achieves 35.2% more accurate loadforecasting results than the SOTA deep approaches.
\subsubsection{Fully-Decentralized Multi-Kernel Online Learning over Networks}
abstract:Fully decentralized online learning with multiple kernels (named FDOMKL) is studied, where each node in a network learns a sequence of global functions in an online fashion without the control of a central server. Each node finds the best global function via online alternating direction method of multipliers (ADMM) and the Hedge algorithm to integrate information from its one-hop neighboring nodes. The learning framework for an individual node is based on the kernel learning method and the proposed algorithm successfully leverages multiple kernel scheme to find the best common function over the entire network. To the best of our knowledge, this is the first work that proposes a fully-decentralized online learning algorithm based on multiple kernels.The proposed FDOMKL preserves privacy by maintaining the local data at the edge nodes and exchanging model parameters only. We rigorously prove that FDOMKL achieves a sublinear regret bound compared with the genie-aided scenario where the fusion center knows the best kernel function. Furthermore, numerical tests on real time-series datasets demonstrate the superiority of the proposed algorithm with respect to learning accuracy and network consistency compared to state-of-the-art single kernel methods.
\subsubsection{Deep Learning-based QoS Prediction with Innate Knowledge of the Radio Access Network}
abstract:To enable safe and advanced cooperative driving, quality of service (QoS) prediction in the radio access network has triggered recent attention to gracefully adapt vehicle-to-everything (V2X) applications to conform to expected network performance prior to the actual change in QoS. However, the communication and computation overhead combined with additional factors such as privacy may affect training data availability. Innate knowledge of wireless communication in the QoS prediction model could help improve prediction performance in environments with reduced training data availability. This paper presents a heuristic approach to explicitly incorporate prior knowledge to a deep learning-based QoS prediction model in the form of wireless communication-based penalties in the cost function of a deep neural network. System level simulations for the teleoperated driving (TOD) use case are used to evaluate our proposal. Results show that incorporating cell-load, channel and uplink inter-cell interference penalties in a deep neural network (DNN) improve uplink data rate prediction performance in scenarios with reduced amount of training data compared to an off-the-shelf DNN.
\subsubsection{Deep Reinforcement Learning for Scheduling Uplink IoT Traffic with Strict Deadlines}
abstract:This paper considers the Multiple Access problem where N Internet of Things (IoT) devices share a common wireless medium towards a central Base Station (BS). We propose a Reinforcement Learning (RL) method where the BS is the agent and the devices are part of the environment. A device is allowed to transmit only when the BS decides to schedule it. Besides the information packets, devices send additional messages like the delay or the number of discarded packets since their last transmission. This information is used to design the RL reward function and constitutes the next observation that the agent can use to schedule the next device. Leveraging RL allows us to learn the sporadic and heterogeneous traffic patterns of the IoT devices and an optimal scheduling policy that maximizes the channel throughput. We adapt the Proximal Policy Optimization (PPO) algorithm with a Recurrent Neural Network (RNN) to handle the partial observability of our problem and exploit the temporal correlations of the users' traffic. We demonstrate the performance of our model through simulations on different number of heterogeneous devices with periodic traffic and individual latency constraints. We show that our RL algorithm outperforms traditional scheduling schemes and distributed medium access algorithms.
\subsubsection{Intelligent Mobile Handover Prediction for Zero Downtime Edge Application Mobility}
abstract:Ultra-Reliable Low-Latency Communication services are intrinsically challenging to deliver, with many 5G and future services, including mobile game streaming, adding further complexity by demanding zero service downtime in high-mobility scenarios. Solving these challenges is essential and must be addressed beyond mobile gaming to realise a multitude of current and future services like Virtual Reality or holoportation in mobile scenarios. Multi-access Edge Computing brings services "closer" to user consumption with evident advantages yet at the cost of maintaining a zero downtime guarantee when user handovers (HOs) are prevalent due to the decentralisation of services towards the network edge. In this work, we design and evaluate intelligent HO prediction models between radio 5G Base Stations. The motivation for timely user HO prediction lies in being a vital presupposition for path steering and other Management and Network Orchestration control actions in contemporary programmable 5G networks to deliver a zero downtime perception during HO events. Our meticulous simulation and actual testbed evaluation results show that effective HO prediction can be achieved using a combination of Long Short-Term Memory (LSTM) or gradient boost regression with classification models, with the latter filtering out any Reference Signal Received Power (RSRP) prediction input outliers for predicting the serving cell.
\subsection{Traffic management II}
\subsubsection{Deep Reinforcement Learning for URLLC data management on top of scheduled eMBB traffic}
abstract:With the advent of 5G and the research into beyond 5G (B5G) networks, a novel and very relevant research issue is how to manage the coexistence of different types of traffic, each with very stringent but completely different requirements. We propose a Deep Reinforcement Learning (DRL) algorithm to slice the available physical layer resources between ultra-reliable low-latency communications (URLLC) and enhanced Mobile BroadBand (eMBB) traffic. Specifically, in our setting the time-frequency resource grid is fully occupied by eMBB traffic and we train the DRL agent to employ Proximal Policy Optimization (PPO), a state-of-the-art DRL algorithm, to dynamically allocate the incoming URLLC traffic by puncturing eMBB codewords. Assuming that each eMBB codeword can tolerate a certain limited amount of puncturing beyond which is in outage, we show that the policy devised by the DRL agent never violates the latency requirement of URLLC traffic and, at the same time, manages to keep the number of eMBB codewords in outage at minimum levels, when compared to other state-of-the-art schemes.
\subsubsection{Optimized Transfer Learning For Wireless Channel Selection}
abstract:A key challenge facing any channel selection technique is the dynamic nature of wireless channels. To addressthis issue, reinforcement learning techniques have widely beenused, e.g., contextual multi-armed bandit (CMAB) theory. In fact,prior works solved the problem at each individual node. However,they did not consider the cooperative learning techniques, e.g.,transfer learning. In communication systems, the advantage oftransfer learning comes with computation and communicationcosts. Therefore, the decision of transferring the knowledgebetween agents should be optimized. In this paper, we developa model to evaluate the feasibility and optimality of transferlearning for CMAB-based channel selection in communicationsystems. To this end, we introduce a utility model for evaluatingthese economical aspects. Leveraging Best Approximation Theory,we propose a new similarity concept and a transfer rule appliedin the context of channel selection. Experimental results show thatExtra Action is an efficient technique for transfer learning in achannel selection regime. More importantly, our proposed utilityand optimization model is shown to be a powerful frameworkfor deciding when transfer learning is feasible, and when it isoptimal.
\subsubsection{Multi-Agent Reinforcement Learning-Based Fairness-Aware Scheduling for Bursty Traffic}
abstract:In this work, we develop practical user scheduling algorithms for downlink bursty traffic with emphasis on user fairness. In contrast to the conventional scheduling algorithms that either equally divides the transmission time slots among users or maximizing some ratios without practical physical interpretations, we propose to use the 5%-tile user data rate (5TUDR) as the metric to evaluate user fairness. Since it is difficult to directly optimize 5TUDR, we first cast the problem into the stochastic game framework and subsequently propose a Multi-Agent Reinforcement Learning (MARL)-based algorithm to perform distributed optimization on the resource block group (RBG) allocation. Furthermore, each MARL agent is designed to take information measured by network counters from multiple network layers (e.g. Channel Quality Indicator, Buffer size) as the input states while the RBG allocation as action with a carefully designed reward function developed to maximize 5TUDR. Extensive simulation is performed to show that the proposed MARL-based scheduler can achieve fair scheduling while maintaining good average network throughput as compared to conventional schedulers.
\subsubsection{Fast and Robust Online Traffic Classification Supporting Unseen Applications}
abstract:Online traffic classification is a fundamental toolkit in network management, such as QoS and network security. The speed and generalization ability of online classification are two requirements that need to be satisfied simultaneously. However, existing methods may suffer from generalization degradation on the traffic with unseen applications which are constantly emerging in the network, due to the feature distribution drift (FDD) caused by their non-robust feature engineering approaches.Based on Deep Metric Learning which can restrict the distances between samples explicitly and clustering algorithm which can learn multiple clusters within each category, this paper presents Robot, a fast and robust online traffic classification system. At its core, Robot leverages two building blocks to classify high-speed traffic flows: 1) For fast classification, Fast model classifies traffic and detects FDD samples simultaneously based on only one packet. 2) For robust classification, once FDD samples are detected, a flow collector will be triggered to collect flows and then Robust model, a multi-center model, will further identify them based on the hybrid of packet-level and flow-level features. Our comprehensive experiments demonstrate that Robot can achieve comparative classification speed and better generalization ability on the mixed traffic datasets with seen and unseen applications (with the FDD detection accuracy of up to 85.8% and with nearly 10% improvement in classification accuracy), compared with the state-of-the-art methods.
\subsubsection{SPPNet: An Approach For Real-Time Encrypted Traffic Classification Using Deep Learning}
abstract:Data flow management has become a key network activity, strengthening the need for efficient data flow classification tools. However, pervasive encryption of communication has dramatically jeopardised the legacy tools. Recent advances in Deep Learning offer a wide variety of architectures that seem relevant for this purpose. These architectures are based on different data representations as input of their classification process. In this paper, we show the need for a deeper understanding of the features used by Deep Learning models to perform such classification. Our objective is to exploit this knowledge for defining a better data processing so that the chosen architecture will significantly improve the classification process. We will show that some information carried by packet headers need to be analyzed through a separate process. This analysis highlight that current Deep Learning approaches in the literature fail to classify encrypted flows in practice. We therefore propose a new modular Deep Learning architecture called Servername Protocol Packet Network (SPPNet) to overcome this drawback. We will show by a proof of concept that SPPNet allows to perform real-time network flow classification at packet level.
\subsection{Traffic management III}
\subsubsection{Adaptive Multi-Receptive Field Spatial-Temporal Graph Convolutional Network for Traffic Forecasting}
abstract:Mobile network traffic forecasting is one of the key functions in daily network operation. A commercial mobile network is large, heterogeneous, complex and dynamic. These intrinsic features make mobile network traffic forecasting far from being solved even with recent advanced algorithms such as graph convolutional network-based prediction approaches and various attention mechanisms, which have been proved successful in vehicle traffic forecasting. In this paper, we cast the problem as a spatial -temporal sequence prediction task. We propose a novel deep learning network architecture, Adaptive Multi-Receptive Field Spatial-Temporal Graph Convolution Networks (AMF-STGCN), to model the traffic dynamics of mobile base stations. AMF-STGCN extends STGCN by (1) applying attention mechanisms to capture various Receptive Fields of heterogeneous base stations, (2) jointly modeling the complex spatial-temporal dependencies in mobile networks, and (3) introducing an extra decoder based on a fully connected deep network to conquer the error propagation challenge with multi-step forecasting. Experiments on four real-world datasets from two different domains consistently show AMF-STGCN outperforms the state-of-the-art methods.
\subsubsection{Spider: Deep Learning-driven Sparse Mobile Traffic Measurement Collection and Reconstruction}
abstract:Data-driven mobile network management hinges on accurate traffic measurements, which routinely require expensive specialized equipment and substantial local storage capabilities, and bear high data transfer overheads. To overcome these challenges, in this paper we propose Spider, a deep-learning-driven mobile traffic measurement collection and reconstruction framework, which reduces the cost of data collection while retaining state-of-the-art accuracy in inferring mobile traffic consumption with fine geographic granularity. Spider harnesses Reinforcement Learning and tackles large action spaces to train a policy network that selectively samples a minimal number of cells where data should be collected. We further introduce a fast and accurate neural model that extracts spatiotemporal correlations from historical data to reconstruct network-wide traffic consumption based on sparse measurements. Experiments we conduct with a real-world mobile traffic dataset demonstrate that Spider samples 48% fewer cells as compared to several benchmarks considered, and yields up to 67% lower reconstruction errors than state-of-the-art interpolation methods. Moreover, our framework can adapt to previously unseen traffic patterns.
\subsubsection{Load Balancing for Communication Networks via Data-Efficient Deep Reinforcement Learning}
abstract:Within a cellular network, load balancing between different cells is of critical importance to network performance and quality of service. Most existing load balancing algorithms are manually designed and tuned rule-based methods where near-optimality is almost impossible to achieve. These rule-based methods are difficult to adapt quickly to traffic changes in real-world environments. Given the success of Reinforcement Learning (RL) algorithms in many application domains, there have been a number of efforts to tackle load balancing for communication systems using RL-based methods. To our knowledge, none of these efforts have addressed the need for data efficiency within the RL framework, which is one of the main obstacles in applying RL to wireless network load balancing. In this paper, we formulate the communication load balancing problem as a Markov Decision Process and propose a transfer deep reinforcement learning algorithm to address it. Experimental results show that the proposed method can significantly improve the system performance over other baselines and is more robust to environmental changes.
\subsubsection{On Meeting a Maximum Delay Constraint}
abstract:The recent applications are required to meet low-latencytransmission with high traffic rates and reliabilities. Fromthe latency point of view, most of the state-of-the-art techniquesconsider the average latency which does not directly apply todelay-sensitive scenarios. In this paper, we propose a novelapproach to tackle the scheduling problem by directly addressingthe max delay constraint; this is an NP-hard problem. Ourmain contributions are first, proposing the Super State Monte-Carlo Tree Search (SS-MCTS) as a version of regular MCTSmodified for large-scale probabilistic environments with lesscomputational complexity, and second, addressing the schedulingproblem with maximum delay constraint on flows. Our numericalresults demonstrate that the proposed approach significantlyimproves the packet delivery rate while meeting the maximumdelay constraint in large-scale scenarios compared to the state-of-the-art technologies.
\subsubsection{Deep Reinforcement Learning for QoS-Aware Package Caching in Serverless Edge Computing}
abstract:Recently, an integration of serverless and edge computing is getting attention as a new paradigm for the deployment of Internet-of-Things (IoT) applications. In serverless-enabled edge computing, container startup delay is one of the most critical issues because it violates some quality-of-service (QoS) requirements such as the ultra-low latency response times. Caching critical packages for the container can mitigate the startup delay associated with container instantiation. However, caches consume the memory resource that is highly limited at edge nodes. It means that the package cache must be carefully managed in serverless-enabled edge computing. This paper proposes a deep reinforcement learning (DRL)-based caching algorithm, which efficiently caches critical and popular packages with per- function response time QoS in hierarchical edge clouds. By conducting multi-agent reinforcement learning (MARL) for the caching agents of on-premise edge nodes in conjunction with a global reward that considers both cache hit and QoS violation numbers, the caching agents can be driven to cooperate with each other. The results of simulation demonstrate that the proposed DRL-based caching policy can improve QoS awareness more effectively than baselines. Compared with the LRU and LFU, the rate of violation fell by 18 and 27 percent, respectively.
\section{Signal Processing for Communications}
\subsection{Channel Estimation and Reconstruction}
\subsubsection{A Novel Scheme for Joint Estimation of Velocity, Angle-of-arrival and Range in Multipath Environment}
abstract:The estimation of velocity, angle-of-arrival, and range of a target has been researched for decades, as it finds wide applications in radar and wireless communications. In recent years, this classic problem has gained renewed interest with the advent of 5G internet of things (IoT) technologies, owing to the numerous emerging localization-related applications. This paper studies the joint estimation of velocity, AOA, and range (JEVAR) of a target in a multipath environment. To solve the JEVAR problem, we propose a novel scheme, which has the target transmit a pair of conjugate Zadoff-Chu (ZC) sequences and has the multi-antenna receiver conduct maximum likelihood (ML) estimation. The simulations verify the effectiveness of the proposed scheme by showing that its performance can approach the Cramer-Rao bound (CRB).
\subsubsection{A New Off-grid Channel Estimation Method with Sparse Bayesian Learning for OTFS Systems}
abstract:This paper proposes an off-grid channel estimation scheme for orthogonal time-frequency space (OTFS) systems adopting the sparse Bayesian learning (SBL) framework. To avoid channel spreading caused by the fractional delay andDoppler shifts and to fully exploit the channel sparsity in the delay-Doppler (DD) domain, we estimate the original DD domain channel response rather than the effective DD domain channel response as commonly adopted in the literature. TheOTFS channel estimation problem is formulated as an off-grid sparse signal recovery problem based on a virtual sampling grid defined in the DD space, where the on-grid and off-grid components of the delay and Doppler shifts are separated for estimation. In particular, the on-grid components of the delay and Doppler shifts are jointly determined by the entry indices with significant values in the recovered sparse vector. Then, the corresponding off-grid components are modeled as hyperparameters in the proposed SBL framework, which can be estimated via the expectation-maximization method. Simulation results verify that compared with the on-grid approach, our proposed off-grid OTFS channel estimation scheme enjoys a 1.5dB lower normalized mean square error.
\subsubsection{Offset Learning based Channel Estimation for IRS-Assisted Indoor Communication}
abstract:The system capacity can be remarkably enhanced with the help of intelligent reflecting surface (IRS) which has been recognized as a advanced breaking point for the beyond fifth-generation (B5G) communications. However, the accuracy of IRS channel estimation restricts the potential of IRS-assisted multiple input multiple output (MIMO) systems. Especially, for the resource-limited indoor applications which typically contains lots of parameters estimation calculation and is limited by the rare pilots, the practical applications encountered severe obstacles. Former works takes the advantages of mathematical based statistical approaches to associate the optimization issue, but the increasing of scatterers number reduces the practicality of statistical approaches in more complex situations. To obtain the accurate estimation of indoor channels with appropriate piloting overhead, an offset learning (OL)-based neural network method is proposed. The proposed estimation method can trace the channel state information (CSI) dynamically with non-priori information, which get rid of the IRS-assisted channel structure as well as indoor statistics. Moreover, a convolution neural network (CNN)-based inversion is investigated. The CNN, which owns powerful information extraction capability, is deployed to estimate the offset, it works as a offset estimation operator. Numerical results show that the proposed OL-based estimator can achieve more accurate indoor CSI with a lower complexity as compared to the benchmark schemes.
\subsubsection{Near-Field Channel Estimation for Extremely Large-scale MIMO with Hybrid Precoding}
abstract:Extremely large-scale multiple-input-multipleoutput (XL-MIMO) with hybrid precoding is a promising technique to meet the high rate requirements for future 6G. To realize efficient precoding, accurate channel estimation is essential. Existing channel estimation algorithms with low pilot overhead heavily rely on the channel sparsity in angle domain, which is achieved by the classical far-field planar wavefront assumption. However, this sparsity is not available, due to the non-negligible near-field spherical-wavefront property in XL-MIMO. Therefore, existing far-field estimation schemes will suffer from severe performance loss. To address this problem, in this paper, we study the near-field channel estimation by exploiting the polar-domain sparsity. Specifically, unlike the classical angle-domain representation that only considers the angle information of channel, we propose a polar-domain representation, which simultaneously accounts both the angle and distance information. In this way, the near-field channel also exhibits sparsity in polar domain. Exploiting this polar-domain sparsity, we propose an polar-domain simultaneous orthogonal matching pursuit (P-SOMP) algorithm to efficiently estimate the near-field channel. Finally, simulations are provided to verify the effectiveness of our schemes.
\subsubsection{Triple-Structured Compressive Sensing-based Channel Estimation for RIS-aided MU-MIMO Systems}
abstract:Reconfigurable intelligent surface (RIS) has been recognized as a potential technology for 5G beyond and attracted tremendous research attention. However, channel estimation in RIS-aided system is still a critical challenge due to the excessive amount of parameters in cascaded channel. The existing compressive sensing (CS)-based RIS estimation schemes only adopt incomplete sparsity, which induces redundant pilot consumption. In this paper, we exploit the specific triple-structured sparsity of the cascaded channel, i.e., the common column sparsity, structured row sparsity after offset compensation and the common offsets among all users. Then a novel multi-user joint estimation algorithm is proposed. Simulation results show that our approach can significantly reduce pilot overhead in both ULA and UPA scenarios.
\subsection{Localization and Identification}
\subsubsection{Learning K-Nearest Neighbour Regression for Noisy Dataset with Application in Indoor Localization}
abstract:Many indoor location estimation algorithms compare the received signal strengths from WiFi access points to a prerecorded dataset, which may be composed of carefully measured and curated data through a massive calibration campaign, and a large volume of crowd-sourced data captured by casual users. The crowd-sourced data usually contains valuable, but at the same time noisy, measurements where the noise might be in the features and/or the labels of data points. The treatment of such a dataset is challenging, as improper application of the dataset might result in inaccurate location estimation.In this paper, we propose a learning algorithm that makes the K-Nearest Neighbour (KNN) regression robust to noises both in features and labels of the training data. An intuition on why the learning algorithm should work is provided and the effectiveness of the algorithm is shown by experiments in a real environment.
\subsubsection{Dual Function Trade-off in Joint Communications and Radar: An Electromagnetic Field Analysis}
abstract:Joint communications ad radar (JCR) is a technology to leverage the same waveform for simultaneous data transmission and radar sensing, which is expected to be efficient in bandwidth and power. A theoretical framework is proposed to study JCR, based on the electromagnetic field analysis. Given a simple model of communications and radar sensing, the electromagnetic field is described in terms of forward and scattering Green functions. For radar sensing, the imaging error is derived based on the Fisher operator and the eigen-decomposition of the integral transformations with Green function kernels. For data transmission, the channel capacity is analyzed based on a similar eigen-decomposition. The trade-off between the imaging error of radar and the channel capacity of data transmission is obtained based on the theoretical analysis and numerical computations.
\subsubsection{Deep Generative Model for Simultaneous Range Error Mitigation and Environment Identification}
abstract:Received waveforms contain rich information for both range information and environment semantics. However, its full potential is hard to exploit under multipath and non-line-of-sight (NLOS) conditions. This paper proposes a deep generative model (DGM) for simultaneous range error mitigation and environment identification. In particular, we present a Bayesian model for the generative process of the received waveform composed by latent variables for both range-related features and environment semantics. The simultaneous range error mitigation and environment identification is interpreted as an inference problem based on the DGM, and implemented in a unique end-to-end learning scheme. Comprehensive experiments on a general UWB dataset demonstrate the superior performance on range error mitigation, scalability to different environments, and novel capability on simultaneous environment identification.
\subsubsection{A Multipath Estimation Method via Block Term Decomposition for Multi-carrier Systems}
abstract:Location-based services are increasingly important in recent years, including Internet-of-Things, navigation, and rescue. Network localization is an effective technology to provide high-accuracy position awareness, where one of the most challenging problems is multipath effects. In this paper, we propose a Block term (BT) decomposition-based approach to solving the multipath issue for multi-carrier systems. The method exploits both frequency diversity and temporal-domain sparsity, which applies to single-antenna systems and possesses the ability to distinguish more multipath components. Specifically, we first convert the temporal-domain observation into a low-rank tensor, and then BT decomposition is employed to separate mixed multipath components. Next, we estimate the delay from both baseband and carrier signals. When base stations are equipped with multiple antennas, our method will further extract the angle information. Simulation results show that the proposed method achieves high accuracy in multipath environments.
\subsubsection{Interferometry Based Integrated Sensing and Communications with Imperfect Synchronizations}
abstract:Interferometry is a powerful tool for estimating the incident angle of electromagnetic (EM) waves, by calculating the correlation of received signals at different antennas. Motivated by very-long-baseline interfereometry (VLBI) in radio astronomy, an interferometry based sensing scheme is proposed as integrated sensing and communications (ISAC). It reuses the communication signal from base stations (BSs), similarly to passive radars, which improves the sensing precision and spectrum efficiency. Different from the almost-perfect synchronization in VLBI, realized by atomic clocks, the synchronization in BSs of cellular communication networks (usually based on GPS signals) could have significant errors. Therefore, algorithms for compensating for synchronization errors in both time and frequency are proposed. Numerical simulations demonstrate that the proposed algorithms can substantially alleviate the synchronization errors.
\subsection{Semantic Communications}
\subsubsection{Brain-Inspired Image Quality Assessment Method based on Electroencephalography Feature Learning}
abstract:With the explosion of multimedia data, quality of experience (QoE) has become a critical metric in multimedia transmission, and therefore, QoE-oriented image quality assessment (IQA) turns more important and urgent. However, the performance of the traditional user-based assessment methods is limited by the deviation caused by human cognitive activities. In this paper, we propose a brain-inspired IQA method based on electroencephalography (EEG) feature learning, which is a psychophysiological method for studying human perception for IQA. We first establish the EEG dataset by collecting the corresponding EEG signals when subjects watch distorted facial images and then design a siamese network to extract the EEG features that can distinguish image quality levels and measure user scores. The siamese network establishes the relationship between image quality and QoE that is reflected by the EEG scores. The relationship is then embedded into a prediction network that directly obtains the EEG scores from images with different qualities. In this way, EEG scores can be predicted through end-to-end learning. Experiment results show that our proposed method can not only better evaluate the perceptual quality of facial images and reflect real human perceptions but also achieve better score prediction performance on the facial image datasets.
\subsubsection{Deformable Geometry based Semantic Reconstruction from Scene Graphs}
abstract:Structural scene graph based image generation provides a new paradigm for image-oriented semantic communications, whose goal is the semantic level rather than pixel-level reconstruction. The challenges include capturing relationships between objects and producing a reasonable geometric layout for each object accordingly. However, category information alone is not instructive enough for the generation process at the receiver side. Moreover, it is worth effort to extract the spatial dependencies among different objects in an image, therefore determine the object layouts on the whole instead of in an independent manner. In this paper, a deformable geometry framework for scene graph based image generation is proposed, in order to reconstruct images with higher semantic fidelity and visual pleasure. In particular, we introduce shape and appearance information to guide the generation process, from the scope of statistic modeling. Furthermore, we apply a spatial warping network to conduct geometric deformations on the layouts of different objects. Qualitative and quantitative experiments illustrate the superiority of our model compared to the state-of-the-art Sg2im method.
\subsubsection{Performance Optimization for Semantic Communications: An Attention-based Learning Approach}
abstract:In this paper, a semantic communication framework is proposed for wireless networks. In the proposed framework, a BS extracts the text meaning from the original text data and transmits it to each user. The semantic information is modeled by a KG and hence, the semantic information consists of a set of semantic triples. After receiving the semantic information, each user recovers the original text using a text generation model. To measure the performance of the studied semantic communication system, a metric of semantic similarity (MSS) that jointly captures the semantic accuracy and completeness of the recovered text is proposed. Due to wireless resource limitations, the BS can only transmit a partial semantic information to each user so as to satisfy the transmission delay constraint. Hence, the BS must select an appropriate resource block for each user and determine partial semantic information to be transmitted. This problem is formulated as an optimization problem whose goal is to maximize the total MSS via optimizing the resource allocation and determining the partial semantic information to be transmitted. To solve this problem, a policy gradient-based RL algorithm integrated with the attention network is proposed. The proposed algorithm can evaluate the importance of each triple in the semantic information. Simulation results demonstrate that the proposed semantic communication framework can reduce the size of data that the BS needs to transmit by up to 46% and yield a 2-fold improvement in the total MSS compared to a standard communication network.
\subsubsection{Deep Learning-Based Image Semantic Coding for Semantic Communications}
abstract:This paper studies the Generative Adversarial Networks (GANs)-based image semantic coding, whose goal is semantic exchange rather than symbol transmission. State-of-the-art visually pleasing reconstruction and semantic preserving performance is obtained in extreme low bitrate via a rate-perception-distortion optimization framework. In particular, we investigate convolutional encoder, quantizer, conditional SPADE generator, residual coding as well as perceptual losses. In contrast to previous work, i) we present a coarse-to-fine image semantic coding framework, facilitating to semantic communication system. The base layer is fully generated and preserves semantic information, while the enhancement layer restores the fine details. ii) We explore the perception and distortion performance trade-off by tuning the rate of base and enhancement layer. iii) Besides the traditional distortion metric PSNR and SSIM, we train and evaluate the proposed compression model with multiple perception metrics, in line with the purpose of semantic communications. Experimental results demonstrate the effectiveness of our model, compared to BPG, WebP, JPEG2000, JPEG and other deep learning-based image codec even if they use several times of the bitrate.
\subsubsection{A Novel Deep Learning Architecture for Wireless Image Transmission}
abstract:In this paper, the problem of neural compression based image transmission over wireless channels is studied. Since all procedures are considered over wireless links, the quality of training is affected by wireless factors such as packet errors. In the considered model, compressed data given by the neural source encoder (NSE) are fed into an error-control channel encoder and modulated as discrete symbols sent over a memoryless channel. In the receiving end, the channel decoder and the neural source decoder (NSD) forms an iterative structure to reconstruct the original image. Since all neural compressed data are transmitted over wireless channels, the training of NSD is affected by wireless channel factors such as residual bit errors given by the channel decoder. Meanwhile, during outer-loop iterations, the NSD needs to match the variant of information reliability output by the channel decoder so as to build a global optimal receiver. To this end, a refiner neural network is first attached after the NSD to adjust its output as the format of a priori information sent into the channel decoder. Then, the extrinsic information transfer (EXIT) functions of channel decoder and NSD are derived. At each iteration, the reliability of messages sent into the NSD is explicitly predicted by using the EXIT chart. By this means, the NSD can be trained in a residual bit error aware manner, and we realize a joint learning and iterative decoding framework to ensure the quality of neural image transmission over realistic wireless channels.
\subsection{Signal Processing for Emerging Systems}
\subsubsection{Transmit Beamforming Optimization for Integrated Sensing and Communication}
abstract:This paper studies the transmit beamforming in a downlink integrated sensing and communication (ISAC) system, where a base station (BS) equipped with a uniform linear array (ULA) sends combined information-bearing and dedicated radar signals to simultaneously perform downlink multiuser communication and radar target sensing. Under this setup, we minimize the radar sensing beampattern matching errors, subject to the communication users' minimum signal-to-interference-plus-noise ratio (SINR) requirements and the BS's transmit power constraints. In particular, we consider two types of communication receivers, namely Type-I and Type-II receivers, which do not have and do have the capability of cancelling the interference from the a-priori known dedicated radar signals, respectively. Under both Type-I and Type-II receivers, the non-convex beampattern matching problems are globally optimally solved via applying the semidefinite relaxation (SDR) technique. It is shown that at the optimality, dedicated radar signals are not required with Type-I receivers under some specific conditions, while dedicated radar signals are always needed to enhance the performance with Type-II receivers. Numerical results show that by exploiting the capability of canceling the interference caused by the radar signals, the case with Type-II receivers results in better sensing performance in terms of beampattern matching error than that with Type-I receivers and other conventional designs.
\subsubsection{On the Performance of Multi-Agent Detection in Mobile Delay-Sensitive Networks}
abstract:Mobile multi-agent detection has enabled comprehensive applications for intelligent sensing networks including Internet of Vehicles, Internet of Things and unmanned aerial vehicle formation. Regardless of the significant advantages of broad coverage and great flexibility, the implementation and popularization of sensing technologies are also limited by the inherent issues of position uncertainty, status update delay and sampling frequency. In this paper, we propose a detection performance evaluation scheme for distributed multi-agent detection in the presence of delayed update of agent positions. By deriving the spatial-temporal detection utility function across the network, we determine the influence mechanism of various non-ideal factors. Moreover, the universal lower bound of detection performance and the upper bounds for two scheduling policies are presented via asymptotic analysis on infinite time horizon. Numerical results further validate the superiority of delay-aware scheduling in mobile detection networks.
\subsubsection{Joint Recommendation and Pricing for Cache-Aided RAN with Malicious Users: A Game Theoretic Method}
abstract:As mobile data traffic has explosively grown during the past decades, pushing popular contents to small cells has been proposed to deal with the growing data demands. To improve the cache hit ratio, the recommender system is employed to recommend cached contents when the requests are not hit by the cache. However, how to persuade users to accept recommended files remains an open problem. A feasible solution is to give a discount on the traffic cost of the recommended contents. However, some users may maliciously request unpopular contents to get the discount, which reduces the profit of the virtual network operator (VNO). In order to punish the malicious behaviour, the VNO can reduce the recommendation probability to these users. Meanwhile, these malicious users will reduce the malicious probability to increase the revenue. To study the interaction between the profit of the VNO and the revenue of the users, we formulate a non-cooperative game to find the Nash equilibrium point (NE) of the recommendation probability of the VNO and the malicious probability of the users. Simulation results indicate that the VNO's profit and the user's revenue can be significantly increased with the proposed system compared with the system without joint recommendation and pricing schemes.
\subsubsection{User-Side RIS: Realizing Large-Scale Array at User Side}
abstract:Massive multiple-input multiple-output (MIMO) with a large-scale antenna array at base station (BS) side is one of the most essential techniques in 5G wireless communications. However, due to the forbidden hardware cost and mismatched size, it is physically limited to deploy massive MIMO at user equipment (UE) side. To break this limitation, inspired by the promising technique called reconfigurable intelligent surface (RIS), we firstly propose the concept of user-centric RIS (UC-RIS) which is a cost-efficient and energy-efficient realization for large-scale array at UE side. Different from the existing RISs that work as cell-centric RISs (CC-RISs), UC-RIS is the first usage of RIS at UE side. Then, we propose a novel architecture of UE with the aid of UC-RIS with a multi-layer structure for compact implementation. Based on the proposed multi-layer UC-RIS, we formulate the signal-to-noise ratio (SNR) maximization problem in the UC-RIS-aided communication. To tackle the challenge of solving this non-convex problem, we propose a multi-layer precoding design that can obtain the optimal parameters at transceivers and UC-RIS by iterative optimization. Finally, numerical simulation results are shown to verify the practicability and superiorities of the proposed multi-layer UC-RIS as a realization of the large-scale array at UE side.
\subsubsection{Device Activity Detection for Grant-Free Massive Access Under Frequency-Selective Rayleigh Fading}
abstract:Device activity detection and channel estimation for grant-free massive access under frequency-selective fading have unfortunately been an outstanding problem. This paper aims to address the challenge. Specifically, we present an orthogonal frequency division multiplexing (OFDM)-based grant-free massive access scheme for a wideband system with one M-antenna base station (BS), N single-antenna Internet of Things (IoT) devices, and P channel taps. We obtain two different but equivalent models for the received pilot signals under frequency-selective Rayleigh fading. Based on each model, we formulate device activity detection as a non-convex maximum likelihood estimation (MLE) problem and propose an iterative algorithm to obtain a stationary point using optimal techniques. The two proposed MLE-based methods have the identical computational complexity order NPL^2, irrespective of M, and degrade to the existing MLE-based device activity detection method when P=1. Conventional channel estimation methods can be readily applied for channel estimation of detected active devices under frequency-selective Rayleigh fading, based on one of the derived models for the received pilot signals. Numerical results show that the two proposed methods have different preferable system parameters and complement each other to offer promising device activity detection design for grant-free massive access under frequency-selective Rayleigh fading.
\subsection{Signal Processing for Intelligent Surface Systems-I}
\subsubsection{SINR Maximization for RIS-Assisted Secure Dual-Function Radar Communication Systems}
abstract:This paper investigates joint transmit beampattern and phase shifts optimization techniques for a reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output (MIMO) radar in the presence of an eavesdropping target. We propose an optimization technique to maximize the signal-to-interference plus noise ratio (SINR) at the MIMO radar. However, the problem is non-convex due to the non-concavity of the secrecy rate function. To tackle this issue, we apply the block coordinate descent (BCD) algorithm to update the transmit power and the phase shifts of the RIS alternately. Specifically, we utilize the majorization-minimization (MM) algorithm to optimize the phase shifts for a given transmit power and utilize the first-order Taylor expansion to reformulate the problem as a convex problem to optimize the transmit power for a given set of phase shifts. Two transmit beamforming vectors are designed to detect the target and convey information safely to the legitimate receiver. Simulation results show that the RIS-assisted MIMO radar can significantly enhance the SINR compared to an ordinary MIMO radar.
\subsubsection{Joint Dynamic Beamforming Design and Resource Allocation for IRS-Aided FD-WPCN}
abstract:This paper studies intelligent reflecting surface (IRS)-aided full-duplex (FD) wireless-powered communication network (WPCN), where a hybrid access point (HAP) broadcasts energy signals to multiple devices for their energy harvesting in the downlink (DL) and meanwhile receives information signals in the uplink (UL) with the help of IRS. We propose a fully dynamic IRS beamforming design, where the IRS phase-shift vectors vary with each time slot for both DL wireless energy transfer (WET) and UL wireless information transmission (WIT). We aim to maximize the system throughput by jointly optimizing the time allocation, HAP transmit power, and IRS phase shifts. Since the formulated problem is non-convex due to the highly coupled optimization variables in the objective function and non-convex unit-modulus constraints of phase shifts, we propose a novel penalty-based algorithm consisting of a two-layer iteration, i.e., an inner layer iteration and an outer layer iteration. Specifically, the inner layer solves the penalized optimization problem, while the outer layer updates the penalty coefficient over iterations to guarantee convergence. Simulation results demonstrate that integrating IRS into WPCN significantly improve the system throughput and also unveil that the IRS-aided FD-WPCN is particularly beneficial for the large number of devices scenario.
\subsubsection{Joint Active and Passive Secure Precoding in IRS-Aided MIMO Systems}
abstract:Using intelligent reflecting surfaces (IRSs), wireless propagation channels can be manipulated such that information leakage to eavesdropping terminals in a multiple-input multiple-output (MIMO) setting is significantly suppressed. This observation illustrates the potential secrecy gains of IRS-aided MIMO systems. This work develops a novel low-complexity algorithm by which these potential gains are exploited. Invoking methods from fractional programming, the algorithm iteratively designs the digital precoder at the transmitter and tunes the IRS elements, such that the weighted secrecy sum-rate is maximized. It is shown that as the algorithm iterates, the weighted secrecy sum-rate evolves in a non-decreasing way. Numerical investigations confirm the efficiency of the proposed algorithm.
\subsubsection{Simultaneously Transmitting And Reflecting (STAR) RIS Assisted NOMA Systems}
abstract:In this paper, a novel simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisted non-orthogonal multiple access (NOMA) system is proposed, where the STAR-RIS can simultaneously transmitting and reflecting the incident signals. Our objective is to maximize the achievable sum rate by jointly optimizing the decoding order, power allocation coefficients, active beamforming, transmission and reflection beamformings. However, the formulated problem is non-convex with intricately coupled variables. To tackle this challenge, a suboptimal two-layer iterative algorithm is proposed. Specifically, in the inner-layer iteration, for a given decoding order, the power allocation coefficients, active beamforming, transmission and reflection beamformings are optimized alternatively. For the outer-layer iteration, the decoding order of NOMA users in each cluster is updated with the solutions obtained from the inner-layer iteration. Simulation results are provided to demonstrate that theproposed STAR-RSI-NOMA system outperforms conventional RIS assisted systems.
\subsubsection{Analysis and Compensation of Spatial Correlation in Data Transmission Using RIS}
abstract:Reconfigurable intelligent surfaces (RIS) are currently drawing a lot of attention in the research community as a key technology for future wireless networks. In addition to boosting the signal quality, they can also be used to transmit data in the same way spatial modulation (SM) transmits data by mapping it to the activated antenna indices in MIMO systems. The problem of this transmission technique which we refer to as RIS-SM is that spatial correlation between elements of the RIS array strongly degrade bit error rate performance. In this paper, we analyze this degradation and introduce two techniques to compensate for spatial correlation. The first employs tile-specific phase shifts in the RIS elements and the second employs dynamic phase shifts that are specific to the RIS element patterns activated by the information bits to be transmitted. The analysis and the simulation results show that the proposed techniques provide substantial performance improvements and make RIS-SM transmission reliable even in the presence of very strong spatial correlation.
\subsection{Signal Processing for Intelligent Surface Systems-II}
\subsubsection{Primal Dual PPO Learning Resource Allocation in Indoor IRS-Aided Networks}
abstract:Terahertz communications is regarded as a promising technology due to its higher bandwidth and narrower beamwidths, which can improve capacity and coverage for indoor wireless users. In this paper, the intelligent reflecting surface (IRS) technique and non-orthogonal multiple access (NOMA) are utilized to compensate drawbacks of indoor transmission mismatch in the terahertz band. Then wireless resource allocation optimization in indoor terahertz IRS-aided systems is transformed into a universal optimization problem with ergodic constraints. With the aid of parametrization features of deep neural networks (DNNs), proximal policy optimization (PPO) is adopted to train the policy and corresponding actions to allocate power and bandwidths. The actor part generates continuous power allocation, and the critic part takes charge of discrete bandwidths allocation. In the design of a deep reinforcement learning (DRL) framework, primal dual ascent is proposed to realize model-free training. Simulation results demonstrate the effectiveness of the primal dual PPO learning algorithm in different settings.
\subsubsection{Max-Min Energy Efficiency for RIS-aided HetNets with Hardware Impairments and Imperfect CSI}
abstract:Beamforming design is crucial to reconfigurable intelligent surface (RIS)-aided communication networks. However, most of the the existing works assume ideal hardware and perfect channel state information (CSI), which are unrealistic assumptions in practical systems. In order to improve system robustnessand user fairness, in this paper, we firstly study the max-min energy efficiency problem for RIS-aided heterogeneous networks under non-ideal hardware and imperfect CSI. Specifically, the joint optimization of transmit beamforming vectors of femto base stations (FBSs) and the phase shift matrices of RISs is formulated as a nonconvex problem to maximize the minimum energyefficiency of femtocells subject to the constraints of the maximum transmit power of FBSs, the maximum cross-tier interference power of macrocell users, the minimum rates of femtocell users, and unit modulus of RISs. To facilitate the design, we develop an iterative block coordinate descent-based algorithm which exploits the semidefinite relaxation, the S-procedure, the successive convex approximation method, and the singular value decomposition method. Simulation results demonstrate the superiority of the proposed algorithm.
\subsubsection{Meta-learning for RIS-assisted Non-Orthogonal Multiple Access Networks}
abstract:A novel framework is proposed for integrating reconfigurable intelligent surfaces (RISs) in non-orthogonal multiple access (NOMA) downlink networks. We propose a quality-of-service (QoS)-based clustering scheme to improve user fairness and formulate a sum rate maximization problem through joint optimization of phase shift and power allocation. A model-agnostic meta-learning (MAML)-based learning algorithm is proposed to solve the joint optimization problem with fast convergence rate. Extensive simulation results demonstrate that the proposed QoS-based NOMA networks achieve significantly higher throughput compared to OMA networks. It can also be observed that significant throughput gain can be achieved with the aid of RIS.
\subsubsection{Optimal, Low-Complexity Beamforming for Discrete Phase Reconfigurable Intelligent Surfaces}
abstract:Reflective reconfigurable intelligent surface (RIS) technology is regarded as an innovative, cost- and power-effective solution that aims at influencing the wireless channel through controlled scattering. The technology can be realized by using metamaterials and/or resonant elements that scatter electromagnetic waves with a configurable phase shift. Most of the previous work on beamforming techniques for RIS assumes ideal hardware and, thus, continuous phase shifts. However, hardware constraints limit the phase shift resolution, manifested into the amount of discrete phase shifts that can be configured into each RIS element. This paper aims to offer a discrete phase shift beamforming algorithm for reflective RISs that targets minimization of the quantization error resulting from discretization of continuous phase shifts. The beamforming solution proves to be optimal under perfect channel knowledge for any discrete set of uniformly distributed phase shifts. The required complexity to find the optimal beamforming vector for our approach is found to be linear with the number of RIS elements, the minimum needed to obtain optimal results. Simulated behavior is validated by measurements, showing robustness against angle misalignments and distance variations.
\subsubsection{Beamforming and link activation methods for energy efficient RIS-aided transmissions in C-RANs}
abstract:This work studies the application of a reconfigurable intelligent surface (RIS) in a cloud radio access network (CRAN) targeting the reduction of resource usage while providing adequate capacity. We investigate if an RIS can contribute to improve the trade-off between the downlink system spectral efficiency (SE) and energy consumption of a multi-base-station (BS) multi-user single-RIS setup by means of link activation, radiated power control, and operational power mode decisions that can benefit from RIS-enhanced radio channels. For this purpose, we optimize the activations jointly with BS and RIS beamforming for maximum energy efficiency (EE) under a centralized approach and subject to SE, power, fronthaul capacity, and RIS phase-shift constraints. The associated mixed-boolean non-linear problem is solved using monotonic and semidefinite relaxation methods integrated in a Branch-Reduce-and-Bound procedure. Simulations show that the RIS helps to increase the EE of a C-RAN w.r.t. its non-RIS-aided and fully-connectedversions by 30% and 80%, respectively.
\subsection{Signal Processing for Massive MIMO}
\subsubsection{Dynamic Metasurface Antennas for Energy Efficient Uplink Massive MIMO Communications}
abstract:This paper studies the energy efficiency (EE) optimization of a single-cell multiuser massive multiple-input multiple-output (MIMO) uplink system where dynamic metasurface antennas (DMAs) with configurable weights are set at the base station. To maximize the system EE, we develop a framework for the joint optimization of users' transmit precoding and the DMAs weights, which includes Dinkelbach's transform and an alternating optimization algorithm. Since the physical structure constraint of DMAs exhibits a non-convex form, we firstly ignore this constraint and obtain the optimal unconstrained DMAs weights in a close form. Then, we configure the DMAs weights with the non-convex constraint to approximate the optimal unconstrained ones. Numerical results show a much better EE performance of the DMAs-assisted massive MIMOuplink over the conventional antenna-assisted uplink. The results also show that the EE performance of the DMAs-assisted massive MIMO uplink can be further improved by adjusting the number of microstrips.
\subsubsection{Energy-Efficient Massive MIMO for Serving Multiple Federated Learning Groups}
abstract:With its privacy preservation and communication efficiency, federated learning (FL) has emerged as a learning framework that suits beyond 5G and towards 6G systems. This work looks into a future scenario in which there are multiple groups with different learning purposes and participating in different FL processes. We give energy-efficient solutions to demonstrate that this scenario can be realistic. First, to ensure a stable operation of multiple FL processes over wireless channels, we propose to use a massive multiple-input multiple-output network to support the local and global FL training updates, and let the iterations of these FL processes be executed within the same large-scale coherence time. Then, we develop asynchronous and synchronous transmission protocols where these iterations are asynchronously and synchronously executed, respectively, using the downlink unicasting and conventional uplink transmission schemes. Zero-forcing processing is utilized for both uplink and downlink transmissions. Finally, we propose an algorithm that optimally allocates power and computation resources to save energy at both base station and user sides, while guaranteeing a given maximum execution time threshold of each FL iteration. Compared to the baseline schemes, the proposed algorithm significantly reduces the energy consumption, especially when the number of base station antennas is large.
\subsubsection{RIS-Assisted Massive MIMO with Multi-Specular Spatially Correlated Fading}
abstract:Reconfigurable intelligent surfaces (RISs) have attracted great attention as a potential beyond 5G technology. These surfaces consist of many passive elements of metamaterials whose impedance can be controllable to change the phase, amplitude, or other characteristics of wireless signals impinging on them. Channel estimation is a critical task when it comes to the control of a large RIS when having a channel with a large number of multipath components. In this paper, we propose a novel channel estimation scheme that exploits spatial correlation characteristics at both the massive multiple-input multiple-output (MIMO) base station and the planar RISs, and other statistical characteristics of multi-specular fading in a mobile environment. Moreover, a novel heuristic for phase-shift selection at the RISs is developed, inspired by signal processing methods that are effective in conventional massive MIMO. Simulation results demonstrate that the proposed uplink RIS-aided framework improves the spectral efficiency of the cell-edge mobile users substantially in comparison to a conventional single-cell massive MIMO system.
\subsubsection{Secure Transmission Using Angle Reciprocity for TDD/FDD Massive MIMO Systems}
abstract:Massive multiple-input-multiple-output (MIMO) systems provide high spatial resolution of the antenna array and the angle reciprocity of the massive MIMO channel holds in both time division duplex (TDD) and frequency division duplex (FDD) systems. In this paper, we propose a new secure transmission strategy in massive MIMO systems. Each coherent time is divided into two stages. The angle signature of the uplink (UL) channel is estimated in the first stage and the downlink (DL) angle signature can be obtained using angle reciprocity. In the second stage, the angle signature of the DL channel is adjusted by spatial rotation according to the data to be transformed. Using the independent distribution of angle signatures of different channels, security performance can be guaranteed.
\subsubsection{Topological Pilot Assignment in Cell-Free Massive MIMO Networks}
abstract:We consider the pilot assignment problem in cell-free massive multi-input multi-output (MIMO) networks, where a large number of remote radio head (RRH) antennas are randomly distributed in a wide area, and jointly serve a relatively smaller number of users (UE) coherently. By artificially imposing topological structures on the UE-RRH connectivity, we model the network by a partially-connected interference network and formulate the topological pilot assignment (TPA) problem as a sequential maximum weight induced matching problem that can be solved by either a mixed integer linear program or a simple yet efficient greedy algorithm. The efficiency of the proposed algorithms is evaluated in cell-free massive MIMO networks.
\subsection{Signal Processing for MIMO and MISO}
\subsubsection{Effect of Spatial Correlation on the Performance of Non-coherent Massive MIMO based on DMPSK}
abstract:A rigorous analysis of the effect of spatial correlation for non-coherent (NC) massive multiple-input-multiple-output (MIMO) in Rician channels is important to determine its applicability in these scenarios. We conduct such analysis for a single base station (BS) and a more general case of several BSs, all of them showing correlation among their own antennas but with uncorrelated channels with respect to each other. We first perform an analysis of the distribution of the received symbols, then propose some approximations to give a closed form expression of the symbol error probability (SER) and the signal-to-interference-and-noise-ratio (SINR) as performance measures. Finally, we extract some conclusions from this analysis to show how the combined use of several BSs can be beneficial in this scenario. Some numerical results are added to confirm the accuracy of the analysis.
\subsubsection{A Simplified Multi-Antenna Receiver for General Binary-Modulated Ambient Backscatter Signal}
abstract:Multi-antenna receivers are becoming popular in ambient backscatter communication (AmBC) because they help to improve the detection performance by improving the signal-to-interference-plus-noise ratio (SINR) of the backscatter signal. Recent solutions, however, suffer from high implementation complexity and they tend to require the receiver to have high dynamic range. In this paper, a simplified receiver is proposed, which is able to overcome the above limitations. The receivers for the deterministic-unknown ambient signal and the Gaussian signal are derived using maximum-a-posteriori (MAP) criterion. Then, their detection performances are derived for bit-error-rate analysis. It is shown that the test statistic of the receiver is independent of the ambient signal type. The components for implementation are halved compared with the optimum AmBC receiver at the cost of an SINR penalty as low as 1-dB to achieve the same detection performance for the deterministic ambient signal and 3.6-dB for the Gaussian ambient signal. The dynamic range problem can be mitigated by implementing it fully in the analog domain. By providing attractive practical advantages mentioned above, the proposed simplified receiver, therefore, enables cost-effective AmBC receiver implementations.
\subsubsection{Optimal Joint Beamforming and Jamming Design for Secure and Covert URLLC}
abstract:This paper considers the physical layer security (PLS) and covertness of the signal transmission in a multiple-input multiple-output (MISO) downlink adopting ultra-high reliability and low latency communication (URLLC), where Alice transmits confidential signals in the presence of a multi-antenna eavesdropper (Eve) and a multi-antenna watchful adversary (Willie). To strike a balance between the communication covertness and PLS, we optimize the transmit beamforming and artificial noise (AN) jointly for maximizing the achievable secrecy rate under a covertness constraint. Although the considered problem is non-convex, we propose a globally optimal joint design algorithm based on the branch-reduce-and-bound (BRB) approach to solve the considered problem. Simulation results validate its efficiency, compared with a benchmark algorithm.
\subsubsection{Decentralized Linear MMSE Equalizer Under Colored Noise for Massive MIMO Systems}
abstract:Conventional uplink equalization in massive MIMO systems relies on a centralized baseband processing architecture. However, as the number of base station antenna increases, centralized baseband processing architectures encounter two bottlenecks, i.e., the tremendous data interconnection and the high-dimensional computation. To tackle these obstacles, decentralized baseband processing was proposed for uplink equalization, but only applicable to the scenarios with unpractical white Gaussian noise assumption. This paper presents an uplink linear minimum mean-square error (L-MMSE) equalization method in the daisy chain decentralized baseband processing architecture under colored noise assumption. The optimized L-MMSE equalizer is derived by exploiting the block coordinate descent method, which shows near-optimal performance both in theoretical and simulation while significantly mitigating the bottlenecks.
\subsubsection{Low-Complexity Grouped Symbol-Level Precoding for MU-MISO Systems}
abstract:Symbol-level precoding (SLP), which can convert the harmful multi-user interference (MUI) into beneficial signals, can significantly improve symbol error rate (SER) performance in multi-user communication systems. While enjoying symbolic gain, however, the complicated non-linear symbol-by-symbol SLP design suffers high computational complexity exponential with the number of users, which is unaffordable in realistic systems. In this paper, we propose a novel low-complexity grouped SLP (G-SLP) approach and develop an efficient design algorithm for a typical max-min fairness problem. This practical G-SLP strategy divides all users into several groups. SLP is utilized for the users within each group to convert intra-group MUI into constructive interference, meanwhile the inter-group MUI is also suppressed. In particular, we first use Lagrangian and Karush-Kuhn-Tucker (KKT) conditions to simplify the G-SLP design problem and then propose an iterative majorization-minimization (MM) based algorithm to solve it. Simulation results illustrate that the proposed G-SLP strategy dramatically reduces the computational complexity without causing significant performance loss compared with the traditional SLP scheme.
\subsection{Signal Processing for mmWave & THz Communications}
\subsubsection{Adaptive Channel Estimation Based on Model-Driven Deep Learning for Wideband mmWave Systems}
abstract:Channel estimation in wideband millimeter-wave (mmWave) systems is very challenging due to the beam squint effect. To solve the problem, we propose a learnable iterative shrinkage thresholding algorithm-based channel estimator (LISTA-CE) based on deep learning. The proposed channel estimator can learn to transform the beam-frequency mmWave channel into the domain with sparse features through training data. The transform domain enables us to adopt a simple denoiser with few trainable parameters. We further enhance the adaptivity of the estimator by introducing hypernetwork to automatically generate learnable parameters for LISTA-CE online. Simulation results show that the proposed approach can significantly outperform the state-of-the-art deep learning-based algorithms with lower complexity and fewer parameters and adapt to new scenarios rapidly.
\subsubsection{Dynamic-subarray with Fixed-true-time-delay Architecture for Terahertz Wideband Hybrid Beamforming}
abstract:Hybrid beamforming for Terahertz (THz) ultra-massive MIMO (UM-MIMO) systems is a promising technology for 6G networks, which can overcome huge propagation loss and offer unprecedented data rates. With ultra-wide bandwidth in THz band, the beam squint becomes one of critical problems which could reduce the array gain and degrade the data rate.However, the traditional phase-shifters-based hybrid beamforming architectures cannot tackle this issue due to the frequency-flat property of the phase shifters. In this paper, to combat this beam squint yet with reduced power consumption, a novel dynamic-subarray with fixed-true-time-delay (DS-FTTD) architecture is proposed. Furthermore, a low-complexity successive-row-decomposition (SRD) algorithm is developed to design hybrid beamforming weights for the DS-FTTD architecture. Extensive simulation results show that, by using the SRD algorithm, the DS-FTTD architecture achieves significantly higher array gain and spectral efficiency than the existing architectures. Meanwhile, the energy efficiency is substantially improved thanks to the use of low-cost FTTDs.
\subsubsection{Full-Duplex mmWave Communications With Robust Hybrid Beamforming}
abstract:In this paper, we utilize the full-duplex mode to further improve the rate of millimeter wave communications. Correlated channel estimation errors are considered to develop a robust hybrid beamforming scheme. A zero-space projection based method is proposed to compress the self-interference. Then, analog parts of the transceiver are designed to maximize the gains of RF-to-RF effective channels. Finally, digital parts of the transceiver are solved iteratively by utilizing the equivalence between the maximization of mutual information and the minimization of weighted minimum mean squared error. The simulation results show that the proposed scheme prevails other existing designs.
\subsubsection{Learning-Aided Beam Management for mmWave High-Speed Railway Networks}
abstract:Beam alignment and tracking for millimeter-wave communication networks in highly mobile scenarios, such as high-speed railway, suffer from large overhead cost and time delay loss. To solve this problem, we propose a learning-aided beam management scheme, which divides the high-dimensional beam prediction procedure into two stages, i.e., parameter estimation and hybrid beamforming. The locations and velocities of the mobile terminals are estimated using the maximum likelihood criterion, and a data fusion module is employed to further improve the estimation accuracy and robustness. Then, the next probable beam directions and the corresponding hybrid precoders are derived based on the estimated parameter set. Numerical simulations show that, the proposed method yields significantly lower overhead cost and time delay compared to the existing beam management scheme.
\subsection{Signal Processing for Multiple Access}
\subsubsection{Power Optimization for Secure mmWave-NOMA Network with Hybrid SU-CU Grouping}
abstract:Considering the security issue in mmWave-NOMA based networks, the nonorthogonal interference can be exploited to improve the security. In this paper, we propose a novel mmWave-NOMA framework where the users are classified as secure users (SUs) and common users (CUs), to satisfy their heterogeneous security service needs with the presence of randomly located eavesdroppers. For better secrecy performance, the NOMA users with stronger channel gains are deemed as SUs, and the hybrid precoding for SUs is designed to strengthen the desired signal and reduce interference. In addition, to reduce the complexity and satisfy the diverse demands, user grouping and power allocation are jointly optimized to maximize the sum rate of CUs subject to the SUs' requirements. The non-convex problem is decomposed into two subproblems, i.e., user grouping and power optimization, and a hybrid SU-CU grouping algorithm and a successive convex approximation based algorithm are proposed to solve them, respectively. Finally, simulation results are provided to show the advantages of the proposed scheme.
\subsubsection{Hierarchical Information Accessibility in Downlink MIMO Systems}
abstract:In this paper, we consider a hierarchical information accessibility (HIA) model, which generalizes conventional physical layer security. In the considered model, multiple layers with different security priorities are assumed, where only the usersin a higher priority layer are permitted to decode the message intended to lower priority layers. To maximize the sum secrecy rate of the considered system, we formulate an optimization problem with regard to precoders. To solve the formulatedproblem, we first approximate the objective function by using the LogSumExp technique and show that finding a local optimum is equivalent to finding a leading eigenvector of the first-order optimality condition of the reformulated problem. Accordingly, we propose a novel algorithm called generalized power iteration for hierarchical information accessibility (GPI-HIA) to obtain a solution. Via simulations, we demonstrate that the proposed method significantly outperforms other baseline schemes under the considered HIA scenario.
\subsubsection{Low-Latency Driven Performance Analysis for Single-Cluster NOMA Networks}
abstract:In this paper, we study the total effective capacity (EC) of single-cluster non-orthogonal multiple access (NOMA) networks and demonstrate the performance gain of single-cluster NOMA over user-paired NOMA and orthogonal multiple access (OMA). Specifically, the exact closed-form expression and an approximate closed-form expression at high signal-to-noise ratios (SNRs), in terms of the total EC, are derived for single-cluster NOMA networks. The derivations reveal that the total EC at high SNRs only relies on the statistical delay requirement of the strongest user and is independent of the other users' delay requirements. Further, we theoretically analyze the total EC differences between single-cluster NOMA and user-paired NOMA/OMA communications and explore the impact of transmit SNR. Simulation results verify the accuracy of analytical results and further reveal that the single-cluster NOMA network achieves a greater gain in terms of the total EC, compared to the conventional OMA, when the number of users increases.
\subsubsection{Enabling Ubiquitous Non-Orthogonal Multiple Access and Pervasive Federated Learning via STAR-RIS}
abstract:This paper proposes a new, compatible, unified framework which integrates non-orthogonal multiple access (NOMA) and over-the-air federated learning (AirFL) via concurrent communication. In particular, a simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) is leveraged to adjust the signal processing order for efficient interference mitigation and omni-directional coverage extension. With the aim of investigating the impact of non-ideal wireless communication on AirFL, we provide a closed-form expression for the optimality gap over a given number of communication rounds. It reveals that the learning performance is significantly affected by the resource allocation scheme and channel noise. To minimize the derived optimality gap, a mixed-integer non-linear programming (MINLP) problem is formulated by jointly designing the transmit power at users and configuration mode at the STAR-RIS. By developing trust region-based successive convex approximation and penalty-based semidefinite relaxation, an alternating optimization approach is invoked to handle the decoupled non-convex subproblems for finding suboptimal solutions to the MINLP problem. Simulation results show that the learning performance in terms of training loss and test accuracy can be effectively improved with the aid of the well-tuned STAR-RIS.
\subsection{Signal Processing for OFDM}
\subsubsection{An Efficient Deep Neural Network Structure for RF Power Amplifier Linearization}
abstract:There has been a strong interest in using deep neural networks (DNNs) for modeling the power amplifier (PA) non-linearity and designing the digital pre-distortion (DPD) circuit. Since most DNNs only accept real-valued inputs, whereas the baseband signal has both in-phase and quadrature components, their entire structures can be highly complex. In this paper, we are interested in reducing the complexity of such structures by exploiting both envelope-dependent terms and residual learning. To acquire such an efficient DNN structure, we propose a novel methodology executed over two consecutive steps; at first, we estimate the best input combinations to a shallow NN that allows it to achieve a threshold value of NMSE. Then, we exploit these combinations as inputs to our proposed structure and increase the network depth until we obtain our system's actual requirements. Finally, our optimized structure (ODNN) performance has been evaluated using MATLAB simulation and real measurements. For a 15 MHz test signal, ODNN achieves lower NMSE than conventional DNN by 2.13 dB and 3.08 dB for Doherty PA behavioral modeling and its DPD design, respectively. For a wider 40 MHz test signal, ODNN achieves lower NMSE by 0.94 dB and 1.96 dB also. Moreover, in all previous scenarios, ODNN reduces the complexity of DNN by 26.40%.
\subsubsection{Noise Variance Estimation in 5G NR Receivers: Bias Analysis and Compensation}
abstract:This paper investigates the problem of noise variance estimation in orthogonal frequency domain multiplexing (OFDM)-based systems such as 5G New Radio (NR).Accurate estimation of the noise variance is critical for the receiver performance, especially when applied with linear minimum mean square error (LMMSE) channel estimation (CE). A commonly used method estimates the noise variance from the power of the residual signal at the CE output. In this paper, we prove that such conventional estimator is biased, resulting in underestimation of the noise variance; then, we derive a bias correction method. Simulation results show that the proposed bias correction can significantly improve LMMSE CE performance, achieving up to 1dB gain in terms of block error rate (BLER).
\subsubsection{Wideband Millimeter-Wave Massive MIMO Channel Training via Compressed Sensing}
abstract:In this work, a compressed sensing-aided wideband MIMO-OFDM channel training framework is proposed to reduce the training overhead by exploiting the temporal correlation and common channel support across the operating frequencies. A slowly-varying channel with frequency- and spatial-wideband (dual-wideband) effects is considered. To preserve the common channel support, the frequency-dependent array response matrices are constructed, enabling the formulation to recover the sparse beamspace channel from multiple observations across OFDM subcarriers, so-called multiple measurement vectors (MMV). With the temporal correlation, a channel training algorithm (MMV-LS-CS), motivated by the slowly-varying channel support, is proposed to estimate the multipath channel parameters: MMV least squares (MMV-LS) is first used to estimate the channel on the prior support, followed by MMV compressed sensing (MMV-CS) on the residual to estimate the time-varying multipath components of the channel. To deal with the spatial-wideband effect, a channel refining algorithm is proposed to estimate the gains and time delays of the dominant channel paths jointly on pilot subcarriers. Numerical results show that MMV-LS-CS achieves more accurate and robust channel estimation than the state-of-the-art approach on dual-wideband MIMO-OFDM systems in the slowly-varying environment.
\subsubsection{On Preamble-based FBMC/OQAM Highly Frequency Selective Channel Estimation Without Guard Symbols}
abstract:Intrinsic interference is known to present a challenge for signal processing tasks in systems employing offset quadrature amplitude modulation-based filter bank multicarrier (FBMC/OQAM) transmission. In preamble-based channel estimation, in particular, this otherwise called self interference will corrupt the pilot symbols, thus preventing accurate estimation, unless special means, such as inserting sufficiently many null guard FBMC symbols between the training and the information part, are employed, with a consequent loss in spectral efficiency. Iterative procedures that alternatingly estimate the channel and the interfering data symbols have been proposed to tackle this problem, and they are almost exclusively intended for channels of relatively (to the FBMC symbol duration) low delay spread. In this paper, the problem of preamble-based channel estimation without guard symbols is re-visited, with no simplifying assumption on the relative frequency selectivity of the channel. An optimal (in the mean squared error (MSE) sense) preamble in the presence of data interference is designed, assuming the availability of the second-order statistics of the channel and the signal-to-noise ratio. The effect of adopting this interference-aware preamble in an iterative estimation/detection procedure is assessed via simulations with realistic channels. A significant gain in the detection performance is demonstrated compared to the use of the preamble that is only optimal in the absence of interference.
\section{Wireless Communications}
\subsection{Beamforming 1}
\subsubsection{Optimal Discrete Beamforming for Intelligent Reflecting Surface}
abstract:This work pursues an optimal strategy of designing passive beamformer for intelligent reflecting surface (IRS) in order to maximize the overall channel strength. In particular, the choice of phase shift for each reflective element is restricted to K>=2 discrete values. Although the resulting discrete beamforming problem is believed to be NP-hard in some prior works, the paper shows that the global optimum of the binary case with K=2 can be achieved in quadratic time. For a general K-ary beamforming problem with K>2, the stateof-the-art polynomial time algorithm is to greedily project the relaxed solution to the closest point in the constraint set. However, as shown in the paper, the performance of this greedy method cannot be guaranteed. In contrast, we propose a linear time algorithm that is capable of reaching a near-optimal solution with an approximation ratio of (1+cos(pi/K))=2, i.e., its performance is at least 75% of the global optimum for K 3. Furthermore, inspired by the RFocus method in [1], we develop a statistic implementation of the above approximation algorithm in the absence of channel state information (CSI).
\subsubsection{Joint Beamforming Designs for Intelligent Omni Surface Assisted Wireless Communication Systems}
abstract:Intelligent reflecting surface (IRS) has been widely considered as one of key enabling techniques for the future wireless networks owing to its ability of constructing favorable propagation environment by controlling the phase shifts of reflected electromagnetic (EM) waves that impinge on the surface. While an IRS only focuses on the reflective implementation, recently emerged innovative concept of intelligent omni-surface (IOS) can provide the dual-functionality of manipulating signal reflection and transmission. Thus, an IOS can provide service coverage for both sides of it. In this paper, we consider an IOS-assisted multi-user multi-input single-output (MU-MISO) system, in which the IOS utilizes its reflective and transmissive properties to enhance the MU-MISO transmission. Our goal is to jointly optimize the transmit beamformers at base station (BS), the reflective and transmissive phase-shifts of IOS, and the reflection-to-transmission ratio of IOS to minimize the total transmit power for the MU-MISO system, subject to the signal-to-interference-plus-noise ratio (SINR) requirements of individual users. An efficient iterative algorithm is presented to solve this non-convex optimization problem. Simulation results verify the advantage of the IOS-assisted wireless communication system and the efficiency of the associate beamforming design algorithm.
\subsubsection{Machine Learning Assisted Phase-less Millimeter-Wave Beam Alignment in Multipath Channels}
abstract:Communication systems at millimeter-wave (mmW) and sub-terahertz frequencies are of increasing interest for future high-data rate networks. One critical challenge faced by phased array systems at these high frequencies is the efficiency of the initial beam alignment, typically using only phase-less power measurements due to high frequency oscillator phase noise. Traditional methods for beam alignment require exhaustive sweeps of all possible beam directions, thus scale communications overhead linearly with antenna array size. For better scaling with the large arrays required at high mmW bands, compressive sensing methods have been proposed as their overhead scales logarithmically with the array size. However, algorithms utilizing machine learning have shown more efficient and more accurate alignment when using real hardware due to array impairments. Additionally, few existing phase-less beam alignment algorithms have been tested over varied secondary path strength in multipath channels. In this work, we introduce a novel, machine learning based algorithm for beam alignment in multipath environments using only phase-less received power measurements. We consider the impacts of phased array sounding beam design and machine learning architectures on beam alignment performance and validate our findings experimentally using 60 GHz radios with 36-element phased arrays. Using experimental data in multipath channels, our proposed algorithm demonstrates an 88% reduction in beam alignment overhead compared to an exhaustive search and at least a 62% reduction in overhead compared to existing compressive methods.
\subsubsection{A Learning Based Branch-and-Bound Algorithm for Single-Group Multicast Beamforming}
abstract:Consider the single-group multicast beamforming problem in wireless systems, where a multi-antenna transmitter has a common message intended to a group of users. The problem is known be non-convex and NP-hard except in very few special cases. Traditional optimization-based algorithms either obtain sub-optimal solutions or take exponential complexity to reach the optimum. In this paper, we propose a learning based branch-and-bound (LBB) algorithm to find a near-optimal solution of the quality-of-service (QoS)-constrained multicast beamforming problem with affordable computational complexity. We first relax the non-convex signal-to-noise ratio (SNR) constraints for all users to their convex envelopes based on argument cut so that the traditional branch-and-bound (BB) algorithm can be applied to find the optimal solution (but with exponential complexity). Then, we cast the BB procedure as a sequential decision problem and learn the optimal pruning policy via supervised learning. To tackle the issue of the unbalanced training set generated by BB, we propose an ensemble-supervised learning (ESL) method to train multiple neural networks for policy learning and combine them to get the final solution. We also show that the computational complexity of LBB is determined by the depth of the binary tree in the search procedure and give its expression in the worst case. Numerical results show that LBB runs significantly faster than BB while achieving nearly the same optimal performance.
\subsubsection{Reward-Maximization-Based Passive Beamforming for Multi-RIS-Aided Multi-User MISO Systems}
abstract:Recently, reconfigurable intelligent surfaces (RISs) have emerged as a potential technique for future 6G communications. Considering the practical hardware constraints of RISs, e.g., the availability of only quantized phase shifts for reflecting elements, we investigate codebook-based passive beamforming, and then develop a two-phase precoding algorithm for multi-RIS-aided multi-user multiple-input single-output (MU-MISO) systems, where the required pilot overhead is much less than that for training the perfect channel state information (CSI). Compared with the maximum ratio transmission (MRT), we propose a more efficient codebook-based passive beamforming scheme based on the sum reward maximization. To verify the feasibility of the proposed reward-maximization-based passive beamforming, we compare the average sum rates achieved by the proposed method, the MRT method, as well as the exhaustive method. Further, we design a feasible set with a few codewords to reduce the computational complexity of the exhaustive method. Moreover, the obtained results based on different codebooks are given to illustrate the generality of the proposed scheme.
\subsection{Beamforming 2}
\subsubsection{MmWave MIMO Hybrid Precoding Design Using Phase Shifters and Switches}
abstract:To reduce the number of phase shifters for analog precoding in millimeter wave massive multiple-input multiple-output communications, we investigate the hybrid use of expensive phase shifters and low-cost switches. Different from the existing fixed phase shifter (FPS) architecture where the phases are fixed and independent of the channel state information, we consider variable phase shifter (VPS) whose phases are variable and subject to the hardware constraint. Based on the VPS architecture, a hybrid precoding design (HPD) scheme named VPS-HPD is proposed to optimize the phases according to the channel state information. Specifically, we alternately optimize the analog precoder and the digital precoder, where the former is converted into several subproblems and each subproblem further includes the alternating optimization of the phase matrix and switch matrix. Simulation results show that the spectral efficiency of the VPS-HPD scheme is very close to that of the fully digital precoding, higher than that of the existing MO-AltMin scheme for the fully-connected architecture with much fewer phase shifters, and substantially higher than that of the existing FPS-AltMin scheme for the FPS architecture with the same number of phase shifters.
\subsubsection{Hybrid Analog-Digital Beamforming in Cooperative mmWave MIMO Systems}
abstract:This paper investigates hybrid beamforming design in cooperative millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems. We focus on the sum-rate maximization problem and aim to design the hybrid beamformer to maximize the weighted achievable sum-rate subject to the constraint of maximum transmit power for all base station (BSs) and the constant modulus of phase shifters (PSs). Due to the non-convexity of the weighted sum-of-logarithmic function, we firstly transform the objective function into an equivalent form with the aid of fractional programming (FP) theory. Then, we propose an iterative hybrid beamforming algorithm, in which manifold optimization is first employed to solve analog beamformer and then a closed-form solution of digital beamformer is derived by Lagrange multiplier method. Numerical results demonstrate the remarkable advantages of the proposed hybrid beamforming algorithm compared with other state-of-the-art approaches.
\subsubsection{Low-complexity Multicast Beamforming for Multi-stream Multi-group Communications}
abstract:In this paper, assuming multi-antenna transmitter and receivers, we consider multicast beamformer design for the weighted max-min-fairness (WMMF) problem in a multi-stream multi-group communication setup. Unlike the single-stream scenario, the WMMF objective in this setup is not equivalent to maximizing the minimum weighted SINR due to the summation over the rates of multiple streams. Therefore, the non-convex problem at hand is first approximated with a convex one and then solved using Karush-Kuhn-Tucker (KKT) conditions. Then, a practically appealing closed-form solution is derived, as a function of dual variables, for both transmit and receive beamformers. Finally, we use an iterative solution based on the sub-gradient method to solve for the mutually coupled and interdependent dual variables. The proposed solution does not rely on generic solvers and does not require any bisection loop for finding the achievable rate of various streams. As a result, it significantly outperforms the state-of-art in terms of computational cost and convergence speed.
\subsubsection{Millimeter Wave Analog Beamforming Codebooks Robust to Self-Interference}
abstract:This paper develops a novel methodology for designing analog beamforming codebooks for full-duplex millimeter wave (mmWave) transceivers, the first such codebooks to the best of our knowledge. Our design reduces the self-interference coupled by transmit-receive beam pairs and simultaneously delivers high beamforming gain over desired coverage regions, allowing mmWave full-duplex systems to support beam alignment while minimizing self-interference. To do so, our methodology allows some variability in beamforming gain to strategically shape beams that reject self-interference while still having substantial gain. We present an algorithm for approximately solving our codebook design problem while accounting for the non-convexity posed by digitally-controlled phase shifters and attenuators. Numerical results suggest that our design can outperform or nearly match existing codebooks in sum spectral efficiency across a wide range of self-interference power levels. Results show that our design offers an extra 20-50 dB of robustness to self-interference, depending on hardware constraints.
\subsubsection{Analog Eigen-Beamforming for mmWave Systems: Performance under practical constraints}
abstract:Many millimeter wave systems embed phased array antennas to overcome the large path loss that is experienced at such high frequencies. Digital Fourier Transform based beamforming (DFT-BF) is often implemented as a low-complexity and very limited feedback technique. Although DFT-BF is optimal in pure Line-of-Sight scenarios, it is substantially outperformed in richer scattering environments by more advanced approaches such as Eigenvector-based beamforming (Eigen-BF). The latter is harder to implement in practical scenarios due to its computational complexity and amount of feedback. In this paper, we propose several solutions to alleviate those drawbacks and analyze various performance-complexity trade-offs in the perspective of a practical usage of Eigen-BF solutions.
\subsection{Cell-free communications}
\subsubsection{Performance Analysis of IRS-Assisted Cell-Free Communication}
abstract:In this paper, the feasibility of adopting an intelligent reflective surface (IRS) in a cell-free wireless communication system is studied. The received signal-to-noise ratio (SNR) for this IRS-enabled cell-free set-up is optimized by adjusting phase-shifts of the passive reflective elements. Then, tight approximations for the probability density function and the cumulative distribution function for this optimal SNR are derived for Rayleigh fading. To investigate the performance of this system model, tight bounds/approximations for the achievable rate and outage probability are derived in closed form. The impact of discrete phase-shifts is modeled, and the corresponding detrimental effects are investigated by deriving an upper bounds for the achievable rate in the presence of phase-shift quantization errors. Monte-Carlo simulations are used to validate our statistical characterization of the optimal SNR, and the corresponding analysis is used to investigate the performance gains of the proposed system model. We reveal that the IRS-assisted communications can boost the performance of cell-free wireless architectures.
\subsubsection{Towards Reliable Communications in Intelligent Reflecting Surface-Aided Cell-Free MIMO Systems}
abstract:Intelligent reflecting surface (IRS) and cell-free multiple-input multiple-output (CF-MIMO) systems are two promising multi-antenna technologies for the fifth generation and beyond (B5G) wireless communication systems. In this paper, we formulate a joint phase shift control and beamforming optimization problem to maximize the aggregate throughput subject to the reliability constraint of the users in the IRS-aided CF-MIMO systems. We propose an alternating optimization (AO)-based algorithm, in which the joint problem is decomposed into a phase shift control subproblem and a beamforming subproblem. For the phase shift control subproblem, we propose a complex gradient descent (CGD)-based algorithm, which tackles the unit-modulus constraint and guarantees the aggregate throughput to be monotonic increasing in each iteration. We then propose a difference of convex programming (DCP)-based algorithm for beamforming optimization. Simulation results show that the proposed AO-based algorithm achieves an aggregate throughput that is 53.8% and 25.1% higher than the cellular MIMO system with zero-forcing beamformer and the IRS-aided CF-MIMO system with random phase shift control, respectively. Moreover, the reliability requirements of the users are satisfied with the proposed AO-based algorithm. Our results also demonstrate that the IRS-aided CF-MIMO systems improve the minimum throughput of the users and reduce the standard deviation of the throughput distribution.
\subsubsection{RIS and Cell-Free Massive MIMO: A Marriage For Harsh Propagation Environments}
abstract:This paper considers Cell-Free Massive MIMO systems with the assistance of an RIS for enhancing the system performance. Distributed maximum-ratio combining (MRC) is considered at the access points (APs). We introduce an aggregated channel estimation approach that provides sufficient information for data processing. The considered system is studied by using asymptotic analysis which lets the number of APs and/or the number of RIS elements grow large. A lower bound for the channel capacity is obtained for a finite number of APs and scattering elements of the RIS, and closed-form expressions for the uplink ergodic net throughput is formulated. In addition, a simple control scheme for controlling the configuration of the RIS scattering elements is proposed. Numerical results verify the effectiveness of our system design and the benefits of using RISs in Cell-Free Massive MIMO systems are confirmed.
\subsubsection{Deep Learning-Based Power Control for Uplink Cell-Free Massive MIMO Systems}
abstract:In this paper, a general framework for deep learning-based power control methods for max-min, max-product and max-sum-rate optimization in uplink cell-free massive multiple-input multiple-output (CF mMIMO) systems is proposed. Instead of using supervised learning, the proposed method relies on unsupervised learning, in which optimal power allocations are not required to be known, and thus has low training complexity. More specifically, a deep neural network (DNN) is trained to learn the map between fading coefficients and power coefficients within short time and with low computational complexity. It is interesting to note that the spectral efficiency of CF mMIMO systems with the proposed method outperforms previous optimization methods for max-min optimization and fits well for both max-sum-rate and max-product optimizations.
\subsection{Channel estimation}
\subsubsection{Block-Sparse Channel Estimation in Massive MIMO Systems by Expectation Propagation}
abstract:We consider downlink channel estimation in massive multiple input multiple output (MIMO) systems using a Bayesian compressive sensing (BCS) approach. BCS exploits the sparse structure of the channel in the angular domain in order to reduce the pilot overhead. Due to limited local scattering, the massive MIMO channel has a block-sparse representation in the angular domain. Thus, we use a conditionally independent and identically distributed spike-and-slab prior to model the sparse vector coefficients representing the channel and a Markov prior to model its support. An expectation propagation (EP) algorithm is developed to approximate the intractable joint posterior distribution on the sparse vector and its support with a distribution from an exponential family. The unknown model parameters which are required by EP, are estimated using the expectation maximization (EM) algorithm.The proposed combination of EM and EP algorithms is reminiscent ofvariational EM and is referred to as EM-EP. The approximated distribution is then used for estimating the massive MIMO channel. Simulation results show that our proposed EM-EP algorithm outperforms several recently-proposed algorithms in channel estimation.
\subsubsection{Beam Alignment in mmWave User-Centric Cell-Free Massive MIMO Systems}
abstract:The problem of beam alignment (BA) in a cell-free massive multiple-input multiple-output (CF-mMIMO) system operating at millimeter wave (mmWaves) carrier frequencies is considered in this paper. Two estimation algorithms are proposed, in association with a protocol that permits simultaneous estimation, on a shared set of frequencies, for each user equipment (UE), of the direction of arrival and departure of the radio waves associated to the strongest propagation paths from each of the surrounding access points (APs), so that UE-AP association can take place.The proposed procedure relies on the existence of a reliable control channel at sub-6 GHz frequency, so as to enable exchange of estimated values between the UEs and the network, and assumes that APs can be identifies based on the prior knowledge of the orthogonal channels and transmit beamforming codebook. A strategy for assigning codebook entries to the several APs is also proposed, with the aim of minimizing the mutual interference between APs that are assigned the same entry. Numerical results show the effectiveness of the proposed detection strategy, thus enabling one shot fast BA for CF-mMIMO systems.
\subsubsection{Joint Channel Estimation and Equalization for OTFS Based on EP}
abstract:Orthogonal time-frequency space signaling (OTFS) is a novel modulation scheme that can exploit full time-frequency (TF) diversity when coupled with a proper receiver. Toward this end, this paper presents a joint channel estimation and equalization approach for OTFS based on the expectation propagation (JCEE-EP). First, We apply discrete prolate spheroidal basis expansion model (DPS-BEM) to describe the time-varying (TV) channel and obtain a Gaussian prior of BEM coefficients via LS estimator. Based on the delay-Doppler (DD) domain to time domain transformation, we then decompose the joint posterior probability into a couple of factors and approximate each one by Gaussian distributions according to the EP principle. As the moment-matching (MM) step in channel factor update is computationally intractable, we resort to the first-order conditional EP (CEP) method for an approximate solution. Finally, the bit error rate (BER) simulation results demonstrate the superiority of our proposed JCEE-EP over the existing approaches.
\subsubsection{Compressed Sensing Channel Estimation for OTFS Modulation in Non-Integer Delay-Doppler Domain}
abstract:This paper introduces a Compressed Sensing (CS) estimation scheme for Orthogonal Time Frequency Space (OTFS) channels with sparse multipath. The OTFS waveform represents signals in a two dimensional Delay-Doppler (DD) orthonormal basis. The proposed model does not require the assumption that the delays are integer multiples of the sampling period. The analysis shows that non-integer delay and Doppler shifts in the channel cannot be accurately modelled by integer approximations. An Orthogonal Matching Pursuit with Binary-division Refinement (OMPBR) estimation algorithm is proposed. The proposed estimator finds the best channel approximation over a continuous DD dictionary without integer approximations. This results in a significant reduction of the estimation normalized mean squared error with reasonable computational complexity.
\subsubsection{Adaptive Beam Alignment in Mm-Wave Networks: A Deep Variational Autoencoder Architecture}
abstract:This paper proposes a dual timescale learning and adaptation framework to learn a probabilistic model of beam dynamics and then exploit this model to design adaptive beam-training with low overhead: on a long timescale, a recurrent deep variational autoencoder (R-VAE) uses noisy beam-training observations to learn a probabilistic model of beam dynamics; on a short timescale, an adaptive beam-training procedure is formulated as a partially observable Markov decision process and optimized using point-based value iteration by leveraging beam-training feedback and probabilistic predictions of the strongest beam pair provided by the R-VAE. In turn, beam-training observations are used to refine the R-VAE via stochastic gradient ascent in a continuous process of learning and adaptation. It is shown that the proposed deep R-VAE mobility learning framework learns accurate beam dynamics and, as learning progresses, the training overhead decreases and the spectral efficiency increases. Moreover, the proposed dual timescale approach achieves near-optimal spectral efficiency, with a gain of 85% over a policy that scans exhaustively over the dominant beam pairs, and of 18% over a state-of-the-art POMDP policy.
\subsection{Channel modelling}
\subsubsection{Performance Analysis of Ultra-Dense Millimeter Wave Cloud-RAN under Blockage and Interference}
abstract:In this paper, we consider a downlink Cloud Radio Access Network (Cloud-RAN) transmission, served by multiple beamformed remote radio head (RRHs) coordinated by base band units (BBUs), toward a typical mobile user. Assuming ultra dense5G and beyond network operating in millimeter wave (mm-Waves) bands, we modeled channels as a concatenation of Nakagami-m and binary blockage. In addition, it is assumed there are multiple interfering nodes nearby. We evaluate the system performance in term of outage probability, after calculating channel distributions. Moreover, the impact of random blockage and interference on the performance of Nakagami-blockage channels with multiple RRHs are investigated by simulations.
\subsubsection{A Non-Stationary Channel Model with Correlated NLoS/LoS States for ELAA-mMIMO}
abstract:In this paper, a novel spatially non-stationary channel model is proposed for link-level computer simulations of massive multiple-input multiple-output (mMIMO) with extremely large aperture array (ELAA). The proposed channel model allows a mix of non-line-of-sight (NLoS) and LoS links between a user and service antennas. The NLoS/LoS state of each link is characterized by a binary random variable, which obeys a correlated Bernoulli distribution. The correlation is described in the form of an exponentially decaying window. In addition, the proposed model incorporates shadowing effects which are non-identical for NLoS and LoS states. It is demonstrated, through computer emulation, that the proposed model can capture almost all spatially non-stationary fading behaviors of the ELAA-mMIMO channel. Moreover, it has a low implementational complexity. With the proposed channel model, Monte-Carlo simulationsare carried out to evaluate the channel capacity of ELAA-mMIMO. It is shown that the ELAA-mMIMO channel capacity has considerably different stochastic characteristics from the conventional mMIMO due to the presence of channel spatial non-stationarity.
\subsubsection{Correlation Discovery and Channel Prediction in Mobile Networks: A Revisiting to Gaussian Process}
abstract:With accurate knowledge of future Channel State Information (CSI), it becomes possible to better comprehend the radio propagating environment and manipulate the wireless resources in a proactive manner, so as to provide solid support to smart and high quality wireless transmission. However, in mobile environment, the evolving correlation patterns in CSI series challenge the existing data-driven algorithms to adaptively learn and predict its behavior. In this article, an adaptive learning algorithm is proposed based on Gaussian Process (GP), to discover and utilize the spatial correlation within a channel and across channels, and produce accurate CSI prediction. Specifically, 1). To track the evolving correlation of a channel, we tailor Spectrum Mixture (SM) kernel to not only approximate the optimal kernel adapting to the current CSI, but also capture the combined effect of path loss and User Equipment (UE) motion. 2). The correlation across channels is encoded into the GP-based learning framework through Linear Model of Co-regionalization (LMC). Finally, we verify the performance improvements through simulation.
\subsubsection{Sub-Terahertz Spatial Statistical MIMO Channel Model for Urban Microcells at 142 GHz}
abstract:Sixth generation cellular systems are expected to extend the operational range to sub-Terahertz (THz) frequencies between 100 and 300 GHz due to the broad unexploited spectrum therein. A proper channel model is needed to accurately describe spatial and temporal channel characteristics and faithfully create channel impulse responses at sub-THz frequencies. This paper studies the channel spatial statistics such as the number of clusters and cluster power distribution based on recent radio propagation measurements conducted at 142 GHz in an urban microcell (UMi) scenario. A detailed spatial statistical multiple input multiple output (MIMO) channel generation procedure is introduced based on the derived empirical channel statistics. We provide a spectral efficiency analysis at sub-THz frequencies using both beamforming and spatial multiplexing and find that beamforming is always preferred in the LOS scenario where a superior boresight path exists, and at most two data streams can be well supported due to the extreme channel sparsity at sub-THz frequencies.
\subsection{Intelligent Reflective Surfaces 1}
\subsubsection{On the Performance of IRS-Assisted Relay Systems}
abstract:This paper investigates the performance of intelligence reflective surface (IRS)-assisted relay systems. To this end, we quantify the optimal signal-to-noise ratio (SNR) attained by smartly controlling the phase-shifts of impinging electromagnetic waves upon an IRS. Thereby, a tightly approximated cumulative distribution function is derived to probabilistically characterize this optimal SNR. Then, we derive tight approximations/bounds for the achievable rate, outage probability, and average symbol error rate. Monte-Carlo simulations are used to validate our performance analysis. We present numerical results to reveal that the IRS-assisted relay system can boost the performance of end-to-end wireless transmissions.
\subsubsection{A Bayesian Tensor Approach to Enable RIS for 6G Massive Unsourced Random Access}
abstract:This paper investigates the problem of joint massive devices separation and channel estimation for a reconfigurable intelligent surface (RIS)-aided unsourced random access (URA) scheme in the sixth-generation (6G) wireless networks. In particular, by associating the data sequences to a rank-one tensor and exploiting the angular sparsity of the channel, the detection problem is cast as a high-order coupled tensor decomposition problem. However, the coupling among multiple devices to RIS (device-RIS) channels together with their sparse structure make the the problem intractable. By devising novel priors to incorporate problem structures, we design a novel probabilistic model to capture both the element-wise sparsity from the angular channel model and the low rank property due to the sporadic nature of URA. Based on the this probabilistic model, we develop a coupled tensor-based automatic detection (CTAD) algorithm under the framework of variational inference with fast convergence and low computational complexity. Moreover, the proposed algorithm can automatically learn the number of active devices and thus effectively avoid the noise overfitting. Extensive simulation results confirm the effectiveness and improvements of the proposed URA algorithm in large-scale RIS regime.
\subsubsection{Low-complexity Robust Optimization for an IRS-assisted Multi-Cell Network}
abstract:The impacts of channel estimation errors, inter-cell interference, phase adjustment cost, and computation cost on an intelligent reflecting surface (IRS)-assisted system are severe in practice but have been ignored for simplicity in most existing works. In this paper, we consider a multi-antenna base station (BS) serving a single-antenna user with the help of a multi-element IRS in the presence of channel estimation errors and inter-cell interference. We also take into account phase adjustment and computation costs. First, we formulate the robust optimization of the BS's instantaneous channel state information (CSI)-adaptive beamforming and IRS's quasi-static phase shifts for the ergodic rate maximization as a very challenging two-timescale stochastic non-convex problem. Then, we obtain a closed-form beamformer for any given phase shifts and a more tractable single-timescale stochastic non-convex problem only for phase shifts. Next, we propose a low-complexity stochastic algorithm to obtain quasi-static phase shifts which correspond to a KKT point of the single-timescale stochastic problem. It is worth noting that the proposed method offers a closed-form robust beamforming design that can promptly adapt to instantaneous changes of CSI and a robust quasi-static phase shift design of low computation and phase adjustment costs in the presence of channel estimation errors and inter-cell interference. Finally, numerical results demonstrate the notable gains of the proposed robust joint design over existing designs.
\subsubsection{Deployment Optimization for Meta-material Based Internet of Things}
abstract:In this paper, we propose a Meta-IoT system to achieve ubiquitous deployment and pervasive sensing for future Internet of Things (IoT). In such a system, sensors are composed of dedicated meta-materials whose frequency response of wireless signal is sensitive to environmental conditions. Therefore, we can obtain sensing results from reflected signals through Meta-IoT devices and the energy supplies for IoT devices can be removed. Nevertheless, in the Meta-IoT system, because the positions of the Meta-IoT devices decide the interference among the reflected signals, which may make the sensing results of different positions hard to be distinguished and the estimation function should integrate the results to reconstruct 3D distribution. It is a challenge to optimize the positions of the Meta-IoT devices to ensure sensing accuracy of 3D environmental conditions. To handle this challenge, we establish a mathematical model of Meta-IoT devices' sensing and transmission to calculate the interference between Meta-IoT devices. Then, an algorithm is proposed to jointly minimize the interference and reconstruction error by optimizing the Meta-IoT devices' position and the estimation function. The simulation results verify that the proposed system can obtain a 3D environmental conditions' distribution with high accuracy.
\subsubsection{RIS-Assisted Secure Transmission Exploiting Statistical CSI of Eavesdropper}
abstract:We investigate the reconfigurable intelligent surface (RIS) assisted downlink secure transmission where only the statistical channel of eavesdropper is available. To handle the stochastic ergodic secrecy rate (ESR) maximization problem, a deterministic lower bound of ESR (LESR) is derived. We aim to maximize the LESR by jointly designing the transmit beamforming at the access point (AP) and reflect beamforming by the phase shifts at the RIS. To solve the non-convex LESR maximization problem, we develop a novel penalty dual convex approximation (PDCA) algorithm based on the penalty dual decomposition (PDD) optimization framework, where the exacting constraints are penalized and dualized into the objective function as augmented Lagrangian components. The proposed PDCA algorithm performs double-loop iterations, i.e., the inner loop resorts to the block successive convex approximation (BSCA) to update the optimization variables; while the outer loop adjusts the Lagrange multipliers and penalty parameter of the augmented Lagrangian cost function. The convergence to a Karush-Kuhn-Tucker (KKT) solution is theoretically guaranteed with low computational complexity. Simulation results show that the proposed PDCA scheme is better than the commonly adopted alternating optimization (AO) scheme with the knowledge of statistical channel of eavesdropper.
\subsubsection{MmWave MIMO Communication with Semi-Passive RIS: A Low-Complexity Channel Estimation Scheme}
abstract:Reconfigurable intelligent surfaces (RISs) have recently received widespread attention in the field of wireless communication. An RIS can be controlled to reflect incident waves from the transmitter towards the receiver; a feature that is believed to fundamentally contribute to beyond 5G wireless technology.The typical RIS consists of entirely passive elements, which requires the high-dimensional channel estimation to be done elsewhere. Therefore, in this paper, we present a semi-passive large-scale RIS architecture equipped with only a small fraction of simplified receiver units with only 1-bit quantization. Based on this architecture, we first propose an alternating direction method of multipliers (ADMM)-based approach to recover the training signals at the passive RIS elements, We then obtain the global channel by combining a channel sparsification step with the generalized approximate message passing (GAMP) algorithm. Our proposed scheme exploits both the sparsity and low-rankness properties of the channel in the joint spatial-frequency domain of a wideband mmWave multiple-input-multiple-output (MIMO) communication system. Simulation results show that the proposed algorithm can significantly reduce the pilot signaling needed for accurate channel estimation and outperform previous methods, even with fewer receiver units.
\subsection{Intelligent Reflective Surfaces 2}
\subsubsection{Simultaneously Transmitting And Reflecting RIS Aided NOMA With Randomly Deployed Users}
abstract:To achieve 360 coverage, we investigate a simultaneous transmitting and reflecting reconfigurable intelligent surfaces (STAR-RIS) aided downlink non-orthogonal multiple access (NOMA) network with randomly deployed users. For different scenarios, we first derive two STAR-RIS-aided channel models, namely the central limit model and the curve fitting model. More specifically, the central limit model fits the scenarios with numerous RIS elements while the curve fitting model can be extended to multi-cell scenarios. The analytical results reveal that 1) the central limit model has closed-form expressions calculated as the error functions, and 2) the curve fitting model can be closely modeled as a Gamma distribution. We then derive the closed-form outage probability expressions for the NOMA users. Numerical results indicate that 1) the two channel models match the simulation results well in low signal-to-noise-ratio (SNR) regions and perform as boundaries in high SNR regions, 2) the central limit model performs as an upper bound of the simulation results, while a lower bound can be obtained by the curve fitting model, and 3) the both users in the NOMA pair have no error floor.
\subsubsection{Intelligently Wireless Batteryless RF-Powered Reconfigurable Surface}
abstract:This work exploits commodity, ultra-low cost, commercial radio frequency identification tags (RFID) as the elements of a reconfigurable surface. Such batteryless tags are powered and controlled by a software-defined (SDR) reader, with properly modified software, so that a source-destination link is assisted, operating at a different carrier frequency. In terms of theory, the optimal gain and corresponding best element configuration is offered, with tractable polynomial complexity (instead of exponential) in number of elements. In terms of practice, a concrete way to design and prototype a wireless, batteryless, RF-powered, reconfigurable surface is offered and a proof-of-concept is experimentally demonstrated. It is also found that even with perfect channel estimation, the weak nature of backscattered links limits the performance gains, even for large number of surface elements. Impact of channel estimation errors is also studied. Future extensions at various carrier frequencies could be directly accommodated, through simple modifications in the antenna and matching network of each RFID tag/surface element.
\subsubsection{Sum-rate Maximization for RIS-assisted Radar and Communication Coexistence System}
abstract:Next-generation wireless communication systems are believed to share the same spectrum previously allocated to radar applications. The coexisting communication system will cause harmful interference to the radar system. In this paper, we investigate the deployment of the Reconfigurable intelligent surface (RIS) to improve the performance of a Multiple-Input Multiple-Output (MIMO) Radar and Communication Coexistence (RCC) system. RIS consists of a large number of nearly passive, and low-cost cost elements. RIS provides passive, and a relatively high beamforming gain by controlling the reflecting elements' reflection coefficients. Moreover, RIS can eliminate the mutual interference between the radar and communication systems. Meanwhile, we optimally design the transmit beamforming and the phase shifts for the RIS elements to improve the achievable sum-rate performance for the communication system. We jointly optimize the phase shifts and the beamforming vectors by using the local search approach. Numerical results verify the effectiveness of the utilization of the RIS to enhance the downlink sum-rate for the communication system users without interrupting the radar operation.
\subsubsection{Joint Transmit Power and Reflection Beamforming Design for IRS-Aided Covert Communications}
abstract:This work examines the performance gain achieved by deploying an intelligent reflecting surface (IRS) for delay-constrained covert communications. To this end, we formulate the joint design of the transmit power and the IRS reflection coefficients, including its phase shifts and reflection amplitudes, to maximize the communication quality subject to a covertness constraint.We first prove that perfect covertness is achievable with the aid of the IRS even for a single-antenna transmitter, which is impossible without the IRS. Then, we develop a penalty-based successive convex approximation (PSCA) algorithm to tackle the design optimization problem. Considering the high complexity of the PSCA algorithm, we further propose a low-complexity two-stage algorithm, where closed-form expressions for the transmit power and the IRS's reflection coefficients are derived. Our examination shows that significant performance gain can be achieved by deploying an IRS into covert communications.
\subsection{Localization}
\subsubsection{A Robust Single-Anchor Localization Method With Multipath Assistance in NLOS Environments}
abstract:Location-aware technologies are fast becoming a key instrument in civil applications. Ultra-wide bandwidth signals and their multipath components can provide precise channel information for radio frequency-based simultaneous localization and mapping (SLAM). In this paper, we provide a multipath-assisted single-anchor localization (MA-SAL) method to achieve robust localization of a user equipment as well as reflection points in indoor complex environments. In the proposed scheme, we develop a matching filter to determine the positions of the multipath components in the recorded channel impulse response and then estimate the delay and the angle-of-arrival of the signal on each propagation path. These estimates are fused with IMU by an adaptive federated filter to infer the states of a user equipment and reflecting surfaces. Experimental results show that the proposed MA-SAL outperforms traditional single-anchor localization in terms of localization accuracy under indoor non-line-of-sight propagation conditions.
\subsubsection{MAP-CSI: Single-site Map-Assisted Localization Using Massive MIMO CSI}
abstract:This paper presents a new map-assisted localization approach using Chanel State Information (CSI) in Massive Multiple-Input Multiple-Output (MIMO) systems. Map-assisted localization is an environment aware approach in which the communication system has information regarding the surrounding environment. By combining radio frequency ray tracing parameters of the multipath components (MPC) with the environment map, it is possible to accomplish localization. Unfortunately, in real-world scenarios, ray tracing parameters are typically not explicitly available. Thus, additional complexity is added at the base station to obtain this information. On the other hand, CSI is common communication parameter, usually estimated for any communication channel. In this work, we leverage the already available CSI data to propose a novel map-assisted CSI (MAP-CSI) localization approach. We show that Angle-of-Departure (AoD) and Time-of-Arrival (ToA) can be extracted from CSI and then used in combination with the environment map to localize the user. We perform simulations on a public MIMO dataset and show that our method works for both line-of-sight (LOS) and non-line-of-sight (NLOS) scenarios. We compare our method to the state-of-the-art method that uses the ray tracing data. Using MAP-CSI, we accomplish an average localization error of 1.8 m in LOS and 2.8 m in Mixed (combination of LOS and NLOS samples) scenarios. On the other hand, state-of-the-art ray tracing has an average error of 1.0 m and 2.2 m, respectively, but requires explicit AoD and ToA information to perform the localization task.
\subsubsection{Inseparable Waveform Synthesis in Joint Communications and Radar via Spatial-Frequency Spectrum}
abstract:Joint communications and radar (JCR), which use the same waveform for both functions, provide an efficient scheme of spectrum access and find various applications in practice such as autonomous driving. A convenient signaling framework is the orthogonal frequency-division multiplexing and multi-in-multi-out (OFDM-MIMO) structure, which corresponds to the spatial-frequency spectrum resulted from high-dimensional Fourier transform. The key challenge of JCR is how to resolve the interest conflict between communications and radar sensing, when they share the same waveform in an inseparable manner. The corresponding trade-off is formulated as constrained optimization problems for the cases of analog and digital beamformings. Numerical results show that the proposed schemes are effective in the spatial-frequency spectrum management and achieve good performance trade-offs between communications and radar sensing.
\subsubsection{High Precision Indoor Localization with Dummy Antennas - An Experimental Study}
abstract:With the rising demand for indoor localization, high precision technique-based fingerprints became increasingly important nowadays. The newest advanced localization system makes effort to improve localization accuracy in the time orfrequency domain, for example, the UWB localization technique can achieve centimeter-level accuracy but have a high cost. Therefore, we present a spatial domain extension-based scheme with low cost and verify the effectiveness of antennas extension in localization accuracy. In this paper, we achieve sub-meter level localization accuracy using a single AP by extending three radio links of the modified laptops to more antennas. Moreover, the experimental results show that the localization performance is superior as the number of antennas increases with the help of spatial domain extension and angular domain assisted.
\subsubsection{A Fine-Grained Analysis of Radar Detection in Vehicular Networks}
abstract:Automotive radar is a critical feature in advanced driver-assistance systems. It is important in enhancing the vehicle safety by detecting the presence of other vehicles in the vicinity. The performance of radar detection is, however, affected by the interference from radars of other vehicles as well as the variation in the target radar cross-section (RCS) due to varying physical features of the target vehicle. Considering such an interference and random RCS, this work provides a fine-grained performance analysis of radar detection. Specifically, using stochastic geometry, we calculate the meta distribution of the signal-to-interference-and-noise ratio that permits the reliability analysis of radar detection at individual vehicles. We also evaluate the delay aspect of radar detection, namely, the mean local delay which is the average number of transmission attempts needed until the first successful target detection. For a given target distance, we obtain the optimal transmit probability that maximizes the density of successful radar detection while keeping the mean local delay below a threshold. We also provide several system design insights in terms of the fraction of reliable radar links, transmission delay, density of vehicles, and the congestion control.
\subsection{Machine Learning for Communications}
\subsubsection{MAFENN: Multi-Agent Feedback Enabled Neural Network for Wireless Channel Equalization}
abstract:Feedback mechanism has been widely used in wireless communication such as channel equalization and resource allocation. In recent years, DL has made great progress in the field of wireless communication. There is now some work that attempts to introduce plain feedback mechanisms into DL algorithm to solve wireless communication problems. However, the improvement of plain feedback DL methods is limited in complex situations due to those methods lack sufficient learning ability on feedback information. In this paper, we propose a Multi-Agent Feedback Enabled Neural Network (MAFENN) equalizer, which consists of a specific learnable feedback agent and two feed-forward agents. Three fully cooperative intelligent agents help the system improve the ability to remove wireless inter-symbol interference (ISI) in receiving ends. We further formulate it into a three-player Stackelberg Game, which helps us to optimize and train this model more efficiently. To verify the feasibility of our proposed MAFENN system and the Stackelberg Game optimization, we conduct a series of experiments to compare the symbol error rate (SER) performance of the MAFENN equalizer and the other methods which utilizes quadrature phase-shift keying (QPSK) modulation scheme. Our performance outperforms that of the other equalizers at different signal-to-noise ratio (SNR) settings for both linear and nonlinear channels.
\subsubsection{Robust Symbol-Level Precoding Beyond CSI Models: A Probabilistic-Learning Based Approach}
abstract:The use of large antenna arrays poses great difficulties in obtaining perfect channel state information (CSI) in a multi-antenna communication system, which is essential for precoding optimization. To tackle this issue, in this paper we propose a probabilistic-learning based approach (PLA), aiming at alleviating the requirement of perfect CSI. The rationale is that the existing precoding algorithms that output a single precoder are often overconfident in their abilities and the obtained CSI. To avoid overconfidence, we incorporate the idea of regularization in machine learning (ML) into precoding models, so as to limit representative abilities of the precoding models. Compared to the state-of-the-art robust precoding designs, an important advantage of PLA is that CSI uncertainty models are not required. As aspecific application of PLA, we design an efficient symbol-level hybrid precoding algorithm for millimeter wave communication systems. The effectiveness of PLA is confirmed via simulations
\subsubsection{Knowledge Caching for Federated Learning}
abstract:This work examines a novel wireless content distribution problem where machine learning models (e.g., deep neural networks) are cached at local small cell base-stations to facilitate access by users within their coverage. The models are trained by federated learning procedures which allow local users to collaboratively train the models using their locally stored data. Upon the completion of training, the model can also be accessed by all other users depending on their application demand. Different from conventional wireless caching problems, the placement of machine learning models should depend not only on the users' preferences but also on the data available at the users and their channel conditions. In this work, we propose to jointly optimize the caching decision, user selection, and wireless resource allocation, including transmit powers and bandwidth of the selected users, to minimize a training error bound. The problem is reduced to minimizing a weighted sum of local dataset sizes subject to constraints on the cache storage capacity, the communication and computation latency, and the total energy consumption. We first derive the minimum loss achievable for each cached model, and, then, determine the optimal models to cache by solving an equivalent 0-1 Knapsack problem that minimizes the total average loss. Simulations show that the proposed scheme can achieve lower extremity error bounds compared to preference-only and random caching policies.
\subsubsection{Communication-Computation Efficient Device-Edge Co-Inference via AutoML}
abstract:Device-edge co-inference, which partitions a deep neural network between a resource-constrained mobile device and an edge server, recently emerges as a promising paradigm to support intelligent mobile applications. To accelerate the inference process, on-device model sparsification and intermediate feature compression are regarded as two prominent techniques. However, as the on-device model sparsity level and intermediate feature compression ratio have direct impacts on computation workload and communication overhead respectively, and both of them affect the inference accuracy, finding the optimal values of these hyper-parameters brings a major challenge due to the large search space. In this paper, we endeavor to develop an efficient algorithm to determine these hyper-parameters. By selecting a suitable model split point and a pair of encoder/decoder for the intermediate feature vector, this problem is casted as a sequential decision problem, for which, a novel automated machine learning (AutoML) framework is proposed based on deep reinforcement learning (DRL). Experiment results on an image classification task demonstrate the effectiveness of the proposed framework in achieving a better communication-computation trade-off and significant inference speedup against various baseline schemes.
\subsubsection{Unit-Modulus Wireless Federated Learning Via Penalty Alternating Minimization}
abstract:Wireless federated learning (FL) is an emerging machine learning paradigm that trains a global parametric model from distributed datasets via wireless communications. This paper proposes a unit-modulus wireless FL (UMWFL) framework, which simultaneously uploads local model parameters and computes global model parameters via optimized phase shifting. The proposed framework avoids sophisticated baseband signal processing, leading to both low communication delays and implementation costs. A training loss bound is derived and a penalty alternating minimization (PAM) algorithm is proposed to minimize the nonconvex nonsmooth loss bound. Experimental results in the Car Learning to Act (CARLA) platform show that the proposed UMWFL framework with PAM algorithm achieves smaller training losses and testing errors than those of the benchmark scheme.
\subsection{Massive MIMO}
\subsubsection{Massive MIMO Communication Over HF Skywave Channels}
abstract:In this paper, we investigate massive multi-input multi-output (MIMO) high frequency (HF) skywave communications. We first introduce a model for HF skywave massive MIMO channels within the orthogonal frequency division multiplexingtransmission framework by using the matrix of sampled steering vectors. The steering vectors vary across different subcarriers due to the effect of the propagation delay across the largescale antenna array. Specifically, we derive a wideband beam based channel model and show that the beam domain statisticalchannel state information (CSI) is frequency-independent. Then, we consider minimum mean-squared error based uplink receiver and downlink precoder with perfect CSI at the base station (BS). With a large number of antennas at the BS, the sum-rate can be asymptotically increased proportionally to the number of userterminals (UTs) while the transmit power per UT is scaled down inverse-proportionally to the number of antennas. Simulation results demonstrate very significant performance advantages of the proposed HF skywave massive MIMO system.
\subsubsection{FSST: Frequency-Space Signal Transformation of Massive MIMO Channels}
abstract:High overhead of sharing and feedback and high computational complexity are common problems in multi-cell processing. In this paper, a novel framework for bidirectionalsignal transformation between space and frequency domains of massive MIMO channels is proposed to reduce system processing overhead and complexity. We design new space and frequency features and build the framework by two off-line trained neural networks (NN). Moreover, the uniqueness of spatial features is proved. Average errors of uni- and bi-directional transformation are 7.6% and 7.3%. When applying the framework to inter-cell interference coordination (ICIC), the system and edge throughput are both increased compared to the traditional scheme with low information sharing overhead.
\subsubsection{Impact of Channel Aging on Massive MIMO Vehicular Networks in Non-isotropic Scattering Scenarios}
abstract:Massive multiple-input multiple-output (MIMO) relies on accurate channel estimation for precoding and receiving to achieve its claimed performance advantages. When serving vehicular users, the rapid channel aging effect greatly hinders its advantages, and a careful system design is required to ensure an efficient use of wireless resources. In this paper, we investigate this problem for the first time in a non-isotropic scattering scenario. The von Mises distribution is adopted for the angle of arrival (AoA), resulting in a tunable channel temporal correlation coefficient (TCC) model, which can adapt to different AoA spread conditions through the  parameter and incorporates the isotropic Jakes-Clarke model as a special case. The simulated results in a Manhattan grid-type multi-cell network clearly demonstrate the impact of channel aging on the uplink spectral efficiency (SE) performance and moreover, in order to maximize the area average SE, the size of the transmission block should be optimally selected according to some linear equations of .
\subsubsection{Distributed Expectation Propagation Detection for Cell-Free Massive MIMO}
abstract:In cell-free massive MIMO networks, an efficient distributed detection algorithm is of significant importance. In this paper, we propose a distributed expectation propagation (EP) detector for cell-free massive MIMO. The detector is composed of two modules, a nonlinear module at the central processing unit (CPU) and a linear module at the access point (AP). The turbo principle in iterative decoding is utilized to compute and pass the extrinsic information between modules. An analytical framework is then provided to characterize the asymptotic performance of the proposed EP detector with a large number of antennas. Simulation results will show that the proposed method outperforms the distributed detectors in terms of bit-error rate.
\subsection{MIMO}
\subsubsection{Rotatable URAs for Line-of-Sight MIMO Transmission}
abstract:This paper considers line-of-sight multiple-input multiple-output communication, of interest at millimeter-wave and sub-terahertz frequencies. Building on how, with an angular rotation dependent on the signal-to-noise ratio (SNR), uniform linear arrays (ULAs) can tightly approach the capacity of such channels, we assess the performance of rotatable uniform rectangular arrays (URAs). The changeover from ULAs to URAs is motivated by the interest in reducing the array footprints. For both isotropic and directive antennas, various types of rotatable URAs are devised that-except at very low SNRs-perform remarkably close to their ULA counterparts with considerably smaller footprints.
\subsubsection{A Distributed MIMO Relay Scheme Inspired by Backpropagation Algorithm}
abstract:This paper studies a distributed scheme for a multi-input multi-output (MIMO) relay network, where the transmit nodes are subject to the nonlinear instantaneous power constraints. We introduce a novel perspective of regarding a relay network as a so-termed quasi-neural network by drawing its striking analogies with a (four-layer) artificial neural network (ANN). We propose a nonlinear amplify-and-forward (NAF) scheme inspired by the back-propagation (BP) algorithm, namely the NAF-BP, to optimize the transceivers to maximize the output signal-to-interference-plus-noise ratio (SINR) of the data streams. The NAF-BP algorithm can be implemented in a distributed manner with no channel state information (CSI) and no data exchange between the relay nodes. The NAF-BP can also coordinate the distributed relay nodes to form a virtual array to suppress interferences from unknown directions. Extensive simulations verify the effectiveness of the proposed scheme.
\subsubsection{Energy-Efficient Cooperative Backscattering Closed-Form Solution for NOMA}
abstract:In this paper, the energy efficiency of multi-user non orthogonal multiple access (NOMA) systems in the presence of a backscatter device is investigated. The energy efficiency maximization problem is formulated as a tradeoff between the sum rate and the total power consumption and shown to be non-convex. We then derive a closed-form expression of the optimal reflection coefficient. Remarkably, the obtained expression allows the reformulation of the optimization in terms of the power allocation policy into a convex optimization problem that has recently been solved in closed form. This overall solution can then be exploited to reduce the computational complexity of Dinkelbach's algorithm for maximizing the ratio sum rate vs. total power. Simulation results show that the presence of backscatter devices significantly improve the energy efficiency of NOMA systems and reach up to 450% relative gains compared to OMA.
\subsubsection{Delay-Aware Power Control for Downlink Multi-User MIMO via Constrained Deep Reinforcement Learning}
abstract:We investigate the downlink transmission for multiuser multi-input multi-out (MU-MIMO) system, in which the regularized zero forcing (RZF) precoder is adopted and the power allocation and regularization factor are optimized. Ouraim is to find a power allocation and regularization factor control policy that can minimize the long-term average power consumption subject to long-term delay constraint for each user. The induced optimization problem is formulated as aconstrained Markov decision process (CMDP), which is efficiently solved by the proposed constrained deep reinforcement learning algorithm, called successive convex approximation policy optimization (SCAPO). The SCAPO is based on solving a sequence of convex objective/feasibility optimization problems obtained by replacing the objective and constraint functions in the original problems with convex surrogate functions. At each iteration, the SCAPO merely needs to estimate the first-order information and solve a convex surrogate problem that can be efficiently parallel tackled. Moreover, the SCAPO enables to reuse oldexperiences from previous updates, thereby significantly reducing the implementation cost. Numerical results have shown that the novel SCAPO can achieve the state-of-the-art performance over advanced baselines.
\subsection{NOMA}
\subsubsection{Resource Allocation in BER-Constrained Multicarrier NOMA Based on Optimal Channel Gain Ratios}
abstract:The application of non-orthogonal multiple access (NOMA) to multicarrier (MC) systems can improve the spectrum efficiency and enable massive connectivity in future mobile communication systems. Resource allocation in MC NOMA is a non-deterministic polynomial time (NP)-hard problem with prohibitive computational complexity. Thus, efficient algorithms that provide a good trade-off between performance and complexity are needed. In this paper, exact values of the optimal channel gain ratios between a pair of NOMA users are presented for quadrature amplitude modulation (QAM). Further, numerical limits are derived for the values of channel gain ratios that fulfill the system constraints. Unlike previous works, it is demonstrated that the benefit of pairing users with very distinct channel gains is lost under practical QAM schemes, due to the inability of users with poor channel conditions to fulfill bit error rate (BER) constraints. These findings are used to propose a user pairing algorithm with quasi-linear complexity. Further, a novel scheme for data rate and continuous power allocation is proposed. Through numerical simulations, it is proved that the proposed scheme yields an achievable sum-rate close to the performance of exhaustive search, and it outperforms other suboptimal resource allocation schemes.
\subsubsection{Near-Optimal User Clustering and Power Control for Uplink MISO-NOMA Networks}
abstract:This paper investigates the user clustering and power control in the uplink multiple-input single-output non-orthogonal multiple access (MISO-NOMA) networks. A joint optimization problem is formulated to minimize the system transmit power. Since the formulated optimization problem is prohibitively complicated especially when the number of users is large. Alternatively, a two-step user clustering and power control algorithm is proposed. First, an improved K-means algorithm is proposed for user clustering, where the clustering metric is analyzed by considering both channel gain and channel correlation among users to reduce the intra- and inter-cluster interference. Based on this, the optimal cluster number and cluster centers are dynamically obtained by the semi-orthogonal user selection (SUS) algorithm. Further, the closed-form expression of the optimal intra-cluster power control is derived, and the resulting inter-cluster power control problem is solved by an efficient algorithm in an iterative manner. Simulation results show that the proposed scheme achieves the near-optimal performance in terms of power consumption and energy efficiency with low computational complexity.
\subsubsection{Interference Cooperation based Resource Allocation in NOMA Terrestrial-Satellite Networks}
abstract:In this paper, an uplink non-orthogonal multiple access (NOMA) satellite-terrestrial network is investigated, where the terrestrial base stations (BSs) can simultaneously communicate with the satellite by backhaul, and user equipments (UEs) share fronthaul spectrum resource to communicate. The communication of satellite UEs is influenced by cross-tier interference caused by terrestrial cellular UEs. Thus, a utility function which consists of system achieved rate and cross-tier interference is build. And we aim to maximize the utility function while satisfying the constraints of the varying backhaul rate and quality of service (QoS) of UEs. The optimization problem is decomposed into AP-UE association, bandwidth assignment, and power allocation sub-problems, and solved by proposed matching algorithm and successive convex approximation (SCA) method, respectively. Then, a distributed iterative resource allocation algorithm is proposed to achieve the global optimization. The simulation results show the effectiveness of the proposed algorithm.
\subsubsection{Enhancing Security of NOMA Networks via Distributed Intelligent Reflecting Surfaces}
abstract:This paper investigates the security enhancement of an intelligent reflecting surface (IRS) assisted non-orthogonal multiple access (NOMA) network, where a distributed IRS enabled NOMA transmission framework is proposed to serve users securely in the presence of a passive eavesdropper. Considering that the instantaneous channel state information (CSI) of the eavesdropper is challenging to acquire in practice, we utilize the secrecy outage probability (SOP) as the security metric. A problem by jointly optimizing the transmit power at the base station (BS) and reflection phase shifts at IRSs, subject to the successive interference cancellation (SIC) decoding constraints and SOP constraints, is formulated to maximize the minimum secrecy rate among legitimate users. To tackle the non-convex problem, we first derive the exact SOP in closed-form expressions and then propose a novel ring-penalty based successive convex approximation (SCA) algorithm to design power allocation and phase shifts jointly. Numerical results validate the convergence and the secrecy superiority of proposed scheme over the baseline schemes.
\subsubsection{Secrecy Outage Probability Analysis for Downlink Untrusted NOMA Under Practical SIC Error}
abstract:Non-orthogonal multiple access (NOMA) servesmultiple users simultaneously via the same resource block byexploiting superposition coding at the transmitter and successiveinterference cancellation (SIC) at the receivers. Under practicalconsiderations, perfect SIC may not be achieved. Thus, residualinterference (RI) occurs inevitably due to imperfect SIC. In thiswork, we first propose a novel model for characterizing RIto provide a more realistic secrecy performance analysis of adownlink NOMA system under imperfect SIC at receivers. Inthe presence of untrusted users, NOMA has an inherent securityflaw. Therefore, for this untrusted users' scenario, we derive newanalytical expressions of secrecy outage probability (SOP) foreach user in a two-user untrusted NOMA system by using theproposed RI model. To further shed light on the obtained resultsand obtain a deeper understanding, a high signal-to-noise ratioapproximation of the SOPs are also obtained. Lastly, numericalinvestigations are provided to validate the accuracy of the desiredanalytical results and present valuable insights into the impactof various system parameters on the secrecy rate performanceof the secure NOMA communication system.
\subsection{Resource Allocation I}
\subsubsection{Reinforcement learning for Admission Control in 5G Wireless Networks}
abstract:The key challenge in admission control in wireless networks is to strike an optimal trade-off between the blocking probability for new requests while minimizing the dropping probability of ongoing requests. We consider two approaches for solving the admission control problem: i) the typically adopted threshold policy and ii) our proposed policy relying on reinforcement learning with neural networks. Extensive simulation experiments are conducted to analyze the performance of both policies. The results show that the reinforcement learning policy outperforms the threshold-based policies in the scenario with heterogeneous time-varying arrival rates and multiple user equipment types, proving its applicability in realistic wireless network scenarios.
\subsubsection{Learning Proximal Operator Methods for Massive Connectivity in IoT Networks}
abstract:Grant-free random access has been recognized as a promising technique for massive connectivity in Internet of Things (IoT) networks. To enable grant-free random access, joint activity detection and channel estimation (JADCE) is a critical issue that needs to be addressed. The existing methods for JADCE usually suffer from one of the following limitations: high computational complexity, ineffective in inducing sparsity, and incapable of handling complex matrix estimation. To mitigate all the aforementioned limitations, we in this paper propose a novel unfolding neural network framework based on the proximal operator method to solve the JADCE problem for grant-free uplink transmission in IoT networks with a multi-antenna base station (BS). Specifically, we first formulate the JADCE problem as a group-sparse-matrix estimation problem regularized by non-convex minimax concave penalty (MCP). This problem can be iteratively solved by using the proximal operator method, based on which we develop a unfolding neural network structure by parameterizing the algorithmic iterations. By further exploiting the coupling structure among the training parameters as well as the analytical computation, we develop two additional unfolding structures to reduce the training complexity. We theoretically prove the linear convergence guarantee and the support recovery guarantee of our proposed algorithm. Simulation results demonstrate that the proposed three unfolding structures not only achieve a faster convergence rate but also obtain a higher estimation accuracy than the baseline methods.
\subsubsection{Mean-Field Approximation based Scheduling for Broadcast Channels with Massive Receivers}
abstract:The emerging Industrial Internet of Things (IIoT) is driving an ever increasing demand for providing low latency services to massive devices over wireless channels. As a result, how to assure the quality-of-service (QoS) for a large amount of mobile users is becoming a challenging issue in the envisioned sixth-generation (6G) network. In such networks, the delay-optimal wireless access will require a joint channel and queue aware scheduling, whose complexity increases exponentially with the number of users. In this paper, we adopt the mean field approximation to conceive a buffer-aware multi-user diversity or opportunistic access protocol, which serves all backlogged packets of a user if its channel gain is beyond a threshold. A theoretical analysis and numerical results will demonstrate that not only the cross-layer scheduling policy is of low complexity but is also asymptotically optimal for a huge number of devices.
\subsubsection{Resource Allocation for Age of Information Minimization in An OFDM Status Update System}
abstract:For timeliness-sensitive applications in Internet of things (IoT) systems, it is critical to efficiently allocate transmission resources such that the freshness of information updates can be improved. This paper focuses on timely status updating in an orthogonal frequency division multiplexing-based IoT systems, in which all devices update status to one data center by sharing available bandwidth. To improve the timeliness of updates, a resource allocation optimization problem is formulated, based on finite blocklength (FBL) transmission and the age of information (AoI) metric. Two suboptimal policies, namely, fixed time slot policy and fixed blocklength policy, along with an iterative optimization algorithm, and an approximate optimal policy are presented for addressing the optimal resource allocation. By comparing the performance of different policies, it is shown that the iterative algorithm and the approximate optimal policy outperforms the other two suboptimal policies, and closely approaches the global optimal resource allocation.
\subsubsection{Minimizing AoI in Resource-Constrained Multi-Source Relaying Systems with Stochastic Arrivals}
abstract:We consider a multi-source relaying system wherethe sources independently and randomly generate status updatepackets which are sent to the destination with the aid of a buffer-aidedrelay through unreliable links. We formulate a stochasticoptimization problem aiming to minimize the sum average ageof information (AAoI) of sources under per-slot transmissioncapacity constraints and a long-run average resource constraint.To solve the problem, we recast it as a constrained Markovdecision process (CMDP) problem and adopt the Lagrangianmethod. We analyze the structure of an optimal policy, which isin the class of either randomized or mixing policies, that possessesa switching-type structure. We propose an algorithm that obtainsthe optimal value of the sum AAoI, establishing a benchmarkfor the system. Simulation results show the effectiveness of ouralgorithm compared to benchmark algorithms.
\subsection{THz communications}
\subsubsection{Mobility and Blockage-induced Beam Misalignment and Throughput Analysis for THz Networks}
abstract:Terahertz (THz) communication is capable of providing ultra-wide bandwidth and high data rates. Therefore attracts widespread attention to its applications in next-generation networks. Highly directional antennas are used to compensate for the THz propagation loss, which also incurs beam management challenges. Specifically, caused by node mobility and blockage, frequent beam reselections and beam misalignment greatly degrade THz network performance in terms of reliability and spatial throughput. In this paper, using stochastic geometry, we fill the current research gap in system-level theoretical models for the analysis of beam misalignment and network spatial throughput by considering the effects of beamwidth, mobility, blockage, and molecular absorption. Our analyses show that an increase in deployment density or user mobility often results in severe beam misalignment, which in turn requires more signaling overhead and degrades THz network reliability and throughput. Although using wider beams reduces this impact, it increases THz network sensitivity to molecular absorption. To maximize spatial throughput, optimal beamwidth needs to be adjusted according to communication demand priority and network status.Our work provides useful insights into beamwidth adaptation according to parameters trade-off that helps THz network achieve higher reliability and throughput in different applications.
\subsubsection{DFT-Spread Orthogonal Time Frequency Space Modulation Design for Terahertz Communications}
abstract:Terahertz (THz) band communication is a promising pillar technology to satisfy the demands of intelligent information society. The ultra-broad bandwidth in the THz band provides a great potential of a plethora of applications and services. However, THz wireless communication systems encounter stringent challenges, including more severe Doppler effects and more strict peak-to-average power ratio (PAPR) requirements. In this work, a discrete Fourier transform spread orthogonal time frequency space (DFT-s-OTFS) modulation scheme is proposed to address these issues of THz communications. The proposed DFT-s-OTFS can improve the bit error rate (BER) performance by two orders of magnitude compared to orthogonal frequency division multiplexing (OFDM) in presence of high Doppler spread, and reduce the PAPR by approximately 3 dB in contrast with OTFS.
\subsubsection{Resource Management for Intelligent Reflecting Surface Assisted THz-MIMO Network}
abstract:As the preferred frequency band for future high frequency communication, the terahertz (THz) band has attracted wide attention. In this paper, an energy efficient resource optimization problem in THz band is studied. The massive Multiple-Input Multiple-Output (MIMO) technology and intelligent reflecting surface (IRS) are adopted to improve the capacity and energy efficiency (EE) of proposed network. An IRS assisted THz-MIMO downlink wireless network system is established. The original EE problem is decomposed into phase-shift matrix optimization and power allocation. On this basis, a distributed EE optimization algorithm is designed, which transforms the original nonlinear problem into a convex optimization problem. The simulation results reveal that the proposed distributed optimization method converges rapidly and abtains the maximum EE. This also proves that it is feasible and effective to apply both the IRS and the massive MIMO technology into THz communication network.
\subsubsection{Hybrid mmWave-THz Networks with User-Centric Clustering}
abstract:This paper investigates a user-centric clustering model for a hybrid network comprising both millimeter-wave (mmWave) and terahertz (THz) base stations (BSs). Based on the proposed model, a user can choose to be cooperatively served by multiple mmWave or multiple THz BSs depending on their link quality. Besides, to maximize the cooperation gains, the serving clusters are dynamically adjusted to user'schannel conditions pertaining to the different properties of the mmWave and THz networks. Finally, we evaluate the coverage probability of the hybrid network by using stochastic geometric tools and validate the analysis through numerical simulations.
\subsubsection{Intelligent Surface Optimization in Terahertz under Two Manifestations of Molecular Re-radiation}
abstract:The operation of Terahertz (THz) communication can be significantly impacted by the interaction between the transmitted wave and the molecules in the atmosphere. In particular, it has been observed experimentally that the signal undergoes not only molecular absorption, but also molecular re-radiation. Two extreme modeling assumptions are prevalent in the literature, where the re-radiated energy is modeled in the first as additive Gaussian noise and in the second as a scattered component strongly correlated to the actual signal. Since the exact characterization is still an open problem, we provide in this paper the first comparative study of the performance of a reconfigurable intelligent surface (RIS) assisted THz system under these two extreme models of re-radiation. In particular, we employ an RIS to overcome the large pathloss by creating a virtual line-of-sight (LOS) path. We then develop an optimization framework for this setup and utilize the block-coordinate descent (BCD) method to iteratively optimize both RIS configuration vector and receive beamforming weight resulting in significant throughput gains for the user of interest compared to random RIS configurations. As expected, our results reveal that better throughput is achieved under the scattering assumption for the molecular re-radiation than the noise assumption.
\subsection{UAV Communications}
\subsubsection{RNN-Based Twin Channel Predictors for CSI Acquisition in UAV-Assisted 5G+ Networks}
abstract:Unmanned aerial vehicles (UAVs) evolution hasgained an unabated interest for the use in several applications,such as agriculture, aerial surveillance, goods delivery, disasterrecovery, intelligent transportation. The main features of thistechnology are high coverage, strong line-of-sight (LoS) links,promising throughput, cost-effective and flexible deployment.Currently, the Third Generation Partnership Project (3GPP)is working on the specification of release-17 (R-17) new radio(NR) for non-terrestrial networks (NTN). Therefore, owing tothe drastic increase of UAV technology, in this paper, we proposechannel state information (CSI) compression and its recoverywith the aid of machine learning (ML)-based twin channelpredictors. Due to the characteristic of gaining higher LoScommunication paths in UAV network, the proposed strategycan bring potential benefits such as over-the-air (OTA)-overheadreduction, minimizing mean-squared-error (MSE) of a channeland maximizing precoding gain. Simulation-based results corroborate the validity of the proposed strategy, which can reapbenefits in multiple factors.
\subsubsection{Statistical Mechanics Analysis of Flocking UAV Networks with Limited Communications}
abstract:Communication is of key importance for the control of unmanned aerial vehicle (UAV) networks. From the viewpoint of statistical mechanics, it plays the role of bond and correlation in the ensemble of particles. A statistical mechanics model is set for UAV networks with limited communication ranges, including potentials and stochastic evolutions. Based on the framework, an equation describing the behavior of UAV network, in terms of canonical thermodynamics metrics, is derived using the technique of cluster expansion. The critical temperature, characterizing the phase transition (between unstable and stable), is derived in explicit expression. Numerical simulation shows that the statistical mechanics analysis provides an operable estimation on the stability condition of UAV networks, in terms of communication range. In particular, it is shown that the critical temperature (the power of random perturbation) follows a power law in terms of the communication range, given the model adopted in this paper.
\subsubsection{Multipath Fading Channel Modeling with Aerial Intelligent Reflecting Surface}
abstract:Different from the traditional terrestrial intelligent reflecting surface (IRS), aerial IRS (AIRS) can provide some unique advantages, such as flexible deployment and wider-view signal reflection. In this paper, a three-dimensional (3D) single cylinder simulation channel model is proposed for AIRS-aided multiple-input multiple-output (MIMO) communication systems, where the considered propagation scenario consists of a fixed base station (BS) and a mobile station (MS). Based on the model, the channel impulse response (CIR), spreading function, and channel capacity are derived. Then, some heuristic algorithms are proposed to obtain the phase shifts of the IRS elements. It is found that multipath fading and Doppler effects stemming from the movement of MS can be effectively mitigated via adjusting the tunable phase shifts of the IRS elements. Moreover, the channel capacity of the system could also be improved by the proposed schemes. These findings can be used to lay a foundation for developing intelligent and controllable propagation environments.
\subsubsection{Fundamentals of 3D Two-Hop Cellular Networks Analysis with Wireless Backhauled UAVs}
abstract:This paper provides the performance characterization of a three-dimensional (3D) two-hop decode-and-forward (DF) aerial-terrestrial communication network, where unmanned aerial vehicles (UAVs) coexist with terrestrial base stations (BSs) to serve a set of user equipment (UE) on the ground. We assume that each UE connects either to a BS via access link or through a UAV to a BS via joint access and backhaul links, where the link from the UE to the UAV is an access link and from the UAV to the BS is a backhaul link. To capture the impact of directionality in practical antennas, we use a model developed by the third generation partnership project (3GPP) for the antenna radiation pattern of both BSs and UAVs. Following the nearest neighbor association policy, we obtain the joint distance and angle distribution of the serving UAV to the origin in a 3D setting using tools from stochastic geometry. Furthermore, we identify and analyze key mathematical constructs as the building blocks of characterizing the received signal-to-interference-plus-noise ratio (SINR) distribution at the typical UE for the DF relaying protocol. Using these intermediate results, we derive an exact mathematical expression for the coverage probability in UAV-assisted two-hop DF cellular networks. One key takeaway from our analysis is the existence of a mean UAV height and a 3D density of UAVs that optimize the network coverage performance.
\subsection{URLLC}
\subsubsection{Pilot Overhead vs. Pilot Power: Short Packet Structure Optimization for URLLC over Continuous Fading}
abstract:In this paper, we investigate how to strike the balance between pilot overhead and pilot power for short packet ultra reliable and low latency communication (URLLC) systems under continuous fading, as the previous work has not considered the joint impact of pilot overhead and pilot power on the system throughput and has assumed block fading only. It is revealed that, at low to moderate signal to noise ratio (SNR) or in continuous fading, pilot power boosting can reduce pilot overhead and improve throughput significantly, and that at high SNR, pilot overhead plays a dominant role in enhancing the throughput while maintaining a low peak to average power ratio. For throughput formulation, a closed-form expression for the asymptotic block error probability under continuous fading is derived with respect to pilot power, pilot length and block length. And for throughput maximization, the near-optimal pilot power and the near-optimal block length are given in closed form by solving complicated transcendental equations. Based on the analysis, a low-complexity joint pilot power, pilot length and block length optimization (JPLLO) algorithm is proposed, which achieves a near-optimal performance and a dramatic complexity reduction compared to exhaustive search, converging within only 1-3 iterations. The JPLLO algorithm also significantly outperforms the previous joint optimization algorithm under continuous fading.
\subsubsection{Ultra-Reliable and Low Latency Wireless Communications with Burst Traffics: A Large Deviation Method}
abstract:Ultra-reliable and low latency communications (URLLC) has recently attracted much attention because it holds the promise of supporting mission-critical applications in task-driven radio access networks. In URLLC, finite-blocklength coding plays an important role. In this paper, we are interested in the performance limit of finite-blocklength coded wireless URLLC with burst traffics, which may induce severe delay in practice. A large deviation technique is exploited to derive the delay violation probability given a hard delay constraint. Specifically, an approximate QoS exponent is conceived to characterize the reliability-latency tradeoff, i.e., tradeoff between the error probability and delay violation probability. Further, we present closed-form upper and lower bounds of the QoS exponent, which are shown to become tight in the high signal-to-noise ratio (SNR) regime. Our theoretical analysis is also validated by numerical results.
\subsubsection{Achieving Extremely Low Latency: Joint Finite-Blocklength Coding over Multiple Users in Downlinks}
abstract:With over-the-air latency on the order of 0.1ms required in 6G systems, the practical design of Finite-Blocklength Coding (FBC) has been expected to achieve extremely low latency communications. For this purpose, we focus on a joint FBC scheme in multi-user downlink systems. With a requirement of extremely low latency, we jointly encode data bits of multiple users over their orthogonal channel resources. As a result, we obtain throughput gain of the downlink transmission by an enlarged blocklength of FBC. In particular, we first present the joint encoding design for multiple downlink users by a matrix-based method. Under the multi-user joint FBC scheme, we then formulate an Integer Programming (IP) problem to maximize the throughput of downlink users subject to an average constraint on transmission power. By converting the derived IP problem to a nonlinear bipartite matching problem, we finally present a unified algorithm to obtain the optimal power-constrained throughput within the low latency requirement.
\subsubsection{Constrained Deep Reinforcement Learning for Low-Latency Wireless VR Video Streaming}
abstract:Wireless virtual reality (VR) systems are able to provide users with immersive experiences, and require low latency and high data rate. To meet these conflicting requirements with limited radio resources, edge intelligence is a promising architecture. It exploits the edge server co-located at the base station to predict the field of view (FoV) of the next VR video segment, pre-render the three-dimensional video within the predicted FoV, and transmit it to the user in advance. Since the prediction is not error-free, the predicted FoV may not cover the actual FoV requested by the user, and hence may result in video quality loss. To address this issue, we first formulate a constrained partially observable Markov decision process problem to optimize the redundant range of the FoV according to the head motion prediction and the redundant range for the previous video segment. Then, we develop a constrained deep reinforcement learning algorithm to minimize the video quality loss ratio subject to the latency constraint. Simulation results show that the proposed algorithm outperforms the existing methods in terms of video quality loss ratio (from 6.9% to 4.9%) and latency (from 0.72 s to 0.63 s).
\subsubsection{Soft-ACK Feedback Based Link Adaptation for Latency Critical Applications in 5G/B5G}
abstract:Link adaptation plays an essential role in optimizing the performance of wireless communications systems. With the emerging services, link adaptation needs to operate with stringent reliability and latency requirements. This paper introduces Soft-ACK/NACK based link adaptation, which is an adaptation of conventional OLLA to the high reliability and low latency requirements by a range of emerging 5G/6G NR applications, such as URLLC/IIoT, AV/AR, cloud gaming or V2X. We study the limitations of conventional OLLA for these scenarios, and introduce an enhanced OLLA algorithm using Soft-ACK/NACK feedback from the UE. We analyse the possible methods to derive Soft-ACK/NACK reports, and propose using flipped bit counts based on accuracy and feasibility. The performance gains of Soft-OLLA are assessed in performance simulations.
\subsection{Wireless Communications}
\subsubsection{On the Performance of Delay Line Based OAM Communications}
abstract:Delay line based uniform circular antenna array (UCA) is commonly used for orbital angular momentum (OAM) generation. It is known that delay line is preferred for Multiple-input Multiple-output(MIMO) beamforming than the ideal phase shifter(PS), as the latter causes the problem of beam squint. Different from traditional MIMO beamforming, we theoretically prove in this paper that ideal PS is preferred for UCA based OAM communications, while delay line causes vortex distortion. Specifically, through the derivation of baseband equivalent model, we demonstrate that vortex distortion of delay line is a time-varying process, which is affected by OAM mode and signal bandwidth. To verify our analysis, we first simulate instantaneous distortion, and then simulate isolation and spectrum efficiency in different OAM mode and bandwidth configurations. The simulation results indicate that for UCA based OAM communication systems using delay line, higher OAM mode and higher bandwidth greatly limit mode isolation and spectrum efficiency. Our research also applies to delay line based customized OAM antennas, such as helicoidal parabolic antenna, traveling wave circular loop antenna and so on. The research result is of great significance to OAM based wireless communications.
\subsubsection{The effect of ADC resolution on concurrent, multiband, direct RF sampling receivers}
abstract:Connectivity using interband frequencies in 4G and 5G radio access networks, for example, carrier aggregation or dual-connectivity, incurs high receiver complexity and power consumption, in particular, when implemented using multiple radio units. Employing concurrent, multiband, direct RF sampling in a single radio chain architecture reduces the RF component count, leading to lower receiver complexity and power consumption. For this architecture, as the composite signal from multiple concurrent bands is digitised by a common analogue-to-digital converter (ADC), the bit resolution critically affects system performance. In this paper, the effect of ADC resolution on the error vector magnitude (EVM) and Block Error Rate (BLER) performance of a concurrent, multiband, direct RF sampling receiver is investigated. Simulation and hardware measurement of a tri-band Long Term Evolution (LTE) system supporting three simultaneously active channels at 888 MHz, 1.92 GHz and 2.52 GHz is evaluated when reducing the ADC resolution from 8 to 3 bits. Interband interference measurements demonstrate that the multiband, direct RF sampling, wideband LTE receiver remains 3GPP compliant at 4-bit ADC resolution with the signal-to-noise-ratio (SNR) desensitisation over a single-band receiver limited to 9 dB in the 888 MHz band.
\subsubsection{Semantic Communications for Speech Recognition}
abstract:The traditional communications transmit all the source date represented by bits, regardless of the content of source and the semantic information required by the receiver. However, in some applications, the receiver only needs part of the source data that represents critical semantic information, which prompts to transmit the application-related information, especially when bandwidth resources are limited. In this paper, we consider a semantic communication system for speech recognition by designing the transceiver as an end-to-end (E2E) system. Particularly, a deep learning (DL)-enabled semantic communication system, named DeepSC-SR, is developed to learn and extract text-related semantic features at the transmitter, which motivates the system to transmit much less than the source speech data without performance degradation. Moreover, in order to facilitate the proposed DeepSC-SR for dynamic channel environments, we investigate a robust model to cope with various channel environments without requiring retraining. The simulation results demonstrate that our proposed DeepSC-SR outperforms the traditional communication systems in terms of the speech recognition metrics, such as character-error-rate and word-error-rate, and is more robust to channel variations, especially in the low signal-to-noise (SNR) regime.
\subsubsection{Enabling Full Mutualism for Symbiotic Radio with Massive Backscatter Devices}
abstract:Symbiotic radio is a promising technology to achieve spectrum- and energy-efficient wireless communications, where the secondary backscatter device (BD) leverages not only the spectrum but also the power of the primary signals for its own information transmission. In return, the primary communication link can be enhanced by the additional multipaths created by the BD. This is known as the mutualism relationship of symbiotic radio. However, as the backscattering link is much weaker than the direct link due to double attenuations, the improvement of the primary link brought by one single BD is extremely limited. To address this issue and enable full mutualism of symbiotic radio, in this paper, we study symbiotic radio with massive number of BDs. For symbiotic radio multiple access channel (MAC) with successive interference cancellation (SIC), we first derive the achievable rate of both the primary and secondary communications, based on which a receive beamforming optimization problem is formulated and solved. Furthermore, considering the asymptotic regime of massive number of BDs, closed-form expressions are derived for the primary and the secondary communication rates, both of which are shown to be increasing functions of the number of BDs. This thus demonstrates that the mutualism relationship of symbiotic radio can be fully exploited with massive BD access.
\subsubsection{A Novel Rate Combining Model for Coded Modulation with Applications to Power Allocation}
abstract:In this paper, a new rate combining model for coded modulation with practical channel codes is proposed, resulting in a novel utility function. The mathematical properties of the new utility function, such as convexity, are studied. This utility function generalizes the harmonic mean function and demonstrates great potential in various applications in communication and learning problems. The proposed rate combining model is applied to the power allocation problem for parallel Gaussian channels with coded modulation inputs. Our results show that optimal power allocation based on the new rate combining model as the objective outperforms other well known schemes such as mercury water-filling in parallel channels or multiple-input and multiple-output (MIMO) channels.
